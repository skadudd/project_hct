{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JSON ë¡œê·¸ë¥¼ Excel í˜•íƒœë¡œ ë³€í™˜ ===\n",
      "âœ… ê°•ì œ JSON ìˆ˜ì • ì„±ê³µ (ë¼ì¸ 1)\n",
      "âœ… ê°•ì œ JSON ìˆ˜ì • ì„±ê³µ (ë¼ì¸ 2)\n",
      "âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ: 2ê°œ\n",
      "âœ… ìˆ˜ì • ì„±ê³µ: 2ê°œ\n",
      "âœ… ìµœì¢… íŒŒì‹±ëœ ë¡œê·¸: 3ê°œ\n",
      "âœ… ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: 3ê°œ ë¡œê·¸\n",
      "âœ… ìƒì„¸ ë°ì´í„° ë³€í™˜ ì™„ë£Œ: 34ê°œ key-value ìŒ\n",
      "âœ… ê²°í•© ë°ì´í„° ë³€í™˜ ì™„ë£Œ: 3ê°œ ë¡œê·¸\n",
      "âœ… ìƒì„¸ Excel íŒŒì¼ ì €ì¥: ./result/log_analysis_detailed.xlsx\n",
      "   ì´ 34ê°œ key-value ìŒ\n",
      "âœ… ê²°í•© Excel íŒŒì¼ ì €ì¥: ./result/log_analysis_combined.xlsx\n",
      "   ì´ 3ê°œ ë¡œê·¸\n",
      "\n",
      "ğŸ“Š ë³€í™˜ ê²°ê³¼ ìš”ì•½:\n",
      "   total_logs: 3\n",
      "   total_key_value_pairs: 34\n",
      "   average_keys_per_log: 11.33\n",
      "   max_keys_in_single_log: 12\n",
      "   min_keys_in_single_log: 10\n",
      "   unique_keys_count: 20\n",
      "\n",
      "ğŸ“‹ ìƒì„¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 15ê°œ í–‰):\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "No  | Page ID                        | Click Type      | Act Type     | Key                       | Value                         \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  1 | life-dev/news/detail/10515     | ë‰´ìŠ¤              | click        | categoryId                | 1001                          \n",
      "  1 | life-dev/news/detail/10515     | ë‰´ìŠ¤              | click        | channel                   | Rround                        \n",
      "  1 | life-dev/news/detail/10515     | ë‰´ìŠ¤              | click        | page_url                  | https://life-dev.hectoinnovati\n",
      "  1 | life-dev/news/detail/10515     | ë‰´ìŠ¤              | click        | reffer_url                | https://life-dev.hectoinnovati\n",
      "  1 | life-dev/news/detail/10515     | ë‰´ìŠ¤              | click        | click_text                | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ë§¹ìŠ¹ì§€, ê³ ì•¡ ìŠ¤í° ì œì•ˆ D\n",
      "  1 | life-dev/news/detail/10515     | ë‰´ìŠ¤              | click        | area_name                 | ì—°ì˜ˆ                            \n",
      "  1 | life-dev/news/detail/10515     | ë‰´ìŠ¤              | click        | tab_name                  | ì—°ì˜ˆ                            \n",
      "  1 | life-dev/news/detail/10515     | ë‰´ìŠ¤              | click        | el_order                  | 2                             \n",
      "  1 | life-dev/news/detail/10515     | ë‰´ìŠ¤              | click        | list_name                 | ì—°ì˜ˆ                            \n",
      "  1 | life-dev/news/detail/10515     | ë‰´ìŠ¤              | click        | article_id                | 10515                         \n",
      "  1 | life-dev/news/detail/10515     | ë‰´ìŠ¤              | click        | article_title             | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ë§¹ìŠ¹ì§€, ê³ ì•¡ ìŠ¤í° ì œì•ˆ D\n",
      "  1 | life-dev/news/detail/10515     | ë‰´ìŠ¤              | click        | list_id                   | 1001                          \n",
      "  2 | life-dev/news/detail/10531     | ë‰´ìŠ¤              | click        | categoryId                | 1002                          \n",
      "  2 | life-dev/news/detail/10531     | ë‰´ìŠ¤              | click        | channel                   | Rround                        \n",
      "  2 | life-dev/news/detail/10531     | ë‰´ìŠ¤              | click        | page_url                  | https://life-dev.hectoinnovati\n",
      "... ì™¸ 19ê°œ í–‰\n",
      "\n",
      "ğŸ“‹ ê²°í•© ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 5ê°œ í–‰):\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "No  | Page ID                   | Click Type   | Act Type     | Keys                                     | Values                                   | Count\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | categoryId, channel, page_url, reffer_ur | 1001, Rround, https://life-dev.hectoinno |    12\n",
      "  2 | life-dev/news/detail/1053 | ë‰´ìŠ¤           | click        | categoryId, channel, page_url, reffer_ur | 1002, Rround, https://life-dev.hectoinno |    10\n",
      "  3 | life-dev/search           | ìƒí’ˆ           | click        | channel, page_url, reffer_url, click_tex | Rround, https://life-dev.hectoinnovation |    12\n",
      "\n",
      "ğŸ“Š í‚¤ ë¶„ì„ (ì´ 20ê°œ ê³ ìœ  í‚¤):\n",
      "--------------------------------------------------\n",
      "Key                            | ë¹ˆë„ìˆ˜       \n",
      "--------------------------------------------------\n",
      "channel                        |          3\n",
      "page_url                       |          3\n",
      "reffer_url                     |          3\n",
      "click_text                     |          3\n",
      "tab_name                       |          3\n",
      "categoryId                     |          2\n",
      "article_id                     |          2\n",
      "article_title                  |          2\n",
      "list_id                        |          2\n",
      "area_name                      |          1\n",
      "el_order                       |          1\n",
      "list_name                      |          1\n",
      "list_order                     |          1\n",
      "kwd                            |          1\n",
      "prd_code                       |          1\n",
      "prd_name                       |          1\n",
      "prd_brand                      |          1\n",
      "prd_price_final                |          1\n",
      "prd_order                      |          1\n",
      "prd_is_ad                      |          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import re\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "\n",
    "class LogToDetailedConverter:\n",
    "    \"\"\"JSON ë¡œê·¸ë¥¼ ë‹¤ì–‘í•œ Excel í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
    "    def __init__(self):\n",
    "        self.input_data = []\n",
    "        self.detailed_data = []\n",
    "        self.detailed_deduplicated_data = []\n",
    "        self.combined_data = []\n",
    "        self.deduplicated_data = []\n",
    "\n",
    "    def convert_log_to_all_formats(self, input_log_path: str, \n",
    "                                 detailed_output_path: str, \n",
    "                                 detailed_deduplicated_output_path: str,\n",
    "                                 combined_output_path: str,\n",
    "                                 combined_deduplicated_output_path: str):\n",
    "        \"\"\"\n",
    "        JSON ë¡œê·¸ íŒŒì¼ì„ ìƒì„¸ í˜•íƒœì™€ ê²°í•© í˜•íƒœ ë‘ ê°€ì§€ë¡œ ë³€í™˜\n",
    "        \n",
    "        Args:\n",
    "            input_log_path: log_file.txt íŒŒì¼ ê²½ë¡œ\n",
    "            detailed_output_path: log_analysis_detailed.xlsx ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê° key-valueê°€ ê°œë³„ í–‰)\n",
    "            combined_output_path: log_analysis_combined.xlsx ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (key, valueê°€ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì—´)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. ì…ë ¥ ë¡œê·¸ íŒŒì¼ ì½ê¸°\n",
    "            self._load_log_file(input_log_path)\n",
    "            \n",
    "            # 2. ìƒì„¸ ë°ì´í„°ë¡œ ë³€í™˜\n",
    "            self._convert_to_detailed()\n",
    "            \n",
    "            # 3. ê²°í•© ë°ì´í„°ë¡œ ë³€í™˜\n",
    "            self._convert_to_combined()\n",
    "            \n",
    "            # 4. ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±°\n",
    "            self._convert_to_combined_deduplicated()\n",
    "            \n",
    "            # 5. ìƒì„¸ í˜•íƒœ ì¤‘ë³µ ì œê±°\n",
    "            self._convert_to_detailed_deduplicated()\n",
    "            \n",
    "            # 6. ë„¤ í˜•íƒœ ëª¨ë‘ ì €ì¥\n",
    "            self._save_detailed_excel(detailed_output_path)\n",
    "            self._save_detailed_deduplicated_excel(detailed_deduplicated_output_path)\n",
    "            self._save_combined_excel(combined_output_path)\n",
    "            self._save_combined_deduplicated_excel(combined_deduplicated_output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "    def convert_log_to_both_formats(self, input_log_path: str, \n",
    "                                  detailed_output_path: str, \n",
    "                                  combined_output_path: str):\n",
    "        \"\"\"\n",
    "        ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ - ì¤‘ë³µ ì œê±° ì—†ì´ ë‘ ê°€ì§€ í˜•íƒœë§Œ ë³€í™˜\n",
    "        \n",
    "        Args:\n",
    "            input_log_path: log_file.txt íŒŒì¼ ê²½ë¡œ\n",
    "            detailed_output_path: log_analysis_detailed.xlsx ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê° key-valueê°€ ê°œë³„ í–‰)\n",
    "            combined_output_path: log_analysis_combined.xlsx ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (key, valueê°€ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì—´)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. ì…ë ¥ ë¡œê·¸ íŒŒì¼ ì½ê¸°\n",
    "            self._load_log_file(input_log_path)\n",
    "            \n",
    "            # 2. ìƒì„¸ ë°ì´í„°ë¡œ ë³€í™˜\n",
    "            self._convert_to_detailed()\n",
    "            \n",
    "            # 3. ê²°í•© ë°ì´í„°ë¡œ ë³€í™˜\n",
    "            self._convert_to_combined()\n",
    "            \n",
    "            # 4. ë‘ í˜•íƒœ ì €ì¥\n",
    "            self._save_detailed_excel(detailed_output_path)\n",
    "            self._save_combined_excel(combined_output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "    def _load_log_file(self, log_path: str):\n",
    "        \"\"\"JSON ë¡œê·¸ íŒŒì¼ ë¡œë“œ ë° íŒŒì‹± (ì˜¤ë¥˜ ìë™ ìˆ˜ì • ê¸°ëŠ¥ í¬í•¨)\"\"\"\n",
    "        try:\n",
    "            self.input_data = []\n",
    "            json_errors = []\n",
    "            fixed_count = 0\n",
    "            \n",
    "            with open(log_path, 'r', encoding='utf-8') as file:\n",
    "                for line_no, line in enumerate(file, 1):\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # JSON íŒŒì‹±\n",
    "                        log_entry = json.loads(line)\n",
    "                        \n",
    "                        # REQUEST URLì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ\n",
    "                        request_url = log_entry.get('REQUEST', '')\n",
    "                        parsed_data = self._parse_request_url(request_url)\n",
    "                        \n",
    "                        if parsed_data:\n",
    "                            # ê¸°ë³¸ ë¡œê·¸ ì •ë³´ ì¶”ê°€\n",
    "                            parsed_data['log_line'] = line_no\n",
    "                            parsed_data['logtime'] = log_entry.get('LOGTIME', '')\n",
    "                            parsed_data['clientip'] = log_entry.get('CLIENTIP', '')\n",
    "                            parsed_data['useragent'] = log_entry.get('USERAGENT', '')\n",
    "                            parsed_data['status'] = log_entry.get('STATUS', '')\n",
    "                            \n",
    "                            self.input_data.append(parsed_data)\n",
    "                    \n",
    "                    except json.JSONDecodeError as e:\n",
    "                        json_errors.append((line_no, str(e), line))\n",
    "                        \n",
    "                        # JSON ìˆ˜ì • ì‹œë„\n",
    "                        fixed_line = self._try_fix_json(line)\n",
    "                        if fixed_line:\n",
    "                            try:\n",
    "                                log_entry = json.loads(fixed_line)\n",
    "                                \n",
    "                                # REQUEST URLì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ\n",
    "                                request_url = log_entry.get('REQUEST', '')\n",
    "                                parsed_data = self._parse_request_url(request_url)\n",
    "                                \n",
    "                                if parsed_data:\n",
    "                                    # ê¸°ë³¸ ë¡œê·¸ ì •ë³´ ì¶”ê°€\n",
    "                                    parsed_data['log_line'] = line_no\n",
    "                                    parsed_data['logtime'] = log_entry.get('LOGTIME', '')\n",
    "                                    parsed_data['clientip'] = log_entry.get('CLIENTIP', '')\n",
    "                                    parsed_data['useragent'] = log_entry.get('USERAGENT', '')\n",
    "                                    parsed_data['status'] = log_entry.get('STATUS', '')\n",
    "                                    \n",
    "                                    self.input_data.append(parsed_data)\n",
    "                                    fixed_count += 1\n",
    "                                    print(f\"âœ… JSON ìˆ˜ì • ì„±ê³µ (ë¼ì¸ {line_no})\")\n",
    "                            except:\n",
    "                                # ì¶”ê°€ ìˆ˜ì • ì‹œë„\n",
    "                                fixed_line2 = self._force_fix_json(line)\n",
    "                                if fixed_line2:\n",
    "                                    try:\n",
    "                                        log_entry = json.loads(fixed_line2)\n",
    "                                        \n",
    "                                        # REQUEST URLì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ\n",
    "                                        request_url = log_entry.get('REQUEST', '')\n",
    "                                        parsed_data = self._parse_request_url(request_url)\n",
    "                                        \n",
    "                                        if parsed_data:\n",
    "                                            # ê¸°ë³¸ ë¡œê·¸ ì •ë³´ ì¶”ê°€\n",
    "                                            parsed_data['log_line'] = line_no\n",
    "                                            parsed_data['logtime'] = log_entry.get('LOGTIME', '')\n",
    "                                            parsed_data['clientip'] = log_entry.get('CLIENTIP', '')\n",
    "                                            parsed_data['useragent'] = log_entry.get('USERAGENT', '')\n",
    "                                            parsed_data['status'] = log_entry.get('STATUS', '')\n",
    "                                            \n",
    "                                            self.input_data.append(parsed_data)\n",
    "                                            fixed_count += 1\n",
    "                                            print(f\"âœ… ê°•ì œ JSON ìˆ˜ì • ì„±ê³µ (ë¼ì¸ {line_no})\")\n",
    "                                    except:\n",
    "                                        print(f\"âŒ JSON ìˆ˜ì • ì‹¤íŒ¨ (ë¼ì¸ {line_no}): {e}\")\n",
    "                                else:\n",
    "                                    print(f\"âŒ JSON ìˆ˜ì • ì‹¤íŒ¨ (ë¼ì¸ {line_no}): {e}\")\n",
    "                        else:\n",
    "                            print(f\"âŒ JSON ìˆ˜ì • ì‹¤íŒ¨ (ë¼ì¸ {line_no}): {e}\")\n",
    "            \n",
    "            # ì˜¤ë¥˜ ìš”ì•½ ì¶œë ¥\n",
    "            if json_errors:\n",
    "                print(f\"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ: {len(json_errors)}ê°œ\")\n",
    "                if fixed_count > 0:\n",
    "                    print(f\"âœ… ìˆ˜ì • ì„±ê³µ: {fixed_count}ê°œ\")\n",
    "                print(f\"âœ… ìµœì¢… íŒŒì‹±ëœ ë¡œê·¸: {len(self.input_data)}ê°œ\")\n",
    "            else:\n",
    "                print(f\"âœ… ëª¨ë“  JSON íŒŒì‹± ì„±ê³µ\")\n",
    "            \n",
    "            print(f\"âœ… ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(self.input_data)}ê°œ ë¡œê·¸\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _try_fix_json(self, line: str) -> str:\n",
    "        \"\"\"JSON ìˆ˜ì • ì‹œë„ - 1ì°¨ ìˆ˜ì •\"\"\"\n",
    "        try:\n",
    "            # 1. ë§ˆì§€ë§‰ ì‰¼í‘œ ì œê±°\n",
    "            if line.endswith(',}'):\n",
    "                line = line[:-2] + '}'\n",
    "            elif line.endswith(',]'):\n",
    "                line = line[:-2] + ']'\n",
    "            \n",
    "            # 2. ê°’ ë‚´ë¶€ì˜ ë”°ì˜´í‘œë¥¼ ì‘ì€ë”°ì˜´í‘œë¡œ ë³€ê²½\n",
    "            import re\n",
    "            \n",
    "            # ëª¨ë“  \"key\":\"value with quotes\" íŒ¨í„´ì„ ì°¾ì•„ì„œ ìˆ˜ì •\n",
    "            def fix_quotes_in_value(match):\n",
    "                key = match.group(1)\n",
    "                value = match.group(2)\n",
    "                # ê°’ ë‚´ë¶€ì˜ ëª¨ë“  ë”°ì˜´í‘œë¥¼ ì‘ì€ë”°ì˜´í‘œë¡œ ë³€ê²½\n",
    "                fixed_value = value.replace('\"', \"'\")\n",
    "                return f'\"{key}\":\"{fixed_value}\"'\n",
    "            \n",
    "            # ê°’ì— ë”°ì˜´í‘œê°€ í¬í•¨ëœ íŒ¨í„´ ì°¾ê¸°\n",
    "            problem_pattern = r'\"([^\"]+)\":\"([^\"]*\"[^\"]*)\"'\n",
    "            \n",
    "            # ì—¬ëŸ¬ ë²ˆ ì ìš©í•˜ì—¬ ëª¨ë“  ë¬¸ì œ í•´ê²°\n",
    "            prev_line = \"\"\n",
    "            iterations = 0\n",
    "            while line != prev_line and iterations < 5:\n",
    "                prev_line = line\n",
    "                line = re.sub(problem_pattern, fix_quotes_in_value, line)\n",
    "                iterations += 1\n",
    "            \n",
    "            return line\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _force_fix_json(self, line: str) -> str:\n",
    "        \"\"\"JSON ê°•ì œ ìˆ˜ì • - 2ì°¨ ìˆ˜ì •\"\"\"\n",
    "        try:\n",
    "            # ê¸°ë³¸ ì •ë¦¬\n",
    "            if line.endswith(',}'):\n",
    "                line = line[:-2] + '}'\n",
    "            elif line.endswith(',]'):\n",
    "                line = line[:-2] + ']'\n",
    "            \n",
    "            # JSON êµ¬ì¡° ë¶„ì„í•˜ì—¬ í‚¤-ê°’ ìŒë³„ë¡œ ì²˜ë¦¬\n",
    "            if not (line.startswith('{') and line.endswith('}')):\n",
    "                return None\n",
    "            \n",
    "            # ì¤‘ê´„í˜¸ ì œê±°\n",
    "            content = line[1:-1]\n",
    "            \n",
    "            # í‚¤-ê°’ ìŒë“¤ì„ ë¶„ë¦¬\n",
    "            pairs = []\n",
    "            current_pair = \"\"\n",
    "            bracket_count = 0\n",
    "            in_quotes = False\n",
    "            escape_next = False\n",
    "            \n",
    "            for char in content:\n",
    "                if escape_next:\n",
    "                    current_pair += char\n",
    "                    escape_next = False\n",
    "                    continue\n",
    "                \n",
    "                if char == '\\\\':\n",
    "                    current_pair += char\n",
    "                    escape_next = True\n",
    "                    continue\n",
    "                \n",
    "                if char == '\"' and not escape_next:\n",
    "                    in_quotes = not in_quotes\n",
    "                \n",
    "                if not in_quotes:\n",
    "                    if char in '{}[]':\n",
    "                        bracket_count += 1 if char in '{[' else -1\n",
    "                    elif char == ',' and bracket_count == 0:\n",
    "                        pairs.append(current_pair.strip())\n",
    "                        current_pair = \"\"\n",
    "                        continue\n",
    "                \n",
    "                current_pair += char\n",
    "            \n",
    "            if current_pair.strip():\n",
    "                pairs.append(current_pair.strip())\n",
    "            \n",
    "            # ê° í‚¤-ê°’ ìŒì„ ìˆ˜ì •\n",
    "            fixed_pairs = []\n",
    "            for pair in pairs:\n",
    "                if ':' in pair:\n",
    "                    try:\n",
    "                        # í‚¤ì™€ ê°’ ë¶„ë¦¬ (ì²« ë²ˆì§¸ ì½œë¡  ê¸°ì¤€)\n",
    "                        colon_pos = pair.find(':')\n",
    "                        key_part = pair[:colon_pos].strip()\n",
    "                        value_part = pair[colon_pos+1:].strip()\n",
    "                        \n",
    "                        # ê°’ ë¶€ë¶„ì—ì„œ ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¬¸ìì—´ì¸ì§€ í™•ì¸\n",
    "                        if value_part.startswith('\"') and value_part.endswith('\"'):\n",
    "                            # ê°’ ë‚´ë¶€ì˜ ë”°ì˜´í‘œë¥¼ ì‘ì€ë”°ì˜´í‘œë¡œ ë³€ê²½\n",
    "                            inner_value = value_part[1:-1]  # ì–‘ìª½ ë”°ì˜´í‘œ ì œê±°\n",
    "                            fixed_inner = inner_value.replace('\"', \"'\")\n",
    "                            fixed_value = f'\"{fixed_inner}\"'\n",
    "                            fixed_pairs.append(f'{key_part}:{fixed_value}')\n",
    "                        else:\n",
    "                            fixed_pairs.append(pair)\n",
    "                    except:\n",
    "                        fixed_pairs.append(pair)\n",
    "                else:\n",
    "                    fixed_pairs.append(pair)\n",
    "            \n",
    "            # ë‹¤ì‹œ ì¡°í•©\n",
    "            result = '{' + ','.join(fixed_pairs) + '}'\n",
    "            return result\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _parse_request_url(self, request_url: str) -> Dict:\n",
    "    def _parse_request_url(self, request_url: str) -> Dict:\n",
    "        \"\"\"REQUEST URLì—ì„œ íŒŒë¼ë¯¸í„°ë“¤ì„ ì¶”ì¶œ (ê°œì„ ëœ í•œê¸€ ì²˜ë¦¬)\"\"\"\n",
    "        if not request_url:\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ ë””ì½”ë”©\n",
    "            url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "            parsed_url = urlparse(url)\n",
    "            \n",
    "            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ë™ìœ¼ë¡œ íŒŒì‹± (ë³µì¡í•œ ê°’ ì²˜ë¦¬)\n",
    "            query = parsed_url.query\n",
    "            processed_params = {}\n",
    "            \n",
    "            if not query:\n",
    "                return processed_params\n",
    "            \n",
    "            # &ë¡œ ë¶„í• í•˜ë˜, URL ì¸ì½”ë”©ëœ &(%26)ì€ ë¶„í• í•˜ì§€ ì•ŠìŒ\n",
    "            params = []\n",
    "            current_param = \"\"\n",
    "            i = 0\n",
    "            \n",
    "            while i < len(query):\n",
    "                if query[i:i+1] == '&' and not query[max(0, i-3):i] == '%26':\n",
    "                    if current_param:\n",
    "                        params.append(current_param)\n",
    "                    current_param = \"\"\n",
    "                else:\n",
    "                    current_param += query[i]\n",
    "                i += 1\n",
    "            \n",
    "            if current_param:\n",
    "                params.append(current_param)\n",
    "            \n",
    "            # ê° íŒŒë¼ë¯¸í„°ë¥¼ í‚¤=ê°’ìœ¼ë¡œ ë¶„í• \n",
    "            for param in params:\n",
    "                if '=' not in param:\n",
    "                    continue\n",
    "                    \n",
    "                # ì²« ë²ˆì§¸ = ê¸°ì¤€ìœ¼ë¡œë§Œ ë¶„í•  (ê°’ì— =ì´ í¬í•¨ë  ìˆ˜ ìˆìŒ)\n",
    "                key, value = param.split('=', 1)\n",
    "                \n",
    "                # í‚¤ì™€ ê°’ ë””ì½”ë”©\n",
    "                decoded_key = self._fix_korean_text(key)\n",
    "                decoded_value = self._fix_korean_text(value)\n",
    "                \n",
    "                processed_params[decoded_key] = decoded_value\n",
    "            \n",
    "            return processed_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"URL íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "            # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback\n",
    "            try:\n",
    "                url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "                parsed_url = urlparse(url)\n",
    "                query_params = parse_qs(parsed_url.query, keep_blank_values=True)\n",
    "                \n",
    "                processed_params = {}\n",
    "                for key, value_list in query_params.items():\n",
    "                    raw_value = value_list[0] if value_list else ''\n",
    "                    decoded_value = self._fix_korean_text(raw_value)\n",
    "                    processed_params[key] = decoded_value\n",
    "                \n",
    "                return processed_params\n",
    "            except:\n",
    "                return {}\n",
    "\n",
    "    def _fix_korean_text(self, text: str) -> str:\n",
    "        \"\"\"ê¹¨ì§„ í•œê¸€ì„ ì˜¬ë°”ë¥´ê²Œ ë””ì½”ë”©\"\"\"\n",
    "        try:\n",
    "            # URL ë””ì½”ë”© ë¨¼ì € ì‹œë„\n",
    "            from urllib.parse import unquote\n",
    "            decoded = unquote(text, encoding='utf-8')\n",
    "            \n",
    "            # ê¹¨ì§„ í•œê¸€ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "            if self._has_broken_korean(decoded):\n",
    "                # Latin-1ë¡œ ì¸ì½”ë”© í›„ UTF-8ë¡œ ë””ì½”ë”© (ì¼ë°˜ì ì¸ mojibake í•´ê²°ë²•)\n",
    "                try:\n",
    "                    fixed = decoded.encode('latin-1').decode('utf-8')\n",
    "                    return fixed\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                # ê·¸ë˜ë„ ì•ˆë˜ë©´ ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ì²˜ë¦¬\n",
    "                try:\n",
    "                    # ë¬¸ìì—´ì˜ ê° ë¬¸ìë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜ í›„ UTF-8 ë””ì½”ë”©\n",
    "                    byte_data = bytes([ord(c) for c in decoded if ord(c) < 256])\n",
    "                    fixed = byte_data.decode('utf-8', errors='ignore')\n",
    "                    return fixed\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                # UTF-8 ë°”ì´íŠ¸ íŒ¨í„´ìœ¼ë¡œ ë³µêµ¬ ì‹œë„\n",
    "                try:\n",
    "                    # íŠ¹ì • ê¹¨ì§„ íŒ¨í„´ë“¤ì„ ì§ì ‘ ë§¤í•‘\n",
    "                    broken_patterns = {\n",
    "                        'ÃªÂ·Â¹Ã¬Ã¬Ã«ÂªÂ¨': 'ê·¹ì¥ìš©ëª¨',\n",
    "                        'Ã­Â¥ÃªÂ¸Â°Ã«Â¡Â­ÃªÂ²': 'í–¥ê¸°ë¡­ê²Œ',\n",
    "                        'Ã¬Â¤ÃªÂ¸Â°Ã¬Â¼Ã¬Â´': 'ìˆ˜ê¸°ì¼ì´',\n",
    "                        'Ã¬Ã¬Ã«ÂªÂ¨': 'ìš©ëª¨',\n",
    "                        'ÃªÂ·Â¹': 'ê·¹'\n",
    "                    }\n",
    "                    \n",
    "                    result = decoded\n",
    "                    for broken, fixed in broken_patterns.items():\n",
    "                        result = result.replace(broken, fixed)\n",
    "                    \n",
    "                    if result != decoded:\n",
    "                        return result\n",
    "                        \n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return decoded\n",
    "            \n",
    "        except Exception:\n",
    "            return text\n",
    "\n",
    "    def _has_broken_korean(self, text: str) -> bool:\n",
    "        \"\"\"ê¹¨ì§„ í•œê¸€ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸\"\"\"\n",
    "        broken_patterns = [\n",
    "            'Ã«Â¶', 'Ã¬Ã«', 'ÃªÂ¼Â¬', 'Ã«', 'Ã­Â¼', 'Ã¬Â¸', 'ÃªÂ¸Â°', 'ÃªÂ¸', 'Ã¬Ã¬Â¹', 'Ã¬', 'Ãª',\n",
    "            'ÃªÂ·Â¹Ã¬', 'Ã­Â¥ÃªÂ¸Â°', 'Ã¬Â¤ÃªÂ¸Â°', 'Ã¬Ã¬Ã«', 'Ã«ÂªÂ¨'\n",
    "        ]\n",
    "        return any(pattern in text for pattern in broken_patterns)\n",
    "\n",
    "    def _convert_to_detailed(self):\n",
    "        \"\"\"ë¡œê·¸ ë°ì´í„°ë¥¼ ìƒì„¸ key-value í–‰ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "        self.detailed_data = []\n",
    "        \n",
    "        # ì œì™¸í•  ê¸°ë³¸ í•„ë“œë“¤ (ë©”íƒ€ë°ì´í„°)\n",
    "        exclude_fields = {'log_line', 'logtime', 'clientip', 'useragent', 'status'}\n",
    "        \n",
    "        for log_entry in self.input_data:\n",
    "            log_no = log_entry.get('log_line', 0)\n",
    "            \n",
    "            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ\n",
    "            page_id = log_entry.get('page_id', '')\n",
    "            click_type = log_entry.get('click_type', '')\n",
    "            act_type = log_entry.get('act_type', '')\n",
    "            click_text = log_entry.get('click_text', '')  # click_text ì¶”ê°€\n",
    "            \n",
    "            # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ë“¤ì„ key-value ìŒìœ¼ë¡œ ì²˜ë¦¬\n",
    "            for key, value in log_entry.items():\n",
    "                if key in exclude_fields:\n",
    "                    continue\n",
    "                \n",
    "                # ê¸°ë³¸ í•„ë“œë“¤ì€ ê±´ë„ˆë›°ê¸° (click_textëŠ” ì´ì œ í¬í•¨ë¨)\n",
    "                if key in ['page_id', 'click_type', 'act_type']:\n",
    "                    continue\n",
    "                \n",
    "                self.detailed_data.append({\n",
    "                    'no': log_no,\n",
    "                    'page_id': page_id,\n",
    "                    'click_type': click_type,\n",
    "                    'act_type': act_type,\n",
    "                    'click_text': click_text,\n",
    "                    'key': key,\n",
    "                    'value': str(value) if value is not None else ''\n",
    "                })\n",
    "        \n",
    "        print(f\"âœ… ìƒì„¸ ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(self.detailed_data)}ê°œ key-value ìŒ\")\n",
    "\n",
    "    def _convert_to_combined(self):\n",
    "        \"\"\"ë¡œê·¸ ë°ì´í„°ë¥¼ ê²°í•© í˜•íƒœë¡œ ë³€í™˜ (key, valueê°€ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì—´)\"\"\"\n",
    "        self.combined_data = []\n",
    "        \n",
    "        # ì œì™¸í•  ê¸°ë³¸ í•„ë“œë“¤ (ë©”íƒ€ë°ì´í„°)\n",
    "        exclude_fields = {'log_line', 'logtime', 'clientip', 'useragent', 'status', \n",
    "                         'page_id', 'click_type', 'act_type'}\n",
    "        \n",
    "        for log_entry in self.input_data:\n",
    "            log_no = log_entry.get('log_line', 0)\n",
    "            \n",
    "            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ\n",
    "            page_id = log_entry.get('page_id', '')\n",
    "            click_type = log_entry.get('click_type', '')\n",
    "            act_type = log_entry.get('act_type', '')\n",
    "            click_text = log_entry.get('click_text', '')  # click_text ì¶”ê°€\n",
    "            \n",
    "            # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ë“¤ ìˆ˜ì§‘ (ì´ì œ click_textë„ í¬í•¨ë¨)\n",
    "            keys = []\n",
    "            values = []\n",
    "            \n",
    "            for key, value in log_entry.items():\n",
    "                if key not in exclude_fields:\n",
    "                    keys.append(key)\n",
    "                    values.append(str(value) if value is not None else '')\n",
    "            \n",
    "            # í‚¤ì™€ ê°’ì„ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ê²°í•©\n",
    "            keys_combined = ', '.join(keys) if keys else ''\n",
    "            values_combined = ', '.join(values) if values else ''\n",
    "            \n",
    "            combined_entry = {\n",
    "                'no': log_no,\n",
    "                'page_id': page_id,\n",
    "                'click_type': click_type,\n",
    "                'act_type': act_type,\n",
    "                'keys_combined': keys_combined,\n",
    "                'values_combined': values_combined,\n",
    "                'key_count': len(keys)\n",
    "            }\n",
    "            \n",
    "            # click_textê°€ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "            if click_text:\n",
    "                combined_entry['click_text'] = click_text\n",
    "            \n",
    "            self.combined_data.append(combined_entry)\n",
    "        \n",
    "        print(f\"âœ… ê²°í•© ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(self.combined_data)}ê°œ ë¡œê·¸\")\n",
    "\n",
    "    def _convert_to_combined_deduplicated(self):\n",
    "        \"\"\"ê²°í•© ë°ì´í„°ì—ì„œ ì¤‘ë³µì„ ì œê±°í•œ í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
    "        if not self.combined_data:\n",
    "            print(\"âš ï¸ ê²°í•© ë°ì´í„°ê°€ ì—†ì–´ ì¤‘ë³µ ì œê±°ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        # pandas DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "        df = pd.DataFrame(self.combined_data)\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±° ì „ ê°œìˆ˜\n",
    "        original_count = len(df)\n",
    "        \n",
    "        # page_id, click_type, act_typeì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "        # click_textê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ë¡œ í¬í•¨\n",
    "        dedup_columns = ['page_id', 'click_type', 'act_type']\n",
    "        \n",
    "        # click_text ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ í–‰ì´ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "        if 'click_text' in df.columns and df['click_text'].notna().any() and (df['click_text'] != '').any():\n",
    "            dedup_columns.append('click_text')\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±°\n",
    "        df_deduplicated = df.drop_duplicates(subset=dedup_columns, keep='first')\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±° í›„ ê°œìˆ˜\n",
    "        deduplicated_count = len(df_deduplicated)\n",
    "        removed_count = original_count - deduplicated_count\n",
    "        \n",
    "        # DataFrameì„ ë‹¤ì‹œ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "        self.deduplicated_data = df_deduplicated.to_dict('records')\n",
    "        \n",
    "        print(f\"âœ… ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±° ì™„ë£Œ:\")\n",
    "        print(f\"   ì›ë³¸: {original_count}ê°œ\")\n",
    "        print(f\"   ì¤‘ë³µ ì œê±° í›„: {deduplicated_count}ê°œ\")\n",
    "        print(f\"   ì œê±°ëœ ì¤‘ë³µ: {removed_count}ê°œ\")\n",
    "        print(f\"   ì¤‘ë³µ ì œê±° ê¸°ì¤€: {', '.join(dedup_columns)}\")\n",
    "\n",
    "    def _convert_to_detailed_deduplicated(self):\n",
    "        \"\"\"ìƒì„¸ ë°ì´í„°ì—ì„œ ì¤‘ë³µì„ ì œê±°í•œ í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
    "        if not self.detailed_data:\n",
    "            print(\"âš ï¸ ìƒì„¸ ë°ì´í„°ê°€ ì—†ì–´ ì¤‘ë³µ ì œê±°ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        # pandas DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "        df = pd.DataFrame(self.detailed_data)\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±° ì „ ê°œìˆ˜\n",
    "        original_count = len(df)\n",
    "        \n",
    "        # page_id, click_type, act_type, key, valueë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "        # click_textê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ë¡œ í¬í•¨\n",
    "        dedup_columns = ['page_id', 'click_type', 'act_type', 'key', 'value']\n",
    "        \n",
    "        # click_text ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ í–‰ì´ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "        if 'click_text' in df.columns and df['click_text'].notna().any() and (df['click_text'] != '').any():\n",
    "            dedup_columns.append('click_text')\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±°\n",
    "        df_deduplicated = df.drop_duplicates(subset=dedup_columns, keep='first')\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±° í›„ ê°œìˆ˜\n",
    "        deduplicated_count = len(df_deduplicated)\n",
    "        removed_count = original_count - deduplicated_count\n",
    "        \n",
    "        # DataFrameì„ ë‹¤ì‹œ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "        self.detailed_deduplicated_data = df_deduplicated.to_dict('records')\n",
    "        \n",
    "        print(f\"âœ… ìƒì„¸ í˜•íƒœ ì¤‘ë³µ ì œê±° ì™„ë£Œ:\")\n",
    "        print(f\"   ì›ë³¸: {original_count}ê°œ\")\n",
    "        print(f\"   ì¤‘ë³µ ì œê±° í›„: {deduplicated_count}ê°œ\")\n",
    "        print(f\"   ì œê±°ëœ ì¤‘ë³µ: {removed_count}ê°œ\")\n",
    "        print(f\"   ì¤‘ë³µ ì œê±° ê¸°ì¤€: {', '.join(dedup_columns)}\")\n",
    "\n",
    "    def _extract_click_text_from_keys(self, row):\n",
    "        \"\"\"keys_combinedì—ì„œ click_text ê°’ì„ ì¶”ì¶œí•˜ë ¤ê³  ì‹œë„\"\"\"\n",
    "        try:\n",
    "            keys = row.get('keys_combined', '').split(', ')\n",
    "            values = row.get('values_combined', '').split(', ')\n",
    "            \n",
    "            if len(keys) == len(values):\n",
    "                for i, key in enumerate(keys):\n",
    "                    if key.strip() == 'click_text':\n",
    "                        return values[i].strip() if i < len(values) else ''\n",
    "            return ''\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    def _save_detailed_excel(self, output_path: str):\n",
    "        \"\"\"ìƒì„¸ ë°ì´í„°ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(self.detailed_data)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"âš ï¸ ì €ì¥í•  ìƒì„¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return\n",
    "            \n",
    "            # ì»¬ëŸ¼ ìˆœì„œ ì •ì˜\n",
    "            column_order = ['no', 'page_id', 'click_type', 'act_type', 'click_text', 'key', 'value']\n",
    "            \n",
    "            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "            available_columns = [col for col in column_order if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "            \n",
    "            # Excel íŒŒì¼ ì €ì¥\n",
    "            df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"âœ… ìƒì„¸ Excel íŒŒì¼ ì €ì¥: {output_path}\")\n",
    "            print(f\"   ì´ {len(self.detailed_data)}ê°œ key-value ìŒ\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _save_detailed_deduplicated_excel(self, output_path: str):\n",
    "        \"\"\"ìƒì„¸ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(self.detailed_deduplicated_data)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"âš ï¸ ì €ì¥í•  ìƒì„¸ ì¤‘ë³µ ì œê±° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return\n",
    "            \n",
    "            # ì»¬ëŸ¼ ìˆœì„œ ì •ì˜\n",
    "            column_order = ['no', 'page_id', 'click_type', 'act_type', 'click_text', 'key', 'value']\n",
    "            \n",
    "            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "            available_columns = [col for col in column_order if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "            \n",
    "            # Excel íŒŒì¼ ì €ì¥\n",
    "            df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"âœ… ìƒì„¸ ì¤‘ë³µ ì œê±° Excel íŒŒì¼ ì €ì¥: {output_path}\")\n",
    "            print(f\"   ì´ {len(self.detailed_deduplicated_data)}ê°œ key-value ìŒ (ì¤‘ë³µ ì œê±°ë¨)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _save_combined_deduplicated_excel(self, output_path: str):\n",
    "        \"\"\"ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(self.deduplicated_data)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"âš ï¸ ì €ì¥í•  ì¤‘ë³µ ì œê±° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return\n",
    "            \n",
    "            # ì»¬ëŸ¼ ìˆœì„œ ì •ì˜\n",
    "            column_order = ['no', 'page_id', 'click_type', 'act_type', 'keys_combined', 'values_combined', 'key_count']\n",
    "            \n",
    "            # click_text ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "            if 'click_text' in df.columns:\n",
    "                column_order.insert(4, 'click_text')\n",
    "            \n",
    "            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "            available_columns = [col for col in column_order if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "            \n",
    "            # Excel íŒŒì¼ ì €ì¥\n",
    "            df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"âœ… ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±° Excel íŒŒì¼ ì €ì¥: {output_path}\")\n",
    "            print(f\"   ì´ {len(self.deduplicated_data)}ê°œ ë¡œê·¸ (ì¤‘ë³µ ì œê±°ë¨)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _save_combined_excel(self, output_path: str):\n",
    "        \"\"\"ê²°í•© ë°ì´í„°ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(self.combined_data)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"âš ï¸ ì €ì¥í•  ê²°í•© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return\n",
    "            \n",
    "            # ì»¬ëŸ¼ ìˆœì„œ ì •ì˜\n",
    "            column_order = ['no', 'page_id', 'click_type', 'act_type', 'keys_combined', 'values_combined', 'key_count']\n",
    "            \n",
    "            # click_text ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "            if 'click_text' in df.columns:\n",
    "                column_order.insert(4, 'click_text')\n",
    "            \n",
    "            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "            available_columns = [col for col in column_order if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "            \n",
    "            # Excel íŒŒì¼ ì €ì¥\n",
    "            df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"âœ… ê²°í•© Excel íŒŒì¼ ì €ì¥: {output_path}\")\n",
    "            print(f\"   ì´ {len(self.combined_data)}ê°œ ë¡œê·¸\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_conversion_summary(self) -> Dict:\n",
    "        \"\"\"ë³€í™˜ ê²°ê³¼ ìš”ì•½ ì •ë³´ ë°˜í™˜\"\"\"\n",
    "        if not self.detailed_data:\n",
    "            return {\"error\": \"ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\"}\n",
    "        \n",
    "        # í†µê³„ ê³„ì‚°\n",
    "        total_logs = len(self.input_data)\n",
    "        total_key_values = len(self.detailed_data)\n",
    "        avg_keys_per_log = total_key_values / total_logs if total_logs > 0 else 0\n",
    "        \n",
    "        # ê°€ì¥ ë§ì€ í‚¤ë¥¼ ê°€ì§„ ë¡œê·¸ ì°¾ê¸°\n",
    "        key_counts = {}\n",
    "        for item in self.detailed_data:\n",
    "            log_no = item['no']\n",
    "            key_counts[log_no] = key_counts.get(log_no, 0) + 1\n",
    "        \n",
    "        max_keys = max(key_counts.values()) if key_counts else 0\n",
    "        min_keys = min(key_counts.values()) if key_counts else 0\n",
    "        \n",
    "        # ê³ ìœ í•œ í‚¤ ìˆ˜ ê³„ì‚°\n",
    "        unique_keys = set(item['key'] for item in self.detailed_data)\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±° í†µê³„\n",
    "        combined_count = len(self.combined_data) if self.combined_data else 0\n",
    "        combined_deduplicated_count = len(self.deduplicated_data) if self.deduplicated_data else 0\n",
    "        combined_duplicates_removed = combined_count - combined_deduplicated_count\n",
    "        \n",
    "        detailed_count = len(self.detailed_data) if self.detailed_data else 0\n",
    "        detailed_deduplicated_count = len(self.detailed_deduplicated_data) if self.detailed_deduplicated_data else 0\n",
    "        detailed_duplicates_removed = detailed_count - detailed_deduplicated_count\n",
    "        \n",
    "        return {\n",
    "            \"total_logs\": total_logs,\n",
    "            \"total_key_value_pairs\": total_key_values,\n",
    "            \"average_keys_per_log\": round(avg_keys_per_log, 2),\n",
    "            \"max_keys_in_single_log\": max_keys,\n",
    "            \"min_keys_in_single_log\": min_keys,\n",
    "            \"unique_keys_count\": len(unique_keys),\n",
    "            \"unique_keys\": sorted(list(unique_keys)),\n",
    "            \"combined_logs_count\": combined_count,\n",
    "            \"combined_deduplicated_count\": combined_deduplicated_count,\n",
    "            \"combined_duplicates_removed\": combined_duplicates_removed,\n",
    "            \"detailed_key_value_pairs\": detailed_count,\n",
    "            \"detailed_deduplicated_count\": detailed_deduplicated_count,\n",
    "            \"detailed_duplicates_removed\": detailed_duplicates_removed\n",
    "        }\n",
    "\n",
    "    def preview_data(self, num_rows: int = 10):\n",
    "        \"\"\"ë³€í™˜ëœ ìƒì„¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "        print(f\"\\nğŸ“‹ ìƒì„¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ {num_rows}ê°œ í–‰):\")\n",
    "        print(\"-\" * 140)\n",
    "        print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Click Text':15} | {'Key':20} | {'Value':25}\")\n",
    "        print(\"-\" * 140)\n",
    "        \n",
    "        if self.detailed_data:\n",
    "            for item in self.detailed_data[:num_rows]:\n",
    "                page_id = item['page_id'][:25] if len(item['page_id']) > 25 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                click_text = str(item.get('click_text', ''))[:15] if len(str(item.get('click_text', ''))) > 15 else str(item.get('click_text', ''))\n",
    "                key = item['key'][:20] if len(item['key']) > 20 else item['key']\n",
    "                value = item['value'][:25] if len(item['value']) > 25 else item['value']\n",
    "                \n",
    "                print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {click_text:15} | {key:20} | {value:25}\")\n",
    "            \n",
    "            if len(self.detailed_data) > num_rows:\n",
    "                print(f\"... ì™¸ {len(self.detailed_data) - num_rows}ê°œ í–‰\")\n",
    "        else:\n",
    "            print(\"ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def preview_combined_deduplicated_data(self, num_rows: int = 5):\n",
    "        \"\"\"ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "        print(f\"\\nğŸ“‹ ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ {num_rows}ê°œ í–‰):\")\n",
    "        print(\"-\" * 160)\n",
    "        \n",
    "        # í—¤ë” ë™ì  ìƒì„±\n",
    "        has_click_text = self.deduplicated_data and any('click_text' in item for item in self.deduplicated_data[:3])\n",
    "        if has_click_text:\n",
    "            print(f\"{'No':3} | {'Page ID':20} | {'Click Type':12} | {'Act Type':12} | {'Click Text':12} | {'Keys':25} | {'Values':25} | {'Count':5}\")\n",
    "        else:\n",
    "            print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Keys':30} | {'Values':30} | {'Count':5}\")\n",
    "        \n",
    "        print(\"-\" * 160)\n",
    "        \n",
    "        if self.deduplicated_data:\n",
    "            for item in self.deduplicated_data[:num_rows]:\n",
    "                page_id = item['page_id'][:20] if len(item['page_id']) > 20 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                \n",
    "                if 'click_text' in item and item['click_text']:\n",
    "                    click_text = item['click_text'][:12] if len(str(item['click_text'])) > 12 else str(item['click_text'])\n",
    "                    keys = item['keys_combined'][:25] if len(item['keys_combined']) > 25 else item['keys_combined']\n",
    "                    values = item['values_combined'][:25] if len(item['values_combined']) > 25 else item['values_combined']\n",
    "                    print(f\"{item['no']:3} | {page_id:20} | {click_type:12} | {act_type:12} | {click_text:12} | {keys:25} | {values:25} | {item['key_count']:5}\")\n",
    "                else:\n",
    "                    keys = item['keys_combined'][:25] if len(item['keys_combined']) > 25 else item['keys_combined']\n",
    "                    values = item['values_combined'][:25] if len(item['values_combined']) > 25 else item['values_combined']\n",
    "                    print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {keys:25} | {values:25} | {item['key_count']:5}\")\n",
    "            \n",
    "            if len(self.deduplicated_data) > num_rows:\n",
    "                print(f\"... ì™¸ {len(self.deduplicated_data) - num_rows}ê°œ í–‰\")\n",
    "        else:\n",
    "            print(\"ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def preview_detailed_deduplicated_data(self, num_rows: int = 10):\n",
    "        \"\"\"ìƒì„¸ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "        print(f\"\\nğŸ“‹ ìƒì„¸ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ {num_rows}ê°œ í–‰):\")\n",
    "        print(\"-\" * 140)\n",
    "        print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Click Text':15} | {'Key':20} | {'Value':25}\")\n",
    "        print(\"-\" * 140)\n",
    "        \n",
    "        if self.detailed_deduplicated_data:\n",
    "            for item in self.detailed_deduplicated_data[:num_rows]:\n",
    "                page_id = item['page_id'][:25] if len(item['page_id']) > 25 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                click_text = str(item.get('click_text', ''))[:15] if len(str(item.get('click_text', ''))) > 15 else str(item.get('click_text', ''))\n",
    "                key = item['key'][:20] if len(item['key']) > 20 else item['key']\n",
    "                value = item['value'][:25] if len(item['value']) > 25 else item['value']\n",
    "                \n",
    "                print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {click_text:15} | {key:20} | {value:25}\")\n",
    "            \n",
    "            if len(self.detailed_deduplicated_data) > num_rows:\n",
    "                print(f\"... ì™¸ {len(self.detailed_deduplicated_data) - num_rows}ê°œ í–‰\")\n",
    "        else:\n",
    "            print(\"ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def preview_combined_deduplicated_data(self, num_rows: int = 5):\n",
    "        \"\"\"ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "        print(f\"\\nğŸ“‹ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ {num_rows}ê°œ í–‰):\")\n",
    "        print(\"-\" * 160)\n",
    "        \n",
    "        # í—¤ë” ë™ì  ìƒì„±\n",
    "        has_click_text = self.deduplicated_data and any('click_text' in item for item in self.deduplicated_data[:3])\n",
    "        if has_click_text:\n",
    "            print(f\"{'No':3} | {'Page ID':20} | {'Click Type':12} | {'Act Type':12} | {'Click Text':12} | {'Keys':25} | {'Values':25} | {'Count':5}\")\n",
    "        else:\n",
    "            print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Keys':30} | {'Values':30} | {'Count':5}\")\n",
    "        \n",
    "        print(\"-\" * 160)\n",
    "        \n",
    "        if self.deduplicated_data:\n",
    "            for item in self.deduplicated_data[:num_rows]:\n",
    "                page_id = item['page_id'][:20] if len(item['page_id']) > 20 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                \n",
    "                if 'click_text' in item and item['click_text']:\n",
    "                    click_text = item['click_text'][:12] if len(str(item['click_text'])) > 12 else str(item['click_text'])\n",
    "                    keys = item['keys_combined'][:25] if len(item['keys_combined']) > 25 else item['keys_combined']\n",
    "                    values = item['values_combined'][:25] if len(item['values_combined']) > 25 else item['values_combined']\n",
    "                    print(f\"{item['no']:3} | {page_id:20} | {click_type:12} | {act_type:12} | {click_text:12} | {keys:25} | {values:25} | {item['key_count']:5}\")\n",
    "                else:\n",
    "                    keys = item['keys_combined'][:25] if len(item['keys_combined']) > 25 else item['keys_combined']\n",
    "                    values = item['values_combined'][:25] if len(item['values_combined']) > 25 else item['values_combined']\n",
    "                    print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {keys:25} | {values:25} | {item['key_count']:5}\")\n",
    "            \n",
    "            if len(self.deduplicated_data) > num_rows:\n",
    "                print(f\"... ì™¸ {len(self.deduplicated_data) - num_rows}ê°œ í–‰\")\n",
    "        else:\n",
    "            print(\"ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def preview_combined_data(self, num_rows: int = 5):\n",
    "        \"\"\"ê²°í•© ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "        print(f\"\\nğŸ“‹ ê²°í•© ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ {num_rows}ê°œ í–‰):\")\n",
    "        print(\"-\" * 150)\n",
    "        print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Keys':40} | {'Values':40} | {'Count':5}\")\n",
    "        print(\"-\" * 150)\n",
    "        \n",
    "        if self.combined_data:\n",
    "            for item in self.combined_data[:num_rows]:\n",
    "                page_id = item['page_id'][:25] if len(item['page_id']) > 25 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                keys = item['keys_combined'][:40] if len(item['keys_combined']) > 40 else item['keys_combined']\n",
    "                values = item['values_combined'][:40] if len(item['values_combined']) > 40 else item['values_combined']\n",
    "                \n",
    "                print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {keys:40} | {values:40} | {item['key_count']:5}\")\n",
    "            \n",
    "            if len(self.combined_data) > num_rows:\n",
    "                print(f\"... ì™¸ {len(self.combined_data) - num_rows}ê°œ í–‰\")\n",
    "        else:\n",
    "            print(\"ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def get_unique_keys_analysis(self):\n",
    "        \"\"\"ê³ ìœ  í‚¤ ë¶„ì„\"\"\"\n",
    "        if not self.detailed_data:\n",
    "            print(\"ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        # í‚¤ë³„ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "        key_frequency = {}\n",
    "        for item in self.detailed_data:\n",
    "            key = item['key']\n",
    "            key_frequency[key] = key_frequency.get(key, 0) + 1\n",
    "        \n",
    "        # ë¹ˆë„ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "        sorted_keys = sorted(key_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š í‚¤ ë¶„ì„ (ì´ {len(sorted_keys)}ê°œ ê³ ìœ  í‚¤):\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Key':30} | {'ë¹ˆë„ìˆ˜':10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for key, freq in sorted_keys[:20]:  # ìƒìœ„ 20ê°œë§Œ í‘œì‹œ\n",
    "            key_display = key[:30] if len(key) > 30 else key\n",
    "            print(f\"{key_display:30} | {freq:10}\")\n",
    "        \n",
    "        if len(sorted_keys) > 20:\n",
    "            print(f\"... ì™¸ {len(sorted_keys) - 20}ê°œ í‚¤\")\n",
    "\n",
    "\n",
    "def run_log_converter_example():\n",
    "    \"\"\"ë¡œê·¸ ë³€í™˜ê¸° ì‚¬ìš© ì˜ˆì‹œ\"\"\"\n",
    "    converter = LogToDetailedConverter()\n",
    "\n",
    "    print(\"=== JSON ë¡œê·¸ë¥¼ Excel í˜•íƒœë¡œ ë³€í™˜ (JSON ì˜¤ë¥˜ ìë™ ìˆ˜ì • + ì¤‘ë³µ ì œê±° ê¸°ëŠ¥ í¬í•¨) ===\\n\")\n",
    "    \n",
    "    # ë¡œê·¸ íŒŒì¼ì„ ë„¤ ê°€ì§€ í˜•íƒœë¡œ ë³€í™˜\n",
    "    converter.convert_log_to_all_formats(\n",
    "        \"log_file.txt\",                              # ì…ë ¥ JSON ë¡œê·¸ íŒŒì¼\n",
    "        \"log_analysis_detailed.xlsx\",                # ìƒì„¸ í˜•íƒœ ì¶œë ¥\n",
    "        \"log_analysis_detailed_deduplicated.xlsx\",   # ìƒì„¸ í˜•íƒœ ì¤‘ë³µ ì œê±° ì¶œë ¥\n",
    "        \"log_analysis_combined.xlsx\",                # ê²°í•© í˜•íƒœ ì¶œë ¥\n",
    "        \"log_analysis_combined_deduplicated.xlsx\"    # ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±° ì¶œë ¥\n",
    "    )\n",
    "    \n",
    "    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "    summary = converter.get_conversion_summary()\n",
    "    print(f\"\\nğŸ“Š ë³€í™˜ ê²°ê³¼ ìš”ì•½:\")\n",
    "    for key, value in summary.items():\n",
    "        if key != 'unique_keys':  # unique_keysëŠ” ë„ˆë¬´ ê¸¸ì–´ì„œ ë³„ë„ ì¶œë ¥\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "    converter.preview_data(10)\n",
    "    converter.preview_detailed_deduplicated_data(10)\n",
    "    converter.preview_combined_data(5)\n",
    "    converter.preview_combined_deduplicated_data(5)\n",
    "    \n",
    "    # í‚¤ ë¶„ì„\n",
    "    converter.get_unique_keys_analysis()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_log_converter_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JSON ë¡œê·¸ë¥¼ Excel í˜•íƒœë¡œ ë³€í™˜ (JSON ì˜¤ë¥˜ ìë™ ìˆ˜ì • + ì¤‘ë³µ ì œê±° ê¸°ëŠ¥ í¬í•¨) ===\n",
      "\n",
      "âœ… ê°•ì œ JSON ìˆ˜ì • ì„±ê³µ (ë¼ì¸ 1)\n",
      "âœ… ê°•ì œ JSON ìˆ˜ì • ì„±ê³µ (ë¼ì¸ 2)\n",
      "âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ: 2ê°œ\n",
      "âœ… ìˆ˜ì • ì„±ê³µ: 2ê°œ\n",
      "âœ… ìµœì¢… íŒŒì‹±ëœ ë¡œê·¸: 3ê°œ\n",
      "âœ… ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: 3ê°œ ë¡œê·¸\n",
      "âœ… ìƒì„¸ ë°ì´í„° ë³€í™˜ ì™„ë£Œ: 31ê°œ key-value ìŒ\n",
      "âœ… ê²°í•© ë°ì´í„° ë³€í™˜ ì™„ë£Œ: 3ê°œ ë¡œê·¸\n",
      "âœ… ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±° ì™„ë£Œ:\n",
      "   ì›ë³¸: 3ê°œ\n",
      "   ì¤‘ë³µ ì œê±° í›„: 3ê°œ\n",
      "   ì œê±°ëœ ì¤‘ë³µ: 0ê°œ\n",
      "   ì¤‘ë³µ ì œê±° ê¸°ì¤€: page_id, click_type, act_type, click_text\n",
      "âœ… ìƒì„¸ í˜•íƒœ ì¤‘ë³µ ì œê±° ì™„ë£Œ:\n",
      "   ì›ë³¸: 31ê°œ\n",
      "   ì¤‘ë³µ ì œê±° í›„: 31ê°œ\n",
      "   ì œê±°ëœ ì¤‘ë³µ: 0ê°œ\n",
      "   ì¤‘ë³µ ì œê±° ê¸°ì¤€: page_id, click_type, act_type, key, value, click_text\n",
      "âœ… ìƒì„¸ Excel íŒŒì¼ ì €ì¥: ./result/log_analysis_detailed.xlsx\n",
      "   ì´ 31ê°œ key-value ìŒ\n",
      "âœ… ìƒì„¸ ì¤‘ë³µ ì œê±° Excel íŒŒì¼ ì €ì¥: ./result/log_analysis_detailed_deduplicated.xlsx\n",
      "   ì´ 31ê°œ key-value ìŒ (ì¤‘ë³µ ì œê±°ë¨)\n",
      "âœ… ê²°í•© Excel íŒŒì¼ ì €ì¥: ./result/log_analysis_combined.xlsx\n",
      "   ì´ 3ê°œ ë¡œê·¸\n",
      "âœ… ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±° Excel íŒŒì¼ ì €ì¥: ./result/log_analysis_combined_deduplicated.xlsx\n",
      "   ì´ 3ê°œ ë¡œê·¸ (ì¤‘ë³µ ì œê±°ë¨)\n",
      "\n",
      "ğŸ“Š ë³€í™˜ ê²°ê³¼ ìš”ì•½:\n",
      "   total_logs: 3\n",
      "   total_key_value_pairs: 31\n",
      "   average_keys_per_log: 10.33\n",
      "   max_keys_in_single_log: 11\n",
      "   min_keys_in_single_log: 9\n",
      "   unique_keys_count: 19\n",
      "   combined_logs_count: 3\n",
      "   combined_deduplicated_count: 3\n",
      "   combined_duplicates_removed: 0\n",
      "   detailed_key_value_pairs: 31\n",
      "   detailed_deduplicated_count: 31\n",
      "   detailed_duplicates_removed: 0\n",
      "\n",
      "ğŸ“‹ ìƒì„¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 10ê°œ í–‰):\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "No  | Page ID                   | Click Type   | Act Type     | Click Text      | Key                  | Value                    \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | categoryId           | 1001                     \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | channel              | Rround                   \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | page_url             | https://life-dev.hectoinn\n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | reffer_url           | https://life-dev.hectoinn\n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | area_name            | ì—°ì˜ˆ                       \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | tab_name             | ì—°ì˜ˆ                       \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | el_order             | 2                        \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | list_name            | ì—°ì˜ˆ                       \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | article_id           | 10515                    \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | article_title        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ë§¹ìŠ¹ì§€, ê³ ì•¡ ìŠ¤í°\n",
      "... ì™¸ 21ê°œ í–‰\n",
      "\n",
      "ğŸ“‹ ìƒì„¸ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 10ê°œ í–‰):\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "No  | Page ID                   | Click Type   | Act Type     | Click Text      | Key                  | Value                    \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | categoryId           | 1001                     \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | channel              | Rround                   \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | page_url             | https://life-dev.hectoinn\n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | reffer_url           | https://life-dev.hectoinn\n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | area_name            | ì—°ì˜ˆ                       \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | tab_name             | ì—°ì˜ˆ                       \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | el_order             | 2                        \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | list_name            | ì—°ì˜ˆ                       \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | article_id           | 10515                    \n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ | article_title        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ê³ 'â€¦ë§¹ìŠ¹ì§€, ê³ ì•¡ ìŠ¤í°\n",
      "... ì™¸ 21ê°œ í–‰\n",
      "\n",
      "ğŸ“‹ ê²°í•© ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 5ê°œ í–‰):\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "No  | Page ID                   | Click Type   | Act Type     | Keys                                     | Values                                   | Count\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  1 | life-dev/news/detail/1051 | ë‰´ìŠ¤           | click        | categoryId, channel, page_url, reffer_ur | 1001, Rround, https://life-dev.hectoinno |    11\n",
      "  2 | life-dev/news/detail/1053 | ë‰´ìŠ¤           | click        | categoryId, channel, page_url, reffer_ur | 1002, Rround, https://life-dev.hectoinno |     9\n",
      "  3 | life-dev/search           | ìƒí’ˆ           | click        | channel, page_url, reffer_url, kwd, prd_ | Rround, https://life-dev.hectoinnovation |    11\n",
      "\n",
      "ğŸ“‹ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 5ê°œ í–‰):\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "No  | Page ID              | Click Type   | Act Type     | Click Text   | Keys                      | Values                    | Count\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  1 | life-dev/news/detail | ë‰´ìŠ¤           | click        | '3ì¼ì— 4ì²œë§Œì› ì¤€ë‹¤ | categoryId, channel, page | 1001, Rround, https://lif |    11\n",
      "  2 | life-dev/news/detail | ë‰´ìŠ¤           | click        | ì •ì¹˜ì°¨ê¸° ëŒ€í†µë ¹ ì§‘ë¬´ì‹¤ | categoryId, channel, page | 1002, Rround, https://lif |     9\n",
      "  3 | life-dev/search      | ìƒí’ˆ           | click        | [ì±Œë¦°ì§€ë”œ]ì˜¤ì•„ ë´í‹°ìŠ¤ | channel, page_url, reffer | Rround, https://life-dev. |    11\n",
      "\n",
      "ğŸ“Š í‚¤ ë¶„ì„ (ì´ 19ê°œ ê³ ìœ  í‚¤):\n",
      "--------------------------------------------------\n",
      "Key                            | ë¹ˆë„ìˆ˜       \n",
      "--------------------------------------------------\n",
      "channel                        |          3\n",
      "page_url                       |          3\n",
      "reffer_url                     |          3\n",
      "tab_name                       |          3\n",
      "categoryId                     |          2\n",
      "article_id                     |          2\n",
      "article_title                  |          2\n",
      "list_id                        |          2\n",
      "area_name                      |          1\n",
      "el_order                       |          1\n",
      "list_name                      |          1\n",
      "list_order                     |          1\n",
      "kwd                            |          1\n",
      "prd_code                       |          1\n",
      "prd_name                       |          1\n",
      "prd_brand                      |          1\n",
      "prd_price_final                |          1\n",
      "prd_order                      |          1\n",
      "prd_is_ad                      |          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import re\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "\n",
    "class LogToDetailedConverter:\n",
    "    \"\"\"JSON ë¡œê·¸ë¥¼ ë‹¤ì–‘í•œ Excel í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
    "    def __init__(self):\n",
    "        self.input_data = []\n",
    "        self.detailed_data = []\n",
    "        self.detailed_deduplicated_data = []\n",
    "        self.combined_data = []\n",
    "        self.deduplicated_data = []\n",
    "\n",
    "    def convert_log_to_all_formats(self, input_log_path: str, \n",
    "                                 detailed_output_path: str, \n",
    "                                 detailed_deduplicated_output_path: str,\n",
    "                                 combined_output_path: str,\n",
    "                                 combined_deduplicated_output_path: str):\n",
    "        \"\"\"\n",
    "        JSON ë¡œê·¸ íŒŒì¼ì„ ìƒì„¸ í˜•íƒœì™€ ê²°í•© í˜•íƒœ ë‘ ê°€ì§€ë¡œ ë³€í™˜\n",
    "        \n",
    "        Args:\n",
    "            input_log_path: log_file.txt íŒŒì¼ ê²½ë¡œ\n",
    "            detailed_output_path: log_analysis_detailed.xlsx ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê° key-valueê°€ ê°œë³„ í–‰)\n",
    "            combined_output_path: log_analysis_combined.xlsx ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (key, valueê°€ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì—´)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. ì…ë ¥ ë¡œê·¸ íŒŒì¼ ì½ê¸°\n",
    "            self._load_log_file(input_log_path)\n",
    "            \n",
    "            # 2. ìƒì„¸ ë°ì´í„°ë¡œ ë³€í™˜\n",
    "            self._convert_to_detailed()\n",
    "            \n",
    "            # 3. ê²°í•© ë°ì´í„°ë¡œ ë³€í™˜\n",
    "            self._convert_to_combined()\n",
    "            \n",
    "            # 4. ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±°\n",
    "            self._convert_to_combined_deduplicated()\n",
    "            \n",
    "            # 5. ìƒì„¸ í˜•íƒœ ì¤‘ë³µ ì œê±°\n",
    "            self._convert_to_detailed_deduplicated()\n",
    "            \n",
    "            # 6. ë„¤ í˜•íƒœ ëª¨ë‘ ì €ì¥\n",
    "            self._save_detailed_excel(detailed_output_path)\n",
    "            self._save_detailed_deduplicated_excel(detailed_deduplicated_output_path)\n",
    "            self._save_combined_excel(combined_output_path)\n",
    "            self._save_combined_deduplicated_excel(combined_deduplicated_output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "    def convert_log_to_both_formats(self, input_log_path: str, \n",
    "                                  detailed_output_path: str, \n",
    "                                  combined_output_path: str):\n",
    "        \"\"\"\n",
    "        ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ - ì¤‘ë³µ ì œê±° ì—†ì´ ë‘ ê°€ì§€ í˜•íƒœë§Œ ë³€í™˜\n",
    "        \n",
    "        Args:\n",
    "            input_log_path: log_file.txt íŒŒì¼ ê²½ë¡œ\n",
    "            detailed_output_path: log_analysis_detailed.xlsx ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê° key-valueê°€ ê°œë³„ í–‰)\n",
    "            combined_output_path: log_analysis_combined.xlsx ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (key, valueê°€ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì—´)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. ì…ë ¥ ë¡œê·¸ íŒŒì¼ ì½ê¸°\n",
    "            self._load_log_file(input_log_path)\n",
    "            \n",
    "            # 2. ìƒì„¸ ë°ì´í„°ë¡œ ë³€í™˜\n",
    "            self._convert_to_detailed()\n",
    "            \n",
    "            # 3. ê²°í•© ë°ì´í„°ë¡œ ë³€í™˜\n",
    "            self._convert_to_combined()\n",
    "            \n",
    "            # 4. ë‘ í˜•íƒœ ì €ì¥\n",
    "            self._save_detailed_excel(detailed_output_path)\n",
    "            self._save_combined_excel(combined_output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "    def _load_log_file(self, log_path: str):\n",
    "        \"\"\"JSON ë¡œê·¸ íŒŒì¼ ë¡œë“œ ë° íŒŒì‹± (ì˜¤ë¥˜ ìë™ ìˆ˜ì • ê¸°ëŠ¥ í¬í•¨)\"\"\"\n",
    "        try:\n",
    "            self.input_data = []\n",
    "            json_errors = []\n",
    "            fixed_count = 0\n",
    "            \n",
    "            with open(log_path, 'r', encoding='utf-8') as file:\n",
    "                for line_no, line in enumerate(file, 1):\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # JSON íŒŒì‹±\n",
    "                        log_entry = json.loads(line)\n",
    "                        \n",
    "                        # REQUEST URLì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ\n",
    "                        request_url = log_entry.get('REQUEST', '')\n",
    "                        parsed_data = self._parse_request_url(request_url)\n",
    "                        \n",
    "                        if parsed_data:\n",
    "                            # ê¸°ë³¸ ë¡œê·¸ ì •ë³´ ì¶”ê°€\n",
    "                            parsed_data['log_line'] = line_no\n",
    "                            parsed_data['logtime'] = log_entry.get('LOGTIME', '')\n",
    "                            parsed_data['clientip'] = log_entry.get('CLIENTIP', '')\n",
    "                            parsed_data['useragent'] = log_entry.get('USERAGENT', '')\n",
    "                            parsed_data['status'] = log_entry.get('STATUS', '')\n",
    "                            \n",
    "                            self.input_data.append(parsed_data)\n",
    "                    \n",
    "                    except json.JSONDecodeError as e:\n",
    "                        json_errors.append((line_no, str(e), line))\n",
    "                        \n",
    "                        # JSON ìˆ˜ì • ì‹œë„\n",
    "                        fixed_line = self._try_fix_json(line)\n",
    "                        if fixed_line:\n",
    "                            try:\n",
    "                                log_entry = json.loads(fixed_line)\n",
    "                                \n",
    "                                # REQUEST URLì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ\n",
    "                                request_url = log_entry.get('REQUEST', '')\n",
    "                                parsed_data = self._parse_request_url(request_url)\n",
    "                                \n",
    "                                if parsed_data:\n",
    "                                    # ê¸°ë³¸ ë¡œê·¸ ì •ë³´ ì¶”ê°€\n",
    "                                    parsed_data['log_line'] = line_no\n",
    "                                    parsed_data['logtime'] = log_entry.get('LOGTIME', '')\n",
    "                                    parsed_data['clientip'] = log_entry.get('CLIENTIP', '')\n",
    "                                    parsed_data['useragent'] = log_entry.get('USERAGENT', '')\n",
    "                                    parsed_data['status'] = log_entry.get('STATUS', '')\n",
    "                                    \n",
    "                                    self.input_data.append(parsed_data)\n",
    "                                    fixed_count += 1\n",
    "                                    print(f\"âœ… JSON ìˆ˜ì • ì„±ê³µ (ë¼ì¸ {line_no})\")\n",
    "                            except:\n",
    "                                # ì¶”ê°€ ìˆ˜ì • ì‹œë„\n",
    "                                fixed_line2 = self._force_fix_json(line)\n",
    "                                if fixed_line2:\n",
    "                                    try:\n",
    "                                        log_entry = json.loads(fixed_line2)\n",
    "                                        \n",
    "                                        # REQUEST URLì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ\n",
    "                                        request_url = log_entry.get('REQUEST', '')\n",
    "                                        parsed_data = self._parse_request_url(request_url)\n",
    "                                        \n",
    "                                        if parsed_data:\n",
    "                                            # ê¸°ë³¸ ë¡œê·¸ ì •ë³´ ì¶”ê°€\n",
    "                                            parsed_data['log_line'] = line_no\n",
    "                                            parsed_data['logtime'] = log_entry.get('LOGTIME', '')\n",
    "                                            parsed_data['clientip'] = log_entry.get('CLIENTIP', '')\n",
    "                                            parsed_data['useragent'] = log_entry.get('USERAGENT', '')\n",
    "                                            parsed_data['status'] = log_entry.get('STATUS', '')\n",
    "                                            \n",
    "                                            self.input_data.append(parsed_data)\n",
    "                                            fixed_count += 1\n",
    "                                            print(f\"âœ… ê°•ì œ JSON ìˆ˜ì • ì„±ê³µ (ë¼ì¸ {line_no})\")\n",
    "                                    except:\n",
    "                                        print(f\"âŒ JSON ìˆ˜ì • ì‹¤íŒ¨ (ë¼ì¸ {line_no}): {e}\")\n",
    "                                else:\n",
    "                                    print(f\"âŒ JSON ìˆ˜ì • ì‹¤íŒ¨ (ë¼ì¸ {line_no}): {e}\")\n",
    "                        else:\n",
    "                            print(f\"âŒ JSON ìˆ˜ì • ì‹¤íŒ¨ (ë¼ì¸ {line_no}): {e}\")\n",
    "            \n",
    "            # ì˜¤ë¥˜ ìš”ì•½ ì¶œë ¥\n",
    "            if json_errors:\n",
    "                print(f\"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ: {len(json_errors)}ê°œ\")\n",
    "                if fixed_count > 0:\n",
    "                    print(f\"âœ… ìˆ˜ì • ì„±ê³µ: {fixed_count}ê°œ\")\n",
    "                print(f\"âœ… ìµœì¢… íŒŒì‹±ëœ ë¡œê·¸: {len(self.input_data)}ê°œ\")\n",
    "            else:\n",
    "                print(f\"âœ… ëª¨ë“  JSON íŒŒì‹± ì„±ê³µ\")\n",
    "            \n",
    "            print(f\"âœ… ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(self.input_data)}ê°œ ë¡œê·¸\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _try_fix_json(self, line: str) -> str:\n",
    "        \"\"\"JSON ìˆ˜ì • ì‹œë„ - 1ì°¨ ìˆ˜ì •\"\"\"\n",
    "        try:\n",
    "            # 1. ë§ˆì§€ë§‰ ì‰¼í‘œ ì œê±°\n",
    "            if line.endswith(',}'):\n",
    "                line = line[:-2] + '}'\n",
    "            elif line.endswith(',]'):\n",
    "                line = line[:-2] + ']'\n",
    "            \n",
    "            # 2. ê°’ ë‚´ë¶€ì˜ ë”°ì˜´í‘œë¥¼ ì‘ì€ë”°ì˜´í‘œë¡œ ë³€ê²½\n",
    "            import re\n",
    "            \n",
    "            # ëª¨ë“  \"key\":\"value with quotes\" íŒ¨í„´ì„ ì°¾ì•„ì„œ ìˆ˜ì •\n",
    "            def fix_quotes_in_value(match):\n",
    "                key = match.group(1)\n",
    "                value = match.group(2)\n",
    "                # ê°’ ë‚´ë¶€ì˜ ëª¨ë“  ë”°ì˜´í‘œë¥¼ ì‘ì€ë”°ì˜´í‘œë¡œ ë³€ê²½\n",
    "                fixed_value = value.replace('\"', \"'\")\n",
    "                return f'\"{key}\":\"{fixed_value}\"'\n",
    "            \n",
    "            # ê°’ì— ë”°ì˜´í‘œê°€ í¬í•¨ëœ íŒ¨í„´ ì°¾ê¸°\n",
    "            problem_pattern = r'\"([^\"]+)\":\"([^\"]*\"[^\"]*)\"'\n",
    "            \n",
    "            # ì—¬ëŸ¬ ë²ˆ ì ìš©í•˜ì—¬ ëª¨ë“  ë¬¸ì œ í•´ê²°\n",
    "            prev_line = \"\"\n",
    "            iterations = 0\n",
    "            while line != prev_line and iterations < 5:\n",
    "                prev_line = line\n",
    "                line = re.sub(problem_pattern, fix_quotes_in_value, line)\n",
    "                iterations += 1\n",
    "            \n",
    "            return line\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _force_fix_json(self, line: str) -> str:\n",
    "        \"\"\"JSON ê°•ì œ ìˆ˜ì • - 2ì°¨ ìˆ˜ì •\"\"\"\n",
    "        try:\n",
    "            # ê¸°ë³¸ ì •ë¦¬\n",
    "            if line.endswith(',}'):\n",
    "                line = line[:-2] + '}'\n",
    "            elif line.endswith(',]'):\n",
    "                line = line[:-2] + ']'\n",
    "            \n",
    "            # JSON êµ¬ì¡° ë¶„ì„í•˜ì—¬ í‚¤-ê°’ ìŒë³„ë¡œ ì²˜ë¦¬\n",
    "            if not (line.startswith('{') and line.endswith('}')):\n",
    "                return None\n",
    "            \n",
    "            # ì¤‘ê´„í˜¸ ì œê±°\n",
    "            content = line[1:-1]\n",
    "            \n",
    "            # í‚¤-ê°’ ìŒë“¤ì„ ë¶„ë¦¬\n",
    "            pairs = []\n",
    "            current_pair = \"\"\n",
    "            bracket_count = 0\n",
    "            in_quotes = False\n",
    "            escape_next = False\n",
    "            \n",
    "            for char in content:\n",
    "                if escape_next:\n",
    "                    current_pair += char\n",
    "                    escape_next = False\n",
    "                    continue\n",
    "                \n",
    "                if char == '\\\\':\n",
    "                    current_pair += char\n",
    "                    escape_next = True\n",
    "                    continue\n",
    "                \n",
    "                if char == '\"' and not escape_next:\n",
    "                    in_quotes = not in_quotes\n",
    "                \n",
    "                if not in_quotes:\n",
    "                    if char in '{}[]':\n",
    "                        bracket_count += 1 if char in '{[' else -1\n",
    "                    elif char == ',' and bracket_count == 0:\n",
    "                        pairs.append(current_pair.strip())\n",
    "                        current_pair = \"\"\n",
    "                        continue\n",
    "                \n",
    "                current_pair += char\n",
    "            \n",
    "            if current_pair.strip():\n",
    "                pairs.append(current_pair.strip())\n",
    "            \n",
    "            # ê° í‚¤-ê°’ ìŒì„ ìˆ˜ì •\n",
    "            fixed_pairs = []\n",
    "            for pair in pairs:\n",
    "                if ':' in pair:\n",
    "                    try:\n",
    "                        # í‚¤ì™€ ê°’ ë¶„ë¦¬ (ì²« ë²ˆì§¸ ì½œë¡  ê¸°ì¤€)\n",
    "                        colon_pos = pair.find(':')\n",
    "                        key_part = pair[:colon_pos].strip()\n",
    "                        value_part = pair[colon_pos+1:].strip()\n",
    "                        \n",
    "                        # ê°’ ë¶€ë¶„ì—ì„œ ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¬¸ìì—´ì¸ì§€ í™•ì¸\n",
    "                        if value_part.startswith('\"') and value_part.endswith('\"'):\n",
    "                            # ê°’ ë‚´ë¶€ì˜ ë”°ì˜´í‘œë¥¼ ì‘ì€ë”°ì˜´í‘œë¡œ ë³€ê²½\n",
    "                            inner_value = value_part[1:-1]  # ì–‘ìª½ ë”°ì˜´í‘œ ì œê±°\n",
    "                            fixed_inner = inner_value.replace('\"', \"'\")\n",
    "                            fixed_value = f'\"{fixed_inner}\"'\n",
    "                            fixed_pairs.append(f'{key_part}:{fixed_value}')\n",
    "                        else:\n",
    "                            fixed_pairs.append(pair)\n",
    "                    except:\n",
    "                        fixed_pairs.append(pair)\n",
    "                else:\n",
    "                    fixed_pairs.append(pair)\n",
    "            \n",
    "            # ë‹¤ì‹œ ì¡°í•©\n",
    "            result = '{' + ','.join(fixed_pairs) + '}'\n",
    "            return result\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _parse_request_url(self, request_url: str) -> Dict:\n",
    "        \"\"\"REQUEST URLì—ì„œ íŒŒë¼ë¯¸í„°ë“¤ì„ ì¶”ì¶œ (ê°œì„ ëœ í•œê¸€ ì²˜ë¦¬)\"\"\"\n",
    "        if not request_url:\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ ë””ì½”ë”©\n",
    "            url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "            parsed_url = urlparse(url)\n",
    "            \n",
    "            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ë™ìœ¼ë¡œ íŒŒì‹± (ë³µì¡í•œ ê°’ ì²˜ë¦¬)\n",
    "            query = parsed_url.query\n",
    "            processed_params = {}\n",
    "            \n",
    "            if not query:\n",
    "                return processed_params\n",
    "            \n",
    "            # &ë¡œ ë¶„í• í•˜ë˜, URL ì¸ì½”ë”©ëœ &(%26)ì€ ë¶„í• í•˜ì§€ ì•ŠìŒ\n",
    "            params = []\n",
    "            current_param = \"\"\n",
    "            i = 0\n",
    "            \n",
    "            while i < len(query):\n",
    "                if query[i:i+1] == '&' and not query[max(0, i-3):i] == '%26':\n",
    "                    if current_param:\n",
    "                        params.append(current_param)\n",
    "                    current_param = \"\"\n",
    "                else:\n",
    "                    current_param += query[i]\n",
    "                i += 1\n",
    "            \n",
    "            if current_param:\n",
    "                params.append(current_param)\n",
    "            \n",
    "            # ê° íŒŒë¼ë¯¸í„°ë¥¼ í‚¤=ê°’ìœ¼ë¡œ ë¶„í• \n",
    "            for param in params:\n",
    "                if '=' not in param:\n",
    "                    continue\n",
    "                    \n",
    "                # ì²« ë²ˆì§¸ = ê¸°ì¤€ìœ¼ë¡œë§Œ ë¶„í•  (ê°’ì— =ì´ í¬í•¨ë  ìˆ˜ ìˆìŒ)\n",
    "                key, value = param.split('=', 1)\n",
    "                \n",
    "                # í‚¤ì™€ ê°’ ë””ì½”ë”©\n",
    "                decoded_key = self._fix_korean_text(key)\n",
    "                decoded_value = self._fix_korean_text(value)\n",
    "                \n",
    "                processed_params[decoded_key] = decoded_value\n",
    "            \n",
    "            return processed_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"URL íŒŒì‹± ì˜¤ë¥˜: {e}\")\n",
    "            # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback\n",
    "            try:\n",
    "                url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "                parsed_url = urlparse(url)\n",
    "                query_params = parse_qs(parsed_url.query, keep_blank_values=True)\n",
    "                \n",
    "                processed_params = {}\n",
    "                for key, value_list in query_params.items():\n",
    "                    raw_value = value_list[0] if value_list else ''\n",
    "                    decoded_value = self._fix_korean_text(raw_value)\n",
    "                    processed_params[key] = decoded_value\n",
    "                \n",
    "                return processed_params\n",
    "            except:\n",
    "                return {}\n",
    "\n",
    "    def _fix_korean_text(self, text: str) -> str:\n",
    "        \"\"\"ê¹¨ì§„ í•œê¸€ì„ ì˜¬ë°”ë¥´ê²Œ ë””ì½”ë”©\"\"\"\n",
    "        try:\n",
    "            # URL ë””ì½”ë”© ë¨¼ì € ì‹œë„\n",
    "            from urllib.parse import unquote\n",
    "            decoded = unquote(text, encoding='utf-8')\n",
    "            \n",
    "            # ê¹¨ì§„ í•œê¸€ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "            if self._has_broken_korean(decoded):\n",
    "                # Latin-1ë¡œ ì¸ì½”ë”© í›„ UTF-8ë¡œ ë””ì½”ë”© (ì¼ë°˜ì ì¸ mojibake í•´ê²°ë²•)\n",
    "                try:\n",
    "                    fixed = decoded.encode('latin-1').decode('utf-8')\n",
    "                    return fixed\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                # ê·¸ë˜ë„ ì•ˆë˜ë©´ ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ì²˜ë¦¬\n",
    "                try:\n",
    "                    # ë¬¸ìì—´ì˜ ê° ë¬¸ìë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜ í›„ UTF-8 ë””ì½”ë”©\n",
    "                    byte_data = bytes([ord(c) for c in decoded if ord(c) < 256])\n",
    "                    fixed = byte_data.decode('utf-8', errors='ignore')\n",
    "                    return fixed\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                # UTF-8 ë°”ì´íŠ¸ íŒ¨í„´ìœ¼ë¡œ ë³µêµ¬ ì‹œë„\n",
    "                try:\n",
    "                    # íŠ¹ì • ê¹¨ì§„ íŒ¨í„´ë“¤ì„ ì§ì ‘ ë§¤í•‘\n",
    "                    broken_patterns = {\n",
    "                        'ÃªÂ·Â¹Ã¬Ã¬Ã«ÂªÂ¨': 'ê·¹ì¥ìš©ëª¨',\n",
    "                        'Ã­Â¥ÃªÂ¸Â°Ã«Â¡Â­ÃªÂ²': 'í–¥ê¸°ë¡­ê²Œ',\n",
    "                        'Ã¬Â¤ÃªÂ¸Â°Ã¬Â¼Ã¬Â´': 'ìˆ˜ê¸°ì¼ì´',\n",
    "                        'Ã¬Ã¬Ã«ÂªÂ¨': 'ìš©ëª¨',\n",
    "                        'ÃªÂ·Â¹': 'ê·¹'\n",
    "                    }\n",
    "                    \n",
    "                    result = decoded\n",
    "                    for broken, fixed in broken_patterns.items():\n",
    "                        result = result.replace(broken, fixed)\n",
    "                    \n",
    "                    if result != decoded:\n",
    "                        return result\n",
    "                        \n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return decoded\n",
    "            \n",
    "        except Exception:\n",
    "            return text\n",
    "\n",
    "    def _has_broken_korean(self, text: str) -> bool:\n",
    "        \"\"\"ê¹¨ì§„ í•œê¸€ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸\"\"\"\n",
    "        broken_patterns = [\n",
    "            'Ã«Â¶', 'Ã¬Ã«', 'ÃªÂ¼Â¬', 'Ã«', 'Ã­Â¼', 'Ã¬Â¸', 'ÃªÂ¸Â°', 'ÃªÂ¸', 'Ã¬Ã¬Â¹', 'Ã¬', 'Ãª',\n",
    "            'ÃªÂ·Â¹Ã¬', 'Ã­Â¥ÃªÂ¸Â°', 'Ã¬Â¤ÃªÂ¸Â°', 'Ã¬Ã¬Ã«', 'Ã«ÂªÂ¨'\n",
    "        ]\n",
    "        return any(pattern in text for pattern in broken_patterns)\n",
    "\n",
    "    def _convert_to_detailed(self):\n",
    "        \"\"\"ë¡œê·¸ ë°ì´í„°ë¥¼ ìƒì„¸ key-value í–‰ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "        self.detailed_data = []\n",
    "        \n",
    "        # ì œì™¸í•  ê¸°ë³¸ í•„ë“œë“¤ (ë©”íƒ€ë°ì´í„°)\n",
    "        exclude_fields = {'log_line', 'logtime', 'clientip', 'useragent', 'status'}\n",
    "        \n",
    "        for log_entry in self.input_data:\n",
    "            log_no = log_entry.get('log_line', 0)\n",
    "            \n",
    "            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ\n",
    "            page_id = log_entry.get('page_id', '')\n",
    "            click_type = log_entry.get('click_type', '')\n",
    "            act_type = log_entry.get('act_type', '')\n",
    "            click_text = log_entry.get('click_text', '')  # click_text ì¶”ê°€\n",
    "            \n",
    "            # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ë“¤ì„ key-value ìŒìœ¼ë¡œ ì²˜ë¦¬\n",
    "            for key, value in log_entry.items():\n",
    "                if key in exclude_fields:\n",
    "                    continue\n",
    "                \n",
    "                # ê¸°ë³¸ í•„ë“œë“¤ì€ ê±´ë„ˆë›°ê¸°\n",
    "                if key in ['page_id', 'click_type', 'act_type', 'click_text']:\n",
    "                    continue\n",
    "                \n",
    "                self.detailed_data.append({\n",
    "                    'no': log_no,\n",
    "                    'page_id': page_id,\n",
    "                    'click_type': click_type,\n",
    "                    'act_type': act_type,\n",
    "                    'click_text': click_text,\n",
    "                    'key': key,\n",
    "                    'value': str(value) if value is not None else ''\n",
    "                })\n",
    "        \n",
    "        print(f\"âœ… ìƒì„¸ ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(self.detailed_data)}ê°œ key-value ìŒ\")\n",
    "\n",
    "    def _convert_to_combined(self):\n",
    "        \"\"\"ë¡œê·¸ ë°ì´í„°ë¥¼ ê²°í•© í˜•íƒœë¡œ ë³€í™˜ (key, valueê°€ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì—´)\"\"\"\n",
    "        self.combined_data = []\n",
    "        \n",
    "        # ì œì™¸í•  ê¸°ë³¸ í•„ë“œë“¤ (ë©”íƒ€ë°ì´í„°)\n",
    "        exclude_fields = {'log_line', 'logtime', 'clientip', 'useragent', 'status', \n",
    "                         'page_id', 'click_type', 'act_type', 'click_text'}\n",
    "        \n",
    "        for log_entry in self.input_data:\n",
    "            log_no = log_entry.get('log_line', 0)\n",
    "            \n",
    "            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ\n",
    "            page_id = log_entry.get('page_id', '')\n",
    "            click_type = log_entry.get('click_type', '')\n",
    "            act_type = log_entry.get('act_type', '')\n",
    "            click_text = log_entry.get('click_text', '')  # click_text ì¶”ê°€\n",
    "            \n",
    "            # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ë“¤ ìˆ˜ì§‘\n",
    "            keys = []\n",
    "            values = []\n",
    "            \n",
    "            for key, value in log_entry.items():\n",
    "                if key not in exclude_fields:\n",
    "                    keys.append(key)\n",
    "                    values.append(str(value) if value is not None else '')\n",
    "            \n",
    "            # í‚¤ì™€ ê°’ì„ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ê²°í•©\n",
    "            keys_combined = ', '.join(keys) if keys else ''\n",
    "            values_combined = ', '.join(values) if values else ''\n",
    "            \n",
    "            combined_entry = {\n",
    "                'no': log_no,\n",
    "                'page_id': page_id,\n",
    "                'click_type': click_type,\n",
    "                'act_type': act_type,\n",
    "                'keys_combined': keys_combined,\n",
    "                'values_combined': values_combined,\n",
    "                'key_count': len(keys)\n",
    "            }\n",
    "            \n",
    "            # click_textê°€ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "            if click_text:\n",
    "                combined_entry['click_text'] = click_text\n",
    "            \n",
    "            self.combined_data.append(combined_entry)\n",
    "        \n",
    "        print(f\"âœ… ê²°í•© ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(self.combined_data)}ê°œ ë¡œê·¸\")\n",
    "\n",
    "    def _convert_to_combined_deduplicated(self):\n",
    "        \"\"\"ê²°í•© ë°ì´í„°ì—ì„œ ì¤‘ë³µì„ ì œê±°í•œ í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
    "        if not self.combined_data:\n",
    "            print(\"âš ï¸ ê²°í•© ë°ì´í„°ê°€ ì—†ì–´ ì¤‘ë³µ ì œê±°ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        # pandas DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "        df = pd.DataFrame(self.combined_data)\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±° ì „ ê°œìˆ˜\n",
    "        original_count = len(df)\n",
    "        \n",
    "        # page_id, click_type, act_typeì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "        # click_textê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ë¡œ í¬í•¨\n",
    "        dedup_columns = ['page_id', 'click_type', 'act_type']\n",
    "        \n",
    "        # click_text ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ í–‰ì´ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "        if 'click_text' in df.columns and df['click_text'].notna().any() and (df['click_text'] != '').any():\n",
    "            dedup_columns.append('click_text')\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±°\n",
    "        df_deduplicated = df.drop_duplicates(subset=dedup_columns, keep='first')\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±° í›„ ê°œìˆ˜\n",
    "        deduplicated_count = len(df_deduplicated)\n",
    "        removed_count = original_count - deduplicated_count\n",
    "        \n",
    "        # DataFrameì„ ë‹¤ì‹œ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "        self.deduplicated_data = df_deduplicated.to_dict('records')\n",
    "        \n",
    "        print(f\"âœ… ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±° ì™„ë£Œ:\")\n",
    "        print(f\"   ì›ë³¸: {original_count}ê°œ\")\n",
    "        print(f\"   ì¤‘ë³µ ì œê±° í›„: {deduplicated_count}ê°œ\")\n",
    "        print(f\"   ì œê±°ëœ ì¤‘ë³µ: {removed_count}ê°œ\")\n",
    "        print(f\"   ì¤‘ë³µ ì œê±° ê¸°ì¤€: {', '.join(dedup_columns)}\")\n",
    "\n",
    "    def _convert_to_detailed_deduplicated(self):\n",
    "        \"\"\"ìƒì„¸ ë°ì´í„°ì—ì„œ ì¤‘ë³µì„ ì œê±°í•œ í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
    "        if not self.detailed_data:\n",
    "            print(\"âš ï¸ ìƒì„¸ ë°ì´í„°ê°€ ì—†ì–´ ì¤‘ë³µ ì œê±°ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        # pandas DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "        df = pd.DataFrame(self.detailed_data)\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±° ì „ ê°œìˆ˜\n",
    "        original_count = len(df)\n",
    "        \n",
    "        # page_id, click_type, act_type, key, valueë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "        # click_textê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ë¡œ í¬í•¨\n",
    "        dedup_columns = ['page_id', 'click_type', 'act_type', 'key', 'value']\n",
    "        \n",
    "        # click_text ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ í–‰ì´ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "        if 'click_text' in df.columns and df['click_text'].notna().any() and (df['click_text'] != '').any():\n",
    "            dedup_columns.append('click_text')\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±°\n",
    "        df_deduplicated = df.drop_duplicates(subset=dedup_columns, keep='first')\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±° í›„ ê°œìˆ˜\n",
    "        deduplicated_count = len(df_deduplicated)\n",
    "        removed_count = original_count - deduplicated_count\n",
    "        \n",
    "        # DataFrameì„ ë‹¤ì‹œ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "        self.detailed_deduplicated_data = df_deduplicated.to_dict('records')\n",
    "        \n",
    "        print(f\"âœ… ìƒì„¸ í˜•íƒœ ì¤‘ë³µ ì œê±° ì™„ë£Œ:\")\n",
    "        print(f\"   ì›ë³¸: {original_count}ê°œ\")\n",
    "        print(f\"   ì¤‘ë³µ ì œê±° í›„: {deduplicated_count}ê°œ\")\n",
    "        print(f\"   ì œê±°ëœ ì¤‘ë³µ: {removed_count}ê°œ\")\n",
    "        print(f\"   ì¤‘ë³µ ì œê±° ê¸°ì¤€: {', '.join(dedup_columns)}\")\n",
    "\n",
    "    def _extract_click_text_from_keys(self, row):\n",
    "        \"\"\"keys_combinedì—ì„œ click_text ê°’ì„ ì¶”ì¶œí•˜ë ¤ê³  ì‹œë„\"\"\"\n",
    "        try:\n",
    "            keys = row.get('keys_combined', '').split(', ')\n",
    "            values = row.get('values_combined', '').split(', ')\n",
    "            \n",
    "            if len(keys) == len(values):\n",
    "                for i, key in enumerate(keys):\n",
    "                    if key.strip() == 'click_text':\n",
    "                        return values[i].strip() if i < len(values) else ''\n",
    "            return ''\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    def _save_detailed_excel(self, output_path: str):\n",
    "        \"\"\"ìƒì„¸ ë°ì´í„°ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(self.detailed_data)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"âš ï¸ ì €ì¥í•  ìƒì„¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return\n",
    "            \n",
    "            # ì»¬ëŸ¼ ìˆœì„œ ì •ì˜\n",
    "            column_order = ['no', 'page_id', 'click_type', 'act_type', 'click_text', 'key', 'value']\n",
    "            \n",
    "            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "            available_columns = [col for col in column_order if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "            \n",
    "            # Excel íŒŒì¼ ì €ì¥\n",
    "            df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"âœ… ìƒì„¸ Excel íŒŒì¼ ì €ì¥: {output_path}\")\n",
    "            print(f\"   ì´ {len(self.detailed_data)}ê°œ key-value ìŒ\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _save_detailed_deduplicated_excel(self, output_path: str):\n",
    "        \"\"\"ìƒì„¸ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(self.detailed_deduplicated_data)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"âš ï¸ ì €ì¥í•  ìƒì„¸ ì¤‘ë³µ ì œê±° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return\n",
    "            \n",
    "            # ì»¬ëŸ¼ ìˆœì„œ ì •ì˜\n",
    "            column_order = ['no', 'page_id', 'click_type', 'act_type', 'click_text', 'key', 'value']\n",
    "            \n",
    "            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "            available_columns = [col for col in column_order if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "            \n",
    "            # Excel íŒŒì¼ ì €ì¥\n",
    "            df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"âœ… ìƒì„¸ ì¤‘ë³µ ì œê±° Excel íŒŒì¼ ì €ì¥: {output_path}\")\n",
    "            print(f\"   ì´ {len(self.detailed_deduplicated_data)}ê°œ key-value ìŒ (ì¤‘ë³µ ì œê±°ë¨)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _save_combined_deduplicated_excel(self, output_path: str):\n",
    "        \"\"\"ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(self.deduplicated_data)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"âš ï¸ ì €ì¥í•  ì¤‘ë³µ ì œê±° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return\n",
    "            \n",
    "            # ì»¬ëŸ¼ ìˆœì„œ ì •ì˜\n",
    "            column_order = ['no', 'page_id', 'click_type', 'act_type', 'keys_combined', 'values_combined', 'key_count']\n",
    "            \n",
    "            # click_text ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "            if 'click_text' in df.columns:\n",
    "                column_order.insert(4, 'click_text')\n",
    "            \n",
    "            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "            available_columns = [col for col in column_order if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "            \n",
    "            # Excel íŒŒì¼ ì €ì¥\n",
    "            df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"âœ… ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±° Excel íŒŒì¼ ì €ì¥: {output_path}\")\n",
    "            print(f\"   ì´ {len(self.deduplicated_data)}ê°œ ë¡œê·¸ (ì¤‘ë³µ ì œê±°ë¨)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _save_combined_excel(self, output_path: str):\n",
    "        \"\"\"ê²°í•© ë°ì´í„°ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(self.combined_data)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"âš ï¸ ì €ì¥í•  ê²°í•© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return\n",
    "            \n",
    "            # ì»¬ëŸ¼ ìˆœì„œ ì •ì˜\n",
    "            column_order = ['no', 'page_id', 'click_type', 'act_type', 'keys_combined', 'values_combined', 'key_count']\n",
    "            \n",
    "            # click_text ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "            if 'click_text' in df.columns:\n",
    "                column_order.insert(4, 'click_text')\n",
    "            \n",
    "            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "            available_columns = [col for col in column_order if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "            \n",
    "            # Excel íŒŒì¼ ì €ì¥\n",
    "            df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"âœ… ê²°í•© Excel íŒŒì¼ ì €ì¥: {output_path}\")\n",
    "            print(f\"   ì´ {len(self.combined_data)}ê°œ ë¡œê·¸\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_conversion_summary(self) -> Dict:\n",
    "        \"\"\"ë³€í™˜ ê²°ê³¼ ìš”ì•½ ì •ë³´ ë°˜í™˜\"\"\"\n",
    "        if not self.detailed_data:\n",
    "            return {\"error\": \"ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\"}\n",
    "        \n",
    "        # í†µê³„ ê³„ì‚°\n",
    "        total_logs = len(self.input_data)\n",
    "        total_key_values = len(self.detailed_data)\n",
    "        avg_keys_per_log = total_key_values / total_logs if total_logs > 0 else 0\n",
    "        \n",
    "        # ê°€ì¥ ë§ì€ í‚¤ë¥¼ ê°€ì§„ ë¡œê·¸ ì°¾ê¸°\n",
    "        key_counts = {}\n",
    "        for item in self.detailed_data:\n",
    "            log_no = item['no']\n",
    "            key_counts[log_no] = key_counts.get(log_no, 0) + 1\n",
    "        \n",
    "        max_keys = max(key_counts.values()) if key_counts else 0\n",
    "        min_keys = min(key_counts.values()) if key_counts else 0\n",
    "        \n",
    "        # ê³ ìœ í•œ í‚¤ ìˆ˜ ê³„ì‚°\n",
    "        unique_keys = set(item['key'] for item in self.detailed_data)\n",
    "        \n",
    "        # ì¤‘ë³µ ì œê±° í†µê³„\n",
    "        combined_count = len(self.combined_data) if self.combined_data else 0\n",
    "        combined_deduplicated_count = len(self.deduplicated_data) if self.deduplicated_data else 0\n",
    "        combined_duplicates_removed = combined_count - combined_deduplicated_count\n",
    "        \n",
    "        detailed_count = len(self.detailed_data) if self.detailed_data else 0\n",
    "        detailed_deduplicated_count = len(self.detailed_deduplicated_data) if self.detailed_deduplicated_data else 0\n",
    "        detailed_duplicates_removed = detailed_count - detailed_deduplicated_count\n",
    "        \n",
    "        return {\n",
    "            \"total_logs\": total_logs,\n",
    "            \"total_key_value_pairs\": total_key_values,\n",
    "            \"average_keys_per_log\": round(avg_keys_per_log, 2),\n",
    "            \"max_keys_in_single_log\": max_keys,\n",
    "            \"min_keys_in_single_log\": min_keys,\n",
    "            \"unique_keys_count\": len(unique_keys),\n",
    "            \"unique_keys\": sorted(list(unique_keys)),\n",
    "            \"combined_logs_count\": combined_count,\n",
    "            \"combined_deduplicated_count\": combined_deduplicated_count,\n",
    "            \"combined_duplicates_removed\": combined_duplicates_removed,\n",
    "            \"detailed_key_value_pairs\": detailed_count,\n",
    "            \"detailed_deduplicated_count\": detailed_deduplicated_count,\n",
    "            \"detailed_duplicates_removed\": detailed_duplicates_removed\n",
    "        }\n",
    "\n",
    "    def preview_data(self, num_rows: int = 10):\n",
    "        \"\"\"ë³€í™˜ëœ ìƒì„¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "        print(f\"\\nğŸ“‹ ìƒì„¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ {num_rows}ê°œ í–‰):\")\n",
    "        print(\"-\" * 140)\n",
    "        print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Click Text':15} | {'Key':20} | {'Value':25}\")\n",
    "        print(\"-\" * 140)\n",
    "        \n",
    "        if self.detailed_data:\n",
    "            for item in self.detailed_data[:num_rows]:\n",
    "                page_id = item['page_id'][:25] if len(item['page_id']) > 25 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                click_text = str(item.get('click_text', ''))[:15] if len(str(item.get('click_text', ''))) > 15 else str(item.get('click_text', ''))\n",
    "                key = item['key'][:20] if len(item['key']) > 20 else item['key']\n",
    "                value = item['value'][:25] if len(item['value']) > 25 else item['value']\n",
    "                \n",
    "                print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {click_text:15} | {key:20} | {value:25}\")\n",
    "            \n",
    "            if len(self.detailed_data) > num_rows:\n",
    "                print(f\"... ì™¸ {len(self.detailed_data) - num_rows}ê°œ í–‰\")\n",
    "        else:\n",
    "            print(\"ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def preview_combined_deduplicated_data(self, num_rows: int = 5):\n",
    "        \"\"\"ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "        print(f\"\\nğŸ“‹ ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ {num_rows}ê°œ í–‰):\")\n",
    "        print(\"-\" * 160)\n",
    "        \n",
    "        # í—¤ë” ë™ì  ìƒì„±\n",
    "        has_click_text = self.deduplicated_data and any('click_text' in item for item in self.deduplicated_data[:3])\n",
    "        if has_click_text:\n",
    "            print(f\"{'No':3} | {'Page ID':20} | {'Click Type':12} | {'Act Type':12} | {'Click Text':12} | {'Keys':25} | {'Values':25} | {'Count':5}\")\n",
    "        else:\n",
    "            print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Keys':30} | {'Values':30} | {'Count':5}\")\n",
    "        \n",
    "        print(\"-\" * 160)\n",
    "        \n",
    "        if self.deduplicated_data:\n",
    "            for item in self.deduplicated_data[:num_rows]:\n",
    "                page_id = item['page_id'][:20] if len(item['page_id']) > 20 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                \n",
    "                if 'click_text' in item and item['click_text']:\n",
    "                    click_text = item['click_text'][:12] if len(str(item['click_text'])) > 12 else str(item['click_text'])\n",
    "                    keys = item['keys_combined'][:25] if len(item['keys_combined']) > 25 else item['keys_combined']\n",
    "                    values = item['values_combined'][:25] if len(item['values_combined']) > 25 else item['values_combined']\n",
    "                    print(f\"{item['no']:3} | {page_id:20} | {click_type:12} | {act_type:12} | {click_text:12} | {keys:25} | {values:25} | {item['key_count']:5}\")\n",
    "                else:\n",
    "                    keys = item['keys_combined'][:25] if len(item['keys_combined']) > 25 else item['keys_combined']\n",
    "                    values = item['values_combined'][:25] if len(item['values_combined']) > 25 else item['values_combined']\n",
    "                    print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {keys:25} | {values:25} | {item['key_count']:5}\")\n",
    "            \n",
    "            if len(self.deduplicated_data) > num_rows:\n",
    "                print(f\"... ì™¸ {len(self.deduplicated_data) - num_rows}ê°œ í–‰\")\n",
    "        else:\n",
    "            print(\"ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def preview_detailed_deduplicated_data(self, num_rows: int = 10):\n",
    "        \"\"\"ìƒì„¸ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "        print(f\"\\nğŸ“‹ ìƒì„¸ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ {num_rows}ê°œ í–‰):\")\n",
    "        print(\"-\" * 140)\n",
    "        print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Click Text':15} | {'Key':20} | {'Value':25}\")\n",
    "        print(\"-\" * 140)\n",
    "        \n",
    "        if self.detailed_deduplicated_data:\n",
    "            for item in self.detailed_deduplicated_data[:num_rows]:\n",
    "                page_id = item['page_id'][:25] if len(item['page_id']) > 25 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                click_text = str(item.get('click_text', ''))[:15] if len(str(item.get('click_text', ''))) > 15 else str(item.get('click_text', ''))\n",
    "                key = item['key'][:20] if len(item['key']) > 20 else item['key']\n",
    "                value = item['value'][:25] if len(item['value']) > 25 else item['value']\n",
    "                \n",
    "                print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {click_text:15} | {key:20} | {value:25}\")\n",
    "            \n",
    "            if len(self.detailed_deduplicated_data) > num_rows:\n",
    "                print(f\"... ì™¸ {len(self.detailed_deduplicated_data) - num_rows}ê°œ í–‰\")\n",
    "        else:\n",
    "            print(\"ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def preview_combined_deduplicated_data(self, num_rows: int = 5):\n",
    "        \"\"\"ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "        print(f\"\\nğŸ“‹ ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ {num_rows}ê°œ í–‰):\")\n",
    "        print(\"-\" * 160)\n",
    "        \n",
    "        # í—¤ë” ë™ì  ìƒì„±\n",
    "        has_click_text = self.deduplicated_data and any('click_text' in item for item in self.deduplicated_data[:3])\n",
    "        if has_click_text:\n",
    "            print(f\"{'No':3} | {'Page ID':20} | {'Click Type':12} | {'Act Type':12} | {'Click Text':12} | {'Keys':25} | {'Values':25} | {'Count':5}\")\n",
    "        else:\n",
    "            print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Keys':30} | {'Values':30} | {'Count':5}\")\n",
    "        \n",
    "        print(\"-\" * 160)\n",
    "        \n",
    "        if self.deduplicated_data:\n",
    "            for item in self.deduplicated_data[:num_rows]:\n",
    "                page_id = item['page_id'][:20] if len(item['page_id']) > 20 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                \n",
    "                if 'click_text' in item and item['click_text']:\n",
    "                    click_text = item['click_text'][:12] if len(str(item['click_text'])) > 12 else str(item['click_text'])\n",
    "                    keys = item['keys_combined'][:25] if len(item['keys_combined']) > 25 else item['keys_combined']\n",
    "                    values = item['values_combined'][:25] if len(item['values_combined']) > 25 else item['values_combined']\n",
    "                    print(f\"{item['no']:3} | {page_id:20} | {click_type:12} | {act_type:12} | {click_text:12} | {keys:25} | {values:25} | {item['key_count']:5}\")\n",
    "                else:\n",
    "                    keys = item['keys_combined'][:25] if len(item['keys_combined']) > 25 else item['keys_combined']\n",
    "                    values = item['values_combined'][:25] if len(item['values_combined']) > 25 else item['values_combined']\n",
    "                    print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {keys:25} | {values:25} | {item['key_count']:5}\")\n",
    "            \n",
    "            if len(self.deduplicated_data) > num_rows:\n",
    "                print(f\"... ì™¸ {len(self.deduplicated_data) - num_rows}ê°œ í–‰\")\n",
    "        else:\n",
    "            print(\"ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def preview_combined_data(self, num_rows: int = 5):\n",
    "        \"\"\"ê²°í•© ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "        print(f\"\\nğŸ“‹ ê²°í•© ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ {num_rows}ê°œ í–‰):\")\n",
    "        print(\"-\" * 150)\n",
    "        print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Keys':40} | {'Values':40} | {'Count':5}\")\n",
    "        print(\"-\" * 150)\n",
    "        \n",
    "        if self.combined_data:\n",
    "            for item in self.combined_data[:num_rows]:\n",
    "                page_id = item['page_id'][:25] if len(item['page_id']) > 25 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                keys = item['keys_combined'][:40] if len(item['keys_combined']) > 40 else item['keys_combined']\n",
    "                values = item['values_combined'][:40] if len(item['values_combined']) > 40 else item['values_combined']\n",
    "                \n",
    "                print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {keys:40} | {values:40} | {item['key_count']:5}\")\n",
    "            \n",
    "            if len(self.combined_data) > num_rows:\n",
    "                print(f\"... ì™¸ {len(self.combined_data) - num_rows}ê°œ í–‰\")\n",
    "        else:\n",
    "            print(\"ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def get_unique_keys_analysis(self):\n",
    "        \"\"\"ê³ ìœ  í‚¤ ë¶„ì„\"\"\"\n",
    "        if not self.detailed_data:\n",
    "            print(\"ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        # í‚¤ë³„ ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "        key_frequency = {}\n",
    "        for item in self.detailed_data:\n",
    "            key = item['key']\n",
    "            key_frequency[key] = key_frequency.get(key, 0) + 1\n",
    "        \n",
    "        # ë¹ˆë„ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "        sorted_keys = sorted(key_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š í‚¤ ë¶„ì„ (ì´ {len(sorted_keys)}ê°œ ê³ ìœ  í‚¤):\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Key':30} | {'ë¹ˆë„ìˆ˜':10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for key, freq in sorted_keys[:20]:  # ìƒìœ„ 20ê°œë§Œ í‘œì‹œ\n",
    "            key_display = key[:30] if len(key) > 30 else key\n",
    "            print(f\"{key_display:30} | {freq:10}\")\n",
    "        \n",
    "        if len(sorted_keys) > 20:\n",
    "            print(f\"... ì™¸ {len(sorted_keys) - 20}ê°œ í‚¤\")\n",
    "\n",
    "\n",
    "def run_log_converter_example():\n",
    "    \"\"\"ë¡œê·¸ ë³€í™˜ê¸° ì‚¬ìš© ì˜ˆì‹œ\"\"\"\n",
    "    converter = LogToDetailedConverter()\n",
    "\n",
    "    print(\"=== JSON ë¡œê·¸ë¥¼ Excel í˜•íƒœë¡œ ë³€í™˜ (JSON ì˜¤ë¥˜ ìë™ ìˆ˜ì • + ì¤‘ë³µ ì œê±° ê¸°ëŠ¥ í¬í•¨) ===\\n\")\n",
    "    \n",
    "    # ë¡œê·¸ íŒŒì¼ì„ ë„¤ ê°€ì§€ í˜•íƒœë¡œ ë³€í™˜\n",
    "    converter.convert_log_to_all_formats(\n",
    "        \"log_file.txt\",                              # ì…ë ¥ JSON ë¡œê·¸ íŒŒì¼\n",
    "        \"./result/log_analysis_detailed.xlsx\",                # ìƒì„¸ í˜•íƒœ ì¶œë ¥\n",
    "        \"./result/log_analysis_detailed_deduplicated.xlsx\",   # ìƒì„¸ í˜•íƒœ ì¤‘ë³µ ì œê±° ì¶œë ¥\n",
    "        \"./result/log_analysis_combined.xlsx\",                # ê²°í•© í˜•íƒœ ì¶œë ¥\n",
    "        \"./result/log_analysis_combined_deduplicated.xlsx\"    # ê²°í•© í˜•íƒœ ì¤‘ë³µ ì œê±° ì¶œë ¥\n",
    "    )\n",
    "    \n",
    "    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "    summary = converter.get_conversion_summary()\n",
    "    print(f\"\\nğŸ“Š ë³€í™˜ ê²°ê³¼ ìš”ì•½:\")\n",
    "    for key, value in summary.items():\n",
    "        if key != 'unique_keys':  # unique_keysëŠ” ë„ˆë¬´ ê¸¸ì–´ì„œ ë³„ë„ ì¶œë ¥\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°\n",
    "    converter.preview_data(10)\n",
    "    converter.preview_detailed_deduplicated_data(10)\n",
    "    converter.preview_combined_data(5)\n",
    "    converter.preview_combined_deduplicated_data(5)\n",
    "    \n",
    "    # í‚¤ ë¶„ì„\n",
    "    converter.get_unique_keys_analysis()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_log_converter_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
