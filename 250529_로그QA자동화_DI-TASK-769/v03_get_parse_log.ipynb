{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 로그 분석 및 파일 출력 ===\n",
      "JSON 파싱 오류 (라인 94): Expecting ',' delimiter: line 1 column 618 (char 617)\n",
      "=== 총 385개의 로그 발견 ===\n",
      "\n",
      "✅ txt 파일 저장: ./result/log_analysis_result.txt\n",
      "✅ Excel 파일 저장: ./result/log_analysis_result.xlsx\n",
      "   총 385개 로그 처리됨\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from urllib.parse import parse_qs, urlparse, unquote\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "class LogAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "        self.parsed_logs = []\n",
    "\n",
    "    def parse_log_file_from_path(self, file_path: str, output_txt: str = None, output_xlsx: str = None):\n",
    "        \"\"\"\n",
    "        파일 경로에서 로그 파일을 읽어서 분석하고 결과 출력\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            # 로그 파싱\n",
    "            self._parse_log_content(content)\n",
    "            \n",
    "            # txt 파일 출력\n",
    "            if output_txt:\n",
    "                self._export_to_txt(output_txt)\n",
    "            \n",
    "            # Excel 파일 출력\n",
    "            if output_xlsx:\n",
    "                self._export_to_excel(output_xlsx)\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ 파일을 찾을 수 없습니다: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 읽기 오류: {e}\")\n",
    "\n",
    "    def _parse_log_content(self, content: str):\n",
    "        \"\"\"로그 내용 파싱\"\"\"\n",
    "        self.logs = []\n",
    "        self.parsed_logs = []\n",
    "        lines = content.strip().split('\\n')\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    log_data = json.loads(line.strip())\n",
    "                    self.logs.append((i+1, log_data))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"JSON 파싱 오류 (라인 {i+1}): {e}\")\n",
    "        \n",
    "        print(f\"=== 총 {len(self.logs)}개의 로그 발견 ===\\n\")\n",
    "        \n",
    "        # 각 로그의 REQUEST 파라미터 분석\n",
    "        for log_num, log_data in self.logs:\n",
    "            if 'REQUEST' in log_data:\n",
    "                params = self._parse_request_url(log_data['REQUEST'])\n",
    "                if params:\n",
    "                    parsed_log = {\n",
    "                        'log_number': log_num,\n",
    "                        'timestamp': log_data.get('LOGTIME', 'N/A'),\n",
    "                        'params': params\n",
    "                    }\n",
    "                    self.parsed_logs.append(parsed_log)\n",
    "\n",
    "    def _parse_request_url(self, request_url: str) -> Dict[str, str]:\n",
    "        \"\"\"REQUEST URL에서 파라미터를 파싱하고 한글을 올바르게 처리\"\"\"\n",
    "        try:\n",
    "            # 유니코드 이스케이프 디코딩\n",
    "            url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "            parsed_url = urlparse(url)\n",
    "            query_params = parse_qs(parsed_url.query, keep_blank_values=True)\n",
    "            \n",
    "            processed_params = {}\n",
    "            for key, value_list in query_params.items():\n",
    "                raw_value = value_list[0] if value_list else ''\n",
    "                decoded_value = self._fix_korean_text(raw_value)\n",
    "                processed_params[key] = decoded_value\n",
    "            \n",
    "            return processed_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"파싱 오류: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _fix_korean_text(self, text: str) -> str:\n",
    "        \"\"\"깨진 한글을 올바르게 디코딩\"\"\"\n",
    "        try:\n",
    "            decoded = unquote(text, encoding='utf-8')\n",
    "            \n",
    "            if self._has_broken_korean(decoded):\n",
    "                try:\n",
    "                    fixed = decoded.encode('latin-1').decode('utf-8')\n",
    "                    return fixed\n",
    "                except:\n",
    "                    try:\n",
    "                        byte_data = bytes([ord(c) for c in decoded if ord(c) < 256])\n",
    "                        fixed = byte_data.decode('utf-8', errors='ignore')\n",
    "                        return fixed\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            return decoded\n",
    "            \n",
    "        except Exception:\n",
    "            return text\n",
    "\n",
    "    def _has_broken_korean(self, text: str) -> bool:\n",
    "        \"\"\"깨진 한글 패턴이 있는지 확인\"\"\"\n",
    "        broken_patterns = [\n",
    "            'ë¶', 'ìë', 'ê¼¬', 'ë', 'í¼', 'ì¸', 'ê¸°', 'ê¸', 'ìì¹', 'ì', 'ê'\n",
    "        ]\n",
    "        return any(pattern in text for pattern in broken_patterns)\n",
    "\n",
    "    def _export_to_txt(self, output_file: str):\n",
    "        \"\"\"파싱 결과를 txt 파일로 출력\"\"\"\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"=== 총 {len(self.logs)}개의 로그 발견 ===\\n\\n\")\n",
    "                \n",
    "                for parsed_log in self.parsed_logs:\n",
    "                    f.write(\"=\" * 60 + \"\\n\")\n",
    "                    f.write(f\"로그 #{parsed_log['log_number']} - {parsed_log['timestamp']}\\n\")\n",
    "                    f.write(\"=\" * 60 + \"\\n\")\n",
    "                    \n",
    "                    params = parsed_log['params']\n",
    "                    if params:\n",
    "                        # URL 정보 (첫 번째 로그에서 추출)\n",
    "                        for log_num, log_data in self.logs:\n",
    "                            if log_num == parsed_log['log_number']:\n",
    "                                if 'REQUEST' in log_data:\n",
    "                                    url = log_data['REQUEST'].encode('utf-8').decode('unicode_escape')\n",
    "                                    parsed_url = urlparse(url)\n",
    "                                    f.write(f\"URL: {parsed_url.netloc}{parsed_url.path}\\n\")\n",
    "                                    f.write(f\"총 파라미터 수: {len(params)}\\n\")\n",
    "                                    f.write(\"-\" * 50 + \"\\n\")\n",
    "                                break\n",
    "                        \n",
    "                        # 파라미터 출력\n",
    "                        for key, value in params.items():\n",
    "                            f.write(f\"{key:20} : {value}\\n\")\n",
    "                    else:\n",
    "                        f.write(\"REQUEST 필드가 없습니다.\\n\")\n",
    "                    \n",
    "                    f.write(\"\\n\")\n",
    "            \n",
    "            print(f\"✅ txt 파일 저장: {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ txt 파일 저장 오류: {e}\")\n",
    "\n",
    "    def _export_to_excel(self, output_file: str):\n",
    "        \"\"\"파싱 결과를 Excel 파일로 출력 (page_id, click_type, act_type, click_text, key_list, value_list, len(key_list))\"\"\"\n",
    "        try:\n",
    "            excel_data = []\n",
    "            \n",
    "            for parsed_log in self.parsed_logs:\n",
    "                params = parsed_log['params']\n",
    "                \n",
    "                # 각 로그에서 필요한 정보 추출\n",
    "                page_id = params.get('page_id', '')\n",
    "                click_type = params.get('click_type', '')\n",
    "                act_type = params.get('act_type', '')\n",
    "                click_text = params.get('click_text', '')\n",
    "                \n",
    "                # key_list와 value_list 생성\n",
    "                key_list = list(params.keys())\n",
    "                value_list = list(params.values())\n",
    "                \n",
    "                key_list_str = ', '.join(key_list)\n",
    "                value_list_str = ', '.join(str(v) for v in value_list)\n",
    "                key_count = len(key_list)\n",
    "                \n",
    "                excel_data.append({\n",
    "                    'page_id': page_id,\n",
    "                    'click_type': click_type,\n",
    "                    'act_type': act_type,\n",
    "                    'click_text': click_text,\n",
    "                    'key_list': key_list_str,\n",
    "                    'value_list': value_list_str,\n",
    "                    'len(key_list)': key_count\n",
    "                })\n",
    "            \n",
    "            # DataFrame 생성 및 Excel 저장\n",
    "            df = pd.DataFrame(excel_data)\n",
    "            df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"✅ Excel 파일 저장: {output_file}\")\n",
    "            print(f\"   총 {len(excel_data)}개 로그 처리됨\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Excel 파일 저장 오류: {e}\")\n",
    "\n",
    "    def analyze_logs(self, file_path: str):\n",
    "        \"\"\"로그 분석 (기존 기능 유지)\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            self._parse_log_content(content)\n",
    "            \n",
    "            # 콘솔 출력\n",
    "            for parsed_log in self.parsed_logs:\n",
    "                print(\"=\" * 60)\n",
    "                print(f\"로그 #{parsed_log['log_number']} - {parsed_log['timestamp']}\")\n",
    "                print(\"=\" * 60)\n",
    "                \n",
    "                params = parsed_log['params']\n",
    "                if params:\n",
    "                    # URL 정보 출력\n",
    "                    for log_num, log_data in self.logs:\n",
    "                        if log_num == parsed_log['log_number']:\n",
    "                            if 'REQUEST' in log_data:\n",
    "                                url = log_data['REQUEST'].encode('utf-8').decode('unicode_escape')\n",
    "                                parsed_url = urlparse(url)\n",
    "                                print(f\"URL: {parsed_url.netloc}{parsed_url.path}\")\n",
    "                                print(f\"총 파라미터 수: {len(params)}\")\n",
    "                                print(\"-\" * 50)\n",
    "                            break\n",
    "                    \n",
    "                    # 파라미터 출력\n",
    "                    for key, value in params.items():\n",
    "                        print(f\"{key:20} : {value}\")\n",
    "                else:\n",
    "                    print(\"REQUEST 필드가 없습니다.\")\n",
    "                \n",
    "                print()\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ 파일을 찾을 수 없습니다: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 읽기 오류: {e}\")\n",
    "\n",
    "# 사용 예시\n",
    "def run_example():\n",
    "    \"\"\"사용 예시\"\"\"\n",
    "    analyzer = LogAnalyzer()\n",
    "    \n",
    "    # 1. 로그 분석 및 txt, Excel 파일 생성\n",
    "    analyzer.parse_log_file_from_path(\n",
    "        file_path=\"log_file.txt\",\n",
    "        output_txt=\"./result/log_analysis_result.txt\",\n",
    "        output_xlsx=\"./result/log_analysis_result.xlsx\"\n",
    "    )\n",
    "    \n",
    "    # 2. 콘솔에 로그 분석 결과 출력만 하고 싶은 경우\n",
    "    # analyzer.analyze_logs(\"log_file.txt\")\n",
    "\n",
    "# 기존 QA 검증 클래스 (기존 기능 유지)\n",
    "class LogQAValidator:\n",
    "    def __init__(self):\n",
    "        self.scenarios = []\n",
    "        self.logs = []\n",
    "        self.unique_keys = []\n",
    "        \n",
    "    def load_scenario_csv(self, csv_file_path: str):\n",
    "        \"\"\"CSV 파일에서 시나리오 로드\"\"\"\n",
    "        self.scenarios = []\n",
    "        df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "        \n",
    "        # 컬럼 구성에 따라 고유키 조합 결정\n",
    "        columns = df.columns.tolist()\n",
    "        if 'page_id' in columns:\n",
    "            self.unique_keys = ['page_id', 'click_type', 'act_type']\n",
    "            print(\"🔑 고유키 조합: page_id + click_type + act_type\")\n",
    "        elif 'click_text' in columns:\n",
    "            self.unique_keys = ['click_text', 'click_type', 'act_type']\n",
    "            print(\"🔑 고유키 조합: click_text + click_type + act_type\")\n",
    "        else:\n",
    "            raise ValueError(\"지원하지 않는 시나리오 형식입니다. page_id 또는 click_text 컬럼이 필요합니다.\")\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            key_list = [k.strip() for k in str(row['key_list']).split(',') if k.strip()]\n",
    "            scenario = {\n",
    "                'expected_keys': key_list\n",
    "            }\n",
    "            \n",
    "            # 고유키 값들 추가\n",
    "            for key in self.unique_keys:\n",
    "                scenario[key] = str(row[key]) if pd.notna(row[key]) else ''\n",
    "            \n",
    "            self.scenarios.append(scenario)\n",
    "        \n",
    "        print(f\"✅ 시나리오 로드: {len(self.scenarios)}개\")\n",
    "\n",
    "    def parse_logs_from_file(self, log_file_path: str):\n",
    "        \"\"\"로그 txt 파일에서 직접 파싱\"\"\"\n",
    "        try:\n",
    "            with open(log_file_path, 'r', encoding='utf-8') as file:\n",
    "                log_content = file.read()\n",
    "            self.parse_logs(log_content)\n",
    "            print(f\"✅ 로그 파일 로드: {log_file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ 파일을 찾을 수 없습니다: {log_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 읽기 오류: {e}\")\n",
    "\n",
    "    def parse_logs(self, log_content: str):\n",
    "        \"\"\"로그 파싱\"\"\"\n",
    "        self.logs = []\n",
    "        for i, line in enumerate(log_content.strip().split('\\n')):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                log_data = json.loads(line.strip())\n",
    "                if 'REQUEST' not in log_data:\n",
    "                    continue\n",
    "                    \n",
    "                params = self._parse_url(log_data['REQUEST'])\n",
    "                if params:\n",
    "                    self.logs.append({\n",
    "                        'line': i + 1,\n",
    "                        'timestamp': log_data.get('LOGTIME', ''),\n",
    "                        'params': params\n",
    "                    })\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "                \n",
    "        print(f\"✅ 로그 파싱: {len(self.logs)}개\")\n",
    "\n",
    "    def _parse_url(self, request_url: str) -> Dict[str, str]:\n",
    "        \"\"\"URL 파라미터 파싱\"\"\"\n",
    "        try:\n",
    "            url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "            parsed = urlparse(url)\n",
    "            query_params = parse_qs(parsed.query, keep_blank_values=True)\n",
    "            \n",
    "            result = {}\n",
    "            for key, values in query_params.items():\n",
    "                if values:\n",
    "                    decoded = unquote(values[0], encoding='utf-8')\n",
    "                    # 한글 깨짐 복구 시도\n",
    "                    try:\n",
    "                        if any(ord(c) > 127 for c in decoded):\n",
    "                            decoded = decoded.encode('latin-1').decode('utf-8')\n",
    "                    except:\n",
    "                        pass\n",
    "                    result[key] = decoded\n",
    "            return result\n",
    "        except:\n",
    "            return {}\n",
    "\n",
    "    def validate_and_export(self, output_file: str):\n",
    "        \"\"\"검증 수행 및 결과 XLSX 출력\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for scenario in self.scenarios:\n",
    "            # 시나리오와 매칭되는 로그 찾기\n",
    "            matched_logs = self._find_matching_logs(scenario)\n",
    "            \n",
    "            if not matched_logs:\n",
    "                # 매칭되는 로그가 없는 경우\n",
    "                for key in scenario['expected_keys']:\n",
    "                    result = {\n",
    "                        'key': key,\n",
    "                        'value': 'NOT_FOUND',\n",
    "                        'pass': 'FAIL'\n",
    "                    }\n",
    "                    \n",
    "                    # 고유키 값들 추가\n",
    "                    for uk in self.unique_keys:\n",
    "                        result[uk] = scenario.get(uk, '')\n",
    "                    \n",
    "                    results.append(result)\n",
    "                continue\n",
    "            \n",
    "            # 각 매칭된 로그에 대해 검증\n",
    "            for log in matched_logs:\n",
    "                actual_keys = set(log['params'].keys())\n",
    "                expected_keys = set(scenario['expected_keys'])\n",
    "                \n",
    "                # 예상 키들 검증\n",
    "                for key in expected_keys:\n",
    "                    value = log['params'].get(key, 'MISSING')\n",
    "                    pass_status = 'PASS' if key in actual_keys else 'FAIL'\n",
    "                    \n",
    "                    result = {\n",
    "                        'key': key,\n",
    "                        'value': value,\n",
    "                        'pass': pass_status\n",
    "                    }\n",
    "                    \n",
    "                    # 고유키 값들 추가\n",
    "                    for uk in self.unique_keys:\n",
    "                        result[uk] = scenario.get(uk, '')\n",
    "                    \n",
    "                    results.append(result)\n",
    "                \n",
    "                # 예상치 못한 키들 (선택사항)\n",
    "                unexpected_keys = actual_keys - expected_keys\n",
    "                for key in unexpected_keys:\n",
    "                    result = {\n",
    "                        'key': key,\n",
    "                        'value': log['params'][key],\n",
    "                        'pass': 'UNEXPECTED'\n",
    "                    }\n",
    "                    \n",
    "                    # 고유키 값들 추가\n",
    "                    for uk in self.unique_keys:\n",
    "                        result[uk] = scenario.get(uk, '')\n",
    "                    \n",
    "                    results.append(result)\n",
    "\n",
    "        # XLSX 출력 - 컬럼 순서 정렬\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # 고유키 조합에 따른 컬럼 순서 정의\n",
    "        if 'page_id' in self.unique_keys:\n",
    "            column_order = ['page_id', 'click_type', 'act_type', 'key', 'value', 'pass']\n",
    "        else:  # click_text 기반\n",
    "            column_order = ['click_text', 'click_type', 'act_type', 'key', 'value', 'pass']\n",
    "        \n",
    "        # 컬럼 순서 적용\n",
    "        df_results = df_results[column_order]\n",
    "        df_results.to_excel(output_file, index=False, engine='openpyxl')\n",
    "        \n",
    "        # 요약 출력\n",
    "        self._print_summary(results)\n",
    "        print(f\"✅ 결과 저장: {output_file}\")\n",
    "\n",
    "    def _find_matching_logs(self, scenario: Dict) -> List[Dict]:\n",
    "        \"\"\"시나리오와 매칭되는 로그 찾기\"\"\"\n",
    "        matched = []\n",
    "        \n",
    "        for log in self.logs:\n",
    "            params = log['params']\n",
    "            \n",
    "            # 동적 고유키로 매칭\n",
    "            is_match = True\n",
    "            for key in self.unique_keys:\n",
    "                if params.get(key, '') != scenario.get(key, ''):\n",
    "                    is_match = False\n",
    "                    break\n",
    "            \n",
    "            if is_match:\n",
    "                matched.append(log)\n",
    "        \n",
    "        return matched\n",
    "\n",
    "    def _print_summary(self, results: List[Dict]):\n",
    "        \"\"\"결과 요약 출력\"\"\"\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        total = len(df)\n",
    "        passed = len(df[df['pass'] == 'PASS'])\n",
    "        failed = len(df[df['pass'] == 'FAIL'])\n",
    "        unexpected = len(df[df['pass'] == 'UNEXPECTED'])\n",
    "        \n",
    "        print(f\"\\n📊 검증 결과:\")\n",
    "        print(f\"   전체: {total}개\")\n",
    "        print(f\"   통과: {passed}개\")\n",
    "        print(f\"   실패: {failed}개\")\n",
    "        print(f\"   예상외: {unexpected}개\")\n",
    "        print(f\"   성공률: {passed/total*100:.1f}%\" if total > 0 else \"   성공률: 0%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 로그 분석 및 파일 출력 예시\n",
    "    print(\"=== 로그 분석 및 파일 출력 ===\")\n",
    "    run_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 로그 분석 및 파일 출력 ===\n",
      "⚠️ JSON 파싱 오류 발생: 7개\n",
      "================================================================================\n",
      "라인 94: Expecting ',' delimiter: line 1 column 618 (char 617)\n",
      "원본 로그: {\"BYTES\":\"0\",\"MODE\":\"\",\"PROTOCOL\":\"HTTP/1.1\",\"LOGTIME\":\"02/Jun/2025:18:26:13 +0900\",\"REFERRER\":\"-\",\"sid\":\"roundApp_web\",\"USERAGENT\":\"Mozilla/5.0 (iPhone; CPU iPhone OS 18_5 like Mac OS X) AppleWebKit/...\n",
      "--------------------------------------------------------------------------------\n",
      "라인 422: Expecting ',' delimiter: line 1 column 453 (char 452)\n",
      "원본 로그: {\"BYTES\":\"0\",\"MODE\":\"\",\"PROTOCOL\":\"HTTP/1.1\",\"LOGTIME\":\"04/Jun/2025:14:02:03 +0900\",\"REFERRER\":\"-\",\"sid\":\"roundApp_web\",\"USERAGENT\":\"Mozilla/5.0 (iPhone; CPU iPhone OS 18_5 like Mac OS X) AppleWebKit/...\n",
      "--------------------------------------------------------------------------------\n",
      "라인 464: Expecting ',' delimiter: line 1 column 453 (char 452)\n",
      "원본 로그: {\"BYTES\":\"0\",\"MODE\":\"\",\"PROTOCOL\":\"HTTP/1.1\",\"LOGTIME\":\"04/Jun/2025:14:04:04 +0900\",\"REFERRER\":\"-\",\"sid\":\"roundApp_web\",\"USERAGENT\":\"Mozilla/5.0 (iPhone; CPU iPhone OS 18_5 like Mac OS X) AppleWebKit/...\n",
      "--------------------------------------------------------------------------------\n",
      "라인 710: Expecting ',' delimiter: line 1 column 733 (char 732)\n",
      "원본 로그: {\"BYTES\":\"0\",\"MODE\":\"\",\"PROTOCOL\":\"HTTP/1.1\",\"LOGTIME\":\"04/Jun/2025:15:54:38 +0900\",\"REFERRER\":\"-\",\"sid\":\"roundApp_web\",\"USERAGENT\":\"Mozilla/5.0 (iPhone; CPU iPhone OS 18_5 like Mac OS X) AppleWebKit/...\n",
      "--------------------------------------------------------------------------------\n",
      "라인 731: Expecting ',' delimiter: line 1 column 700 (char 699)\n",
      "원본 로그: {\"BYTES\":\"0\",\"MODE\":\"\",\"PROTOCOL\":\"HTTP/1.1\",\"LOGTIME\":\"04/Jun/2025:15:54:54 +0900\",\"REFERRER\":\"-\",\"sid\":\"roundApp_web\",\"USERAGENT\":\"Mozilla/5.0 (iPhone; CPU iPhone OS 18_5 like Mac OS X) AppleWebKit/...\n",
      "--------------------------------------------------------------------------------\n",
      "라인 738: Expecting ',' delimiter: line 1 column 717 (char 716)\n",
      "원본 로그: {\"BYTES\":\"0\",\"MODE\":\"\",\"PROTOCOL\":\"HTTP/1.1\",\"LOGTIME\":\"04/Jun/2025:15:54:58 +0900\",\"REFERRER\":\"-\",\"sid\":\"roundApp_web\",\"USERAGENT\":\"Mozilla/5.0 (iPhone; CPU iPhone OS 18_5 like Mac OS X) AppleWebKit/...\n",
      "--------------------------------------------------------------------------------\n",
      "라인 777: Expecting ',' delimiter: line 1 column 698 (char 697)\n",
      "원본 로그: {\"BYTES\":\"0\",\"MODE\":\"\",\"PROTOCOL\":\"HTTP/1.1\",\"LOGTIME\":\"04/Jun/2025:15:56:00 +0900\",\"REFERRER\":\"-\",\"sid\":\"roundApp_web\",\"USERAGENT\":\"Mozilla/5.0 (iPhone; CPU iPhone OS 18_5 like Mac OS X) AppleWebKit/...\n",
      "--------------------------------------------------------------------------------\n",
      "   성공적으로 파싱된 로그: 770개\n",
      "   전체 라인 수: 777개\n",
      "=== 총 770개의 로그 처리 ===\n",
      "\n",
      "✅ txt 파일 저장: log_analysis_result.txt\n",
      "✅ Excel 파일 저장: log_analysis_result.xlsx\n",
      "   총 770개 로그 처리됨\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# from urllib.parse import parse_qs, urlparse, unquote\n",
    "# from typing import Dict, List\n",
    "# from datetime import datetime\n",
    "\n",
    "# class LogAnalyzer:\n",
    "#     def __init__(self):\n",
    "#         self.logs = []\n",
    "#         self.parsed_logs = []\n",
    "\n",
    "#     def parse_log_file_from_path(self, file_path: str, output_txt: str = None, output_xlsx: str = None):\n",
    "#         \"\"\"\n",
    "#         파일 경로에서 로그 파일을 읽어서 분석하고 결과 출력\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#                 content = file.read()\n",
    "            \n",
    "#             # 로그 파싱\n",
    "#             self._parse_log_content(content)\n",
    "            \n",
    "#             # txt 파일 출력\n",
    "#             if output_txt:\n",
    "#                 self._export_to_txt(output_txt)\n",
    "            \n",
    "#             # Excel 파일 출력\n",
    "#             if output_xlsx:\n",
    "#                 self._export_to_excel(output_xlsx)\n",
    "                \n",
    "#         except FileNotFoundError:\n",
    "#             print(f\"❌ 파일을 찾을 수 없습니다: {file_path}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ 파일 읽기 오류: {e}\")\n",
    "\n",
    "#     def _parse_log_content(self, content: str):\n",
    "#         \"\"\"로그 내용 파싱 - 안전한 JSON 파싱\"\"\"\n",
    "#         self.logs = []\n",
    "#         self.parsed_logs = []\n",
    "#         lines = content.strip().split('\\n')\n",
    "        \n",
    "#         json_errors = []\n",
    "        \n",
    "#         for i, line in enumerate(lines):\n",
    "#             if line.strip():\n",
    "#                 try:\n",
    "#                     log_data = json.loads(line.strip())\n",
    "#                     self.logs.append((i+1, log_data))\n",
    "#                 except json.JSONDecodeError as e:\n",
    "#                     json_errors.append((i+1, str(e), line.strip()))\n",
    "                    \n",
    "#                     # JSON 수정 시도\n",
    "#                     fixed_line = self._try_fix_json(line.strip())\n",
    "#                     if fixed_line:\n",
    "#                         try:\n",
    "#                             log_data = json.loads(fixed_line)\n",
    "#                             self.logs.append((i+1, log_data))\n",
    "#                             print(f\"✅ JSON 수정 성공 (라인 {i+1})\")\n",
    "#                         except:\n",
    "#                             pass\n",
    "        \n",
    "#         # 오류 상세 출력\n",
    "#         if json_errors:\n",
    "#             print(f\"⚠️ JSON 파싱 오류 발생: {len(json_errors)}개\")\n",
    "#             print(\"=\"*80)\n",
    "#             for line_num, error_msg, original_line in json_errors:\n",
    "#                 print(f\"라인 {line_num}: {error_msg}\")\n",
    "#                 print(f\"원본 로그: {original_line[:200]}...\" if len(original_line) > 200 else f\"원본 로그: {original_line}\")\n",
    "#                 print(\"-\" * 80)\n",
    "#             print(f\"   성공적으로 파싱된 로그: {len(self.logs)}개\")\n",
    "#             print(f\"   전체 라인 수: {len(lines)}개\")\n",
    "#         else:\n",
    "#             print(f\"✅ 모든 JSON 파싱 성공\")\n",
    "        \n",
    "#         print(f\"=== 총 {len(self.logs)}개의 로그 처리 ===\\n\")\n",
    "        \n",
    "#         # 각 로그의 REQUEST 파라미터 분석\n",
    "#         for log_num, log_data in self.logs:\n",
    "#             if 'REQUEST' in log_data:\n",
    "#                 params = self._parse_request_url(log_data['REQUEST'])\n",
    "#                 if params:\n",
    "#                     parsed_log = {\n",
    "#                         'log_number': log_num,\n",
    "#                         'timestamp': log_data.get('LOGTIME', 'N/A'),\n",
    "#                         'params': params\n",
    "#                     }\n",
    "#                     self.parsed_logs.append(parsed_log)\n",
    "\n",
    "#     def _try_fix_json(self, line: str) -> str:\n",
    "#         \"\"\"JSON 수정 시도 - 더 강력한 수정\"\"\"\n",
    "#         try:\n",
    "#             # 1. 마지막 쉼표 제거\n",
    "#             if line.endswith(',}'):\n",
    "#                 line = line[:-2] + '}'\n",
    "#             elif line.endswith(',]'):\n",
    "#                 line = line[:-2] + ']'\n",
    "            \n",
    "#             # 2. 값 내부의 따옴표 이스케이프 처리\n",
    "#             import re\n",
    "            \n",
    "#             # JSON 키:값 패턴을 찾아서 값 내부의 따옴표를 이스케이프\n",
    "#             def fix_quotes_in_values(match):\n",
    "#                 key = match.group(1)\n",
    "#                 value = match.group(2)\n",
    "                \n",
    "#                 # 값 내부의 따옴표를 이스케이프\n",
    "#                 # 단, 이미 이스케이프된 따옴표는 제외\n",
    "#                 fixed_value = value\n",
    "                \n",
    "#                 # 따옴표 앞에 백슬래시가 없는 경우만 이스케이프\n",
    "#                 fixed_value = re.sub(r'(?<!\\\\)\"', r'\\\\\"', fixed_value)\n",
    "                \n",
    "#                 return f'\"{key}\":\"{fixed_value}\"'\n",
    "            \n",
    "#             # \"key\":\"value\" 패턴을 찾아서 수정\n",
    "#             # 탐욕적이지 않은 매칭 사용\n",
    "#             pattern = r'\"([^\"]+)\":\"(.*?)(?=\"[,}]|$)'\n",
    "            \n",
    "#             # 특별히 문제가 되는 패턴들을 먼저 처리\n",
    "#             # click_text, article_title 등의 값에서 따옴표 문제 해결\n",
    "#             problem_fields = ['click_text', 'article_title', 'banner_text']\n",
    "            \n",
    "#             for field in problem_fields:\n",
    "#                 # 해당 필드의 값에서 따옴표 문제 수정\n",
    "#                 field_pattern = f'\"{field}\":\"([^\"]*\"[^\"]*)\"'\n",
    "                \n",
    "#                 def fix_field_quotes(match):\n",
    "#                     value = match.group(1)\n",
    "#                     # 값 내부의 따옴표를 이스케이프\n",
    "#                     fixed_value = value.replace('\"', '\\\\\"')\n",
    "#                     return f'\"{field}\":\"{fixed_value}\"'\n",
    "                \n",
    "#                 line = re.sub(field_pattern, fix_field_quotes, line)\n",
    "            \n",
    "#             # 3. 잘못된 이스케이프 문자 수정\n",
    "#             line = line.replace('\\\\u003d', '=').replace('\\\\u0026', '&')\n",
    "            \n",
    "#             return line\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             # 더 간단한 수정 시도\n",
    "#             try:\n",
    "#                 # 값 내부의 모든 따옴표를 작은따옴표로 변경 (임시 해결책)\n",
    "#                 import re\n",
    "                \n",
    "#                 # \"key\":\"value with \"quotes\"\" 패턴을 찾아서 수정\n",
    "#                 def replace_inner_quotes(match):\n",
    "#                     key = match.group(1)\n",
    "#                     value = match.group(2)\n",
    "#                     # 값 내부의 따옴표를 작은따옴표로 변경\n",
    "#                     fixed_value = value.replace('\"', \"'\")\n",
    "#                     return f'\"{key}\":\"{fixed_value}\"'\n",
    "                \n",
    "#                 # 문제가 되는 필드들에 대해서만 적용\n",
    "#                 problem_fields = ['click_text', 'article_title', 'banner_text']\n",
    "#                 for field in problem_fields:\n",
    "#                     pattern = f'\"{field}\":\"([^\"]*\"[^\"]*)\"'\n",
    "#                     line = re.sub(pattern, replace_inner_quotes, line)\n",
    "                \n",
    "#                 return line\n",
    "#             except:\n",
    "#                 return None\n",
    "\n",
    "#     def _parse_request_url(self, request_url: str) -> Dict[str, str]:\n",
    "#         \"\"\"REQUEST URL에서 파라미터를 파싱하고 한글을 올바르게 처리\"\"\"\n",
    "#         try:\n",
    "#             # 유니코드 이스케이프 디코딩\n",
    "#             url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "#             parsed_url = urlparse(url)\n",
    "            \n",
    "#             # 쿼리 파라미터를 수동으로 파싱 (복잡한 값 처리)\n",
    "#             query = parsed_url.query\n",
    "#             processed_params = {}\n",
    "            \n",
    "#             if not query:\n",
    "#                 return processed_params\n",
    "            \n",
    "#             # &로 분할하되, URL 인코딩된 &(%26)은 분할하지 않음\n",
    "#             params = []\n",
    "#             current_param = \"\"\n",
    "#             i = 0\n",
    "            \n",
    "#             while i < len(query):\n",
    "#                 if query[i:i+1] == '&' and not query[max(0, i-3):i] == '%26':\n",
    "#                     if current_param:\n",
    "#                         params.append(current_param)\n",
    "#                     current_param = \"\"\n",
    "#                 else:\n",
    "#                     current_param += query[i]\n",
    "#                 i += 1\n",
    "            \n",
    "#             if current_param:\n",
    "#                 params.append(current_param)\n",
    "            \n",
    "#             # 각 파라미터를 키=값으로 분할\n",
    "#             for param in params:\n",
    "#                 if '=' not in param:\n",
    "#                     continue\n",
    "                    \n",
    "#                 # 첫 번째 = 기준으로만 분할 (값에 =이 포함될 수 있음)\n",
    "#                 key, value = param.split('=', 1)\n",
    "                \n",
    "#                 # 키와 값 디코딩\n",
    "#                 decoded_key = self._fix_korean_text(key)\n",
    "#                 decoded_value = self._fix_korean_text(value)\n",
    "                \n",
    "#                 processed_params[decoded_key] = decoded_value\n",
    "            \n",
    "#             return processed_params\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"파싱 오류: {e}\")\n",
    "#             # 실패 시 기존 방식으로 fallback\n",
    "#             try:\n",
    "#                 url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "#                 parsed_url = urlparse(url)\n",
    "#                 query_params = parse_qs(parsed_url.query, keep_blank_values=True)\n",
    "                \n",
    "#                 processed_params = {}\n",
    "#                 for key, value_list in query_params.items():\n",
    "#                     raw_value = value_list[0] if value_list else ''\n",
    "#                     decoded_value = self._fix_korean_text(raw_value)\n",
    "#                     processed_params[key] = decoded_value\n",
    "                \n",
    "#                 return processed_params\n",
    "#             except:\n",
    "#                 return {}\n",
    "\n",
    "#     def _fix_korean_text(self, text: str) -> str:\n",
    "#         \"\"\"깨진 한글을 올바르게 디코딩\"\"\"\n",
    "#         try:\n",
    "#             # URL 디코딩 먼저 시도\n",
    "#             decoded = unquote(text, encoding='utf-8')\n",
    "            \n",
    "#             # 깨진 한글 패턴이 있는지 확인\n",
    "#             if self._has_broken_korean(decoded):\n",
    "#                 # Latin-1로 인코딩 후 UTF-8로 디코딩 (일반적인 mojibake 해결법)\n",
    "#                 try:\n",
    "#                     fixed = decoded.encode('latin-1').decode('utf-8')\n",
    "#                     return fixed\n",
    "#                 except:\n",
    "#                     pass\n",
    "                    \n",
    "#                 # 그래도 안되면 바이트 단위로 처리\n",
    "#                 try:\n",
    "#                     # 문자열의 각 문자를 바이트로 변환 후 UTF-8 디코딩\n",
    "#                     byte_data = bytes([ord(c) for c in decoded if ord(c) < 256])\n",
    "#                     fixed = byte_data.decode('utf-8', errors='ignore')\n",
    "#                     return fixed\n",
    "#                 except:\n",
    "#                     pass\n",
    "                    \n",
    "#                 # UTF-8 바이트 패턴으로 복구 시도\n",
    "#                 try:\n",
    "#                     # 특정 깨진 패턴들을 직접 매핑\n",
    "#                     broken_patterns = {\n",
    "#                         'ê·¹ììëª¨': '극장용모',\n",
    "#                         'í¥ê¸°ë¡­ê²': '향기롭게',\n",
    "#                         'ì¤ê¸°ì¼ì´': '수기일이',\n",
    "#                         'ììëª¨': '용모',\n",
    "#                         'ê·¹': '극'\n",
    "#                     }\n",
    "                    \n",
    "#                     result = decoded\n",
    "#                     for broken, fixed in broken_patterns.items():\n",
    "#                         result = result.replace(broken, fixed)\n",
    "                    \n",
    "#                     if result != decoded:\n",
    "#                         return result\n",
    "                        \n",
    "#                 except:\n",
    "#                     pass\n",
    "            \n",
    "#             return decoded\n",
    "            \n",
    "#         except Exception:\n",
    "#             return text\n",
    "\n",
    "#     def _has_broken_korean(self, text: str) -> bool:\n",
    "#         \"\"\"깨진 한글 패턴이 있는지 확인\"\"\"\n",
    "#         broken_patterns = [\n",
    "#             'ë¶', 'ìë', 'ê¼¬', 'ë', 'í¼', 'ì¸', 'ê¸°', 'ê¸', 'ìì¹', 'ì', 'ê',\n",
    "#             'ê·¹ì', 'í¥ê¸°', 'ì¤ê¸°', 'ììë', 'ëª¨'\n",
    "#         ]\n",
    "#         return any(pattern in text for pattern in broken_patterns)\n",
    "\n",
    "#     def _export_to_txt(self, output_file: str):\n",
    "#         \"\"\"파싱 결과를 txt 파일로 출력\"\"\"\n",
    "#         try:\n",
    "#             with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(f\"=== 총 {len(self.logs)}개의 로그 발견 ===\\n\\n\")\n",
    "                \n",
    "#                 for parsed_log in self.parsed_logs:\n",
    "#                     f.write(\"=\" * 60 + \"\\n\")\n",
    "#                     f.write(f\"로그 #{parsed_log['log_number']} - {parsed_log['timestamp']}\\n\")\n",
    "#                     f.write(\"=\" * 60 + \"\\n\")\n",
    "                    \n",
    "#                     params = parsed_log['params']\n",
    "#                     if params:\n",
    "#                         # URL 정보 (첫 번째 로그에서 추출)\n",
    "#                         for log_num, log_data in self.logs:\n",
    "#                             if log_num == parsed_log['log_number']:\n",
    "#                                 if 'REQUEST' in log_data:\n",
    "#                                     url = log_data['REQUEST'].encode('utf-8').decode('unicode_escape')\n",
    "#                                     parsed_url = urlparse(url)\n",
    "#                                     f.write(f\"URL: {parsed_url.netloc}{parsed_url.path}\\n\")\n",
    "#                                     f.write(f\"총 파라미터 수: {len(params)}\\n\")\n",
    "#                                     f.write(\"-\" * 50 + \"\\n\")\n",
    "#                                 break\n",
    "                        \n",
    "#                         # 파라미터 출력\n",
    "#                         for key, value in params.items():\n",
    "#                             f.write(f\"{key:20} : {value}\\n\")\n",
    "#                     else:\n",
    "#                         f.write(\"REQUEST 필드가 없습니다.\\n\")\n",
    "                    \n",
    "#                     f.write(\"\\n\")\n",
    "            \n",
    "#             print(f\"✅ txt 파일 저장: {output_file}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ txt 파일 저장 오류: {e}\")\n",
    "\n",
    "#     def _export_to_excel(self, output_file: str):\n",
    "#         \"\"\"파싱 결과를 Excel 파일로 출력 (page_id, click_type, act_type, click_text, key_list, value_list, len(key_list))\"\"\"\n",
    "#         try:\n",
    "#             excel_data = []\n",
    "            \n",
    "#             for parsed_log in self.parsed_logs:\n",
    "#                 params = parsed_log['params']\n",
    "                \n",
    "#                 # 각 로그에서 필요한 정보 추출\n",
    "#                 page_id = params.get('page_id', '')\n",
    "#                 click_type = params.get('click_type', '')\n",
    "#                 act_type = params.get('act_type', '')\n",
    "#                 click_text = params.get('click_text', '')\n",
    "                \n",
    "#                 # key_list와 value_list 생성\n",
    "#                 key_list = list(params.keys())\n",
    "#                 value_list = [self._fix_korean_text(str(v)) for v in params.values()]  # value도 한글 디코딩 적용\n",
    "                \n",
    "#                 key_list_str = ', '.join(key_list)\n",
    "#                 value_list_str = ', '.join(value_list)\n",
    "#                 key_count = len(key_list)\n",
    "                \n",
    "#                 excel_data.append({\n",
    "#                     'page_id': page_id,\n",
    "#                     'click_type': click_type,\n",
    "#                     'act_type': act_type,\n",
    "#                     'click_text': click_text,\n",
    "#                     'key_list': key_list_str,\n",
    "#                     'value_list': value_list_str,\n",
    "#                     'len(key_list)': key_count\n",
    "#                 })\n",
    "            \n",
    "#             # DataFrame 생성 및 Excel 저장\n",
    "#             df = pd.DataFrame(excel_data)\n",
    "#             df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "            \n",
    "#             print(f\"✅ Excel 파일 저장: {output_file}\")\n",
    "#             print(f\"   총 {len(excel_data)}개 로그 처리됨\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Excel 파일 저장 오류: {e}\")\n",
    "\n",
    "#     def analyze_logs(self, file_path: str):\n",
    "#         \"\"\"로그 분석 (기존 기능 유지) - 오류 로그도 출력\"\"\"\n",
    "#         try:\n",
    "#             with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#                 content = file.read()\n",
    "            \n",
    "#             # 오류 로그도 찾기 위해 별도 파싱\n",
    "#             lines = content.strip().split('\\n')\n",
    "#             json_errors = []\n",
    "            \n",
    "#             for i, line in enumerate(lines):\n",
    "#                 if line.strip():\n",
    "#                     try:\n",
    "#                         json.loads(line.strip())\n",
    "#                     except json.JSONDecodeError as e:\n",
    "#                         json_errors.append((i+1, str(e), line.strip()))\n",
    "            \n",
    "#             # 오류 로그 출력\n",
    "#             if json_errors:\n",
    "#                 print(\"=\"*80)\n",
    "#                 print(f\"🚨 JSON 파싱 오류 로그 상세 분석\")\n",
    "#                 print(\"=\"*80)\n",
    "#                 for line_num, error_msg, original_line in json_errors:\n",
    "#                     print(f\"라인 {line_num}: {error_msg}\")\n",
    "#                     print(f\"원본 로그: {original_line[:300]}...\" if len(original_line) > 300 else f\"원본 로그: {original_line}\")\n",
    "#                     print(\"-\" * 80)\n",
    "#                 print()\n",
    "            \n",
    "#             # 기존 파싱 로직 실행\n",
    "#             self._parse_log_content(content)\n",
    "            \n",
    "#             # 콘솔 출력\n",
    "#             for parsed_log in self.parsed_logs:\n",
    "#                 print(\"=\" * 60)\n",
    "#                 print(f\"로그 #{parsed_log['log_number']} - {parsed_log['timestamp']}\")\n",
    "#                 print(\"=\" * 60)\n",
    "                \n",
    "#                 params = parsed_log['params']\n",
    "#                 if params:\n",
    "#                     # URL 정보 출력\n",
    "#                     for log_num, log_data in self.logs:\n",
    "#                         if log_num == parsed_log['log_number']:\n",
    "#                             if 'REQUEST' in log_data:\n",
    "#                                 url = log_data['REQUEST'].encode('utf-8').decode('unicode_escape')\n",
    "#                                 parsed_url = urlparse(url)\n",
    "#                                 print(f\"URL: {parsed_url.netloc}{parsed_url.path}\")\n",
    "#                                 print(f\"총 파라미터 수: {len(params)}\")\n",
    "#                                 print(\"-\" * 50)\n",
    "#                             break\n",
    "                    \n",
    "#                     # 파라미터 출력\n",
    "#                     for key, value in params.items():\n",
    "#                         print(f\"{key:20} : {value}\")\n",
    "#                 else:\n",
    "#                     print(\"REQUEST 필드가 없습니다.\")\n",
    "                \n",
    "#                 print()\n",
    "                \n",
    "#         except FileNotFoundError:\n",
    "#             print(f\"❌ 파일을 찾을 수 없습니다: {file_path}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ 파일 읽기 오류: {e}\")\n",
    "\n",
    "# # 사용 예시\n",
    "# def run_example():\n",
    "#     \"\"\"사용 예시\"\"\"\n",
    "#     analyzer = LogAnalyzer()\n",
    "    \n",
    "#     # 1. 로그 분석 및 txt, Excel 파일 생성\n",
    "#     analyzer.parse_log_file_from_path(\n",
    "#         file_path=\"log_file.txt\",\n",
    "#         output_txt=\"log_analysis_result.txt\",\n",
    "#         output_xlsx=\"log_analysis_result.xlsx\"\n",
    "#     )\n",
    "    \n",
    "#     # 2. 콘솔에 로그 분석 결과 출력만 하고 싶은 경우\n",
    "#     # analyzer.analyze_logs(\"log_file.txt\")\n",
    "\n",
    "# # 기존 QA 검증 클래스 (기존 기능 유지)\n",
    "# class LogQAValidator:\n",
    "#     def __init__(self):\n",
    "#         self.scenarios = []\n",
    "#         self.logs = []\n",
    "#         self.unique_keys = []\n",
    "        \n",
    "#     def load_scenario_csv(self, csv_file_path: str):\n",
    "#         \"\"\"CSV 파일에서 시나리오 로드\"\"\"\n",
    "#         self.scenarios = []\n",
    "#         df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "        \n",
    "#         # 컬럼 구성에 따라 고유키 조합 결정\n",
    "#         columns = df.columns.tolist()\n",
    "#         if 'page_id' in columns:\n",
    "#             self.unique_keys = ['page_id', 'click_type', 'act_type']\n",
    "#             print(\"🔑 고유키 조합: page_id + click_type + act_type\")\n",
    "#         elif 'click_text' in columns:\n",
    "#             self.unique_keys = ['click_text', 'click_type', 'act_type']\n",
    "#             print(\"🔑 고유키 조합: click_text + click_type + act_type\")\n",
    "#         else:\n",
    "#             raise ValueError(\"지원하지 않는 시나리오 형식입니다. page_id 또는 click_text 컬럼이 필요합니다.\")\n",
    "        \n",
    "#         for _, row in df.iterrows():\n",
    "#             key_list = [k.strip() for k in str(row['key_list']).split(',') if k.strip()]\n",
    "#             scenario = {\n",
    "#                 'expected_keys': key_list\n",
    "#             }\n",
    "            \n",
    "#             # 고유키 값들 추가\n",
    "#             for key in self.unique_keys:\n",
    "#                 scenario[key] = str(row[key]) if pd.notna(row[key]) else ''\n",
    "            \n",
    "#             self.scenarios.append(scenario)\n",
    "        \n",
    "#         print(f\"✅ 시나리오 로드: {len(self.scenarios)}개\")\n",
    "\n",
    "#     def parse_logs_from_file(self, log_file_path: str):\n",
    "#         \"\"\"로그 txt 파일에서 직접 파싱\"\"\"\n",
    "#         try:\n",
    "#             with open(log_file_path, 'r', encoding='utf-8') as file:\n",
    "#                 log_content = file.read()\n",
    "#             self.parse_logs(log_content)\n",
    "#             print(f\"✅ 로그 파일 로드: {log_file_path}\")\n",
    "#         except FileNotFoundError:\n",
    "#             print(f\"❌ 파일을 찾을 수 없습니다: {log_file_path}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ 파일 읽기 오류: {e}\")\n",
    "\n",
    "#     def parse_logs(self, log_content: str):\n",
    "#         \"\"\"로그 파싱 - 안전한 JSON 파싱\"\"\"\n",
    "#         self.logs = []\n",
    "#         lines = log_content.strip().split('\\n')\n",
    "        \n",
    "#         json_errors = []\n",
    "        \n",
    "#         for i, line in enumerate(lines):\n",
    "#             if not line.strip():\n",
    "#                 continue\n",
    "                \n",
    "#             try:\n",
    "#                 log_data = json.loads(line.strip())\n",
    "#                 if 'REQUEST' not in log_data:\n",
    "#                     continue\n",
    "                    \n",
    "#                 params = self._parse_url(log_data['REQUEST'])\n",
    "#                 if params:\n",
    "#                     self.logs.append({\n",
    "#                         'line': i + 1,\n",
    "#                         'timestamp': log_data.get('LOGTIME', ''),\n",
    "#                         'params': params\n",
    "#                     })\n",
    "#             except json.JSONDecodeError as e:\n",
    "#                 json_errors.append((i+1, str(e), line.strip()))\n",
    "                \n",
    "#                 # JSON 수정 시도\n",
    "#                 fixed_line = self._try_fix_json_simple(line.strip())\n",
    "#                 if fixed_line:\n",
    "#                     try:\n",
    "#                         log_data = json.loads(fixed_line)\n",
    "#                         if 'REQUEST' in log_data:\n",
    "#                             params = self._parse_url(log_data['REQUEST'])\n",
    "#                             if params:\n",
    "#                                 self.logs.append({\n",
    "#                                     'line': i + 1,\n",
    "#                                     'timestamp': log_data.get('LOGTIME', ''),\n",
    "#                                     'params': params\n",
    "#                                 })\n",
    "#                                 print(f\"✅ JSON 수정 성공 (라인 {i+1})\")\n",
    "#                     except:\n",
    "#                         pass\n",
    "        \n",
    "#         # 오류 상세 출력\n",
    "#         if json_errors:\n",
    "#             print(f\"⚠️ JSON 파싱 오류: {len(json_errors)}개\")\n",
    "#             print(\"=\"*80)\n",
    "#             for line_num, error_msg, original_line in json_errors:\n",
    "#                 print(f\"라인 {line_num}: {error_msg}\")\n",
    "#                 print(f\"원본 로그: {original_line[:200]}...\" if len(original_line) > 200 else f\"원본 로그: {original_line}\")\n",
    "#                 print(\"-\" * 80)\n",
    "#             print(f\"✅ 성공적으로 파싱된 로그: {len(self.logs)}개\")\n",
    "#         else:\n",
    "#             print(f\"✅ 로그 파싱: {len(self.logs)}개\")\n",
    "\n",
    "#     def _try_fix_json_simple(self, line: str) -> str:\n",
    "#         \"\"\"간단한 JSON 수정 시도 - QA용\"\"\"\n",
    "#         try:\n",
    "#             # 1. 마지막 쉼표 제거\n",
    "#             if line.endswith(',}'):\n",
    "#                 line = line[:-2] + '}'\n",
    "#             elif line.endswith(',]'):\n",
    "#                 line = line[:-2] + ']'\n",
    "            \n",
    "#             # 2. 값 내부의 따옴표 문제 해결\n",
    "#             import re\n",
    "            \n",
    "#             # 문제가 되는 필드들의 값에서 따옴표를 작은따옴표로 변경\n",
    "#             problem_fields = ['click_text', 'article_title', 'banner_text']\n",
    "            \n",
    "#             for field in problem_fields:\n",
    "#                 # \"field\":\"value with \"quotes\"\" 패턴 찾기\n",
    "#                 pattern = f'\"{field}\":\"([^\"]*\"[^\"]*)\"'\n",
    "                \n",
    "#                 def fix_quotes(match):\n",
    "#                     value = match.group(1)\n",
    "#                     # 값 내부의 따옴표를 작은따옴표로 변경\n",
    "#                     fixed_value = value.replace('\"', \"'\")\n",
    "#                     return f'\"{field}\":\"{fixed_value}\"'\n",
    "                \n",
    "#                 line = re.sub(pattern, fix_quotes, line)\n",
    "            \n",
    "#             return line\n",
    "            \n",
    "#         except:\n",
    "#             return None\n",
    "\n",
    "#     def _parse_url(self, request_url: str) -> Dict[str, str]:\n",
    "#         \"\"\"URL 파라미터 파싱 (QA용)\"\"\"\n",
    "#         try:\n",
    "#             url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "#             parsed = urlparse(url)\n",
    "            \n",
    "#             # 동일한 개선된 파싱 로직 적용\n",
    "#             query = parsed.query\n",
    "#             result = {}\n",
    "            \n",
    "#             if not query:\n",
    "#                 return result\n",
    "            \n",
    "#             # &로 분할하되, URL 인코딩된 &(%26)은 분할하지 않음\n",
    "#             params = []\n",
    "#             current_param = \"\"\n",
    "#             i = 0\n",
    "            \n",
    "#             while i < len(query):\n",
    "#                 if query[i:i+1] == '&' and not query[max(0, i-3):i] == '%26':\n",
    "#                     if current_param:\n",
    "#                         params.append(current_param)\n",
    "#                     current_param = \"\"\n",
    "#                 else:\n",
    "#                     current_param += query[i]\n",
    "#                 i += 1\n",
    "            \n",
    "#             if current_param:\n",
    "#                 params.append(current_param)\n",
    "            \n",
    "#             # 각 파라미터를 키=값으로 분할\n",
    "#             for param in params:\n",
    "#                 if '=' not in param:\n",
    "#                     continue\n",
    "                    \n",
    "#                 # 첫 번째 = 기준으로만 분할\n",
    "#                 key, value = param.split('=', 1)\n",
    "                \n",
    "#                 # 키와 값 디코딩\n",
    "#                 decoded_key = unquote(key, encoding='utf-8')\n",
    "#                 decoded_value = unquote(value, encoding='utf-8')\n",
    "                \n",
    "#                 # 한글 깨짐 복구 시도\n",
    "#                 try:\n",
    "#                     if any(ord(c) > 127 for c in decoded_key):\n",
    "#                         decoded_key = decoded_key.encode('latin-1').decode('utf-8')\n",
    "#                     if any(ord(c) > 127 for c in decoded_value):\n",
    "#                         decoded_value = decoded_value.encode('latin-1').decode('utf-8')\n",
    "#                 except:\n",
    "#                     pass\n",
    "                    \n",
    "#                 result[decoded_key] = decoded_value\n",
    "            \n",
    "#             return result\n",
    "#         except:\n",
    "#             # 실패 시 기존 방식으로 fallback\n",
    "#             try:\n",
    "#                 url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "#                 parsed = urlparse(url)\n",
    "#                 query_params = parse_qs(parsed.query, keep_blank_values=True)\n",
    "                \n",
    "#                 result = {}\n",
    "#                 for key, values in query_params.items():\n",
    "#                     if values:\n",
    "#                         decoded = unquote(values[0], encoding='utf-8')\n",
    "#                         try:\n",
    "#                             if any(ord(c) > 127 for c in decoded):\n",
    "#                                 decoded = decoded.encode('latin-1').decode('utf-8')\n",
    "#                         except:\n",
    "#                             pass\n",
    "#                         result[key] = decoded\n",
    "#                 return result\n",
    "#             except:\n",
    "#                 return {}\n",
    "\n",
    "#     def validate_and_export(self, output_file: str):\n",
    "#         \"\"\"검증 수행 및 결과 XLSX 출력\"\"\"\n",
    "#         results = []\n",
    "        \n",
    "#         for scenario in self.scenarios:\n",
    "#             # 시나리오와 매칭되는 로그 찾기\n",
    "#             matched_logs = self._find_matching_logs(scenario)\n",
    "            \n",
    "#             if not matched_logs:\n",
    "#                 # 매칭되는 로그가 없는 경우\n",
    "#                 for key in scenario['expected_keys']:\n",
    "#                     result = {\n",
    "#                         'key': key,\n",
    "#                         'value': 'NOT_FOUND',\n",
    "#                         'pass': 'FAIL'\n",
    "#                     }\n",
    "                    \n",
    "#                     # 고유키 값들 추가\n",
    "#                     for uk in self.unique_keys:\n",
    "#                         result[uk] = scenario.get(uk, '')\n",
    "                    \n",
    "#                     results.append(result)\n",
    "#                 continue\n",
    "            \n",
    "#             # 각 매칭된 로그에 대해 검증\n",
    "#             for log in matched_logs:\n",
    "#                 actual_keys = set(log['params'].keys())\n",
    "#                 expected_keys = set(scenario['expected_keys'])\n",
    "                \n",
    "#                 # 예상 키들 검증\n",
    "#                 for key in expected_keys:\n",
    "#                     value = log['params'].get(key, 'MISSING')\n",
    "#                     pass_status = 'PASS' if key in actual_keys else 'FAIL'\n",
    "                    \n",
    "#                     result = {\n",
    "#                         'key': key,\n",
    "#                         'value': value,\n",
    "#                         'pass': pass_status\n",
    "#                     }\n",
    "                    \n",
    "#                     # 고유키 값들 추가\n",
    "#                     for uk in self.unique_keys:\n",
    "#                         result[uk] = scenario.get(uk, '')\n",
    "                    \n",
    "#                     results.append(result)\n",
    "                \n",
    "#                 # 예상치 못한 키들 (선택사항)\n",
    "#                 unexpected_keys = actual_keys - expected_keys\n",
    "#                 for key in unexpected_keys:\n",
    "#                     result = {\n",
    "#                         'key': key,\n",
    "#                         'value': log['params'][key],\n",
    "#                         'pass': 'UNEXPECTED'\n",
    "#                     }\n",
    "                    \n",
    "#                     # 고유키 값들 추가\n",
    "#                     for uk in self.unique_keys:\n",
    "#                         result[uk] = scenario.get(uk, '')\n",
    "                    \n",
    "#                     results.append(result)\n",
    "\n",
    "#         # XLSX 출력 - 컬럼 순서 정렬\n",
    "#         df_results = pd.DataFrame(results)\n",
    "        \n",
    "#         # 고유키 조합에 따른 컬럼 순서 정의\n",
    "#         if 'page_id' in self.unique_keys:\n",
    "#             column_order = ['page_id', 'click_type', 'act_type', 'key', 'value', 'pass']\n",
    "#         else:  # click_text 기반\n",
    "#             column_order = ['click_text', 'click_type', 'act_type', 'key', 'value', 'pass']\n",
    "        \n",
    "#         # 컬럼 순서 적용\n",
    "#         df_results = df_results[column_order]\n",
    "#         df_results.to_excel(output_file, index=False, engine='openpyxl')\n",
    "        \n",
    "#         # 요약 출력\n",
    "#         self._print_summary(results)\n",
    "#         print(f\"✅ 결과 저장: {output_file}\")\n",
    "\n",
    "#     def _find_matching_logs(self, scenario: Dict) -> List[Dict]:\n",
    "#         \"\"\"시나리오와 매칭되는 로그 찾기\"\"\"\n",
    "#         matched = []\n",
    "        \n",
    "#         for log in self.logs:\n",
    "#             params = log['params']\n",
    "            \n",
    "#             # 동적 고유키로 매칭\n",
    "#             is_match = True\n",
    "#             for key in self.unique_keys:\n",
    "#                 if params.get(key, '') != scenario.get(key, ''):\n",
    "#                     is_match = False\n",
    "#                     break\n",
    "            \n",
    "#             if is_match:\n",
    "#                 matched.append(log)\n",
    "        \n",
    "#         return matched\n",
    "\n",
    "#     def _print_summary(self, results: List[Dict]):\n",
    "#         \"\"\"결과 요약 출력\"\"\"\n",
    "#         df = pd.DataFrame(results)\n",
    "        \n",
    "#         total = len(df)\n",
    "#         passed = len(df[df['pass'] == 'PASS'])\n",
    "#         failed = len(df[df['pass'] == 'FAIL'])\n",
    "#         unexpected = len(df[df['pass'] == 'UNEXPECTED'])\n",
    "        \n",
    "#         print(f\"\\n📊 검증 결과:\")\n",
    "#         print(f\"   전체: {total}개\")\n",
    "#         print(f\"   통과: {passed}개\")\n",
    "#         print(f\"   실패: {failed}개\")\n",
    "#         print(f\"   예상외: {unexpected}개\")\n",
    "#         print(f\"   성공률: {passed/total*100:.1f}%\" if total > 0 else \"   성공률: 0%\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 로그 분석 및 파일 출력 예시\n",
    "#     print(\"=== 로그 분석 및 파일 출력 ===\")\n",
    "#     run_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 따옴표 강제 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 로그 분석 및 파일 출력 ===\n",
      "✅ 강제 JSON 수정 성공 (라인 1)\n",
      "✅ 강제 JSON 수정 성공 (라인 2)\n",
      "⚠️ JSON 파싱 오류 발생: 2개\n",
      "✅ 수정 성공: 2개\n",
      "✅ 최종 파싱된 로그: 3개\n",
      "=== 총 3개의 로그 처리 ===\n",
      "\n",
      "✅ txt 파일 저장: ./result/log_analysis_result_2025-06-04_191629.txt\n",
      "✅ Excel 파일 저장: ./result/log_analysis_result_2025-06-04_191629.xlsx\n",
      "   총 3개 로그 처리됨\n",
      "✅ 강제 JSON 수정 성공 (라인 1)\n",
      "✅ 강제 JSON 수정 성공 (라인 2)\n",
      "⚠️ JSON 파싱 오류 발생: 2개\n",
      "✅ 수정 성공: 2개\n",
      "✅ 최종 파싱된 로그: 3개\n",
      "=== 총 3개의 로그 처리 ===\n",
      "\n",
      "✅ txt 파일 저장: ./result/log_analysis_result.txt\n",
      "✅ Excel 파일 저장: ./result/log_analysis_result.xlsx\n",
      "   총 3개 로그 처리됨\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from urllib.parse import parse_qs, urlparse, unquote\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "class LogAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "        self.parsed_logs = []\n",
    "\n",
    "    def parse_log_file_from_path(self, file_path: str, output_txt: str = None, output_xlsx: str = None):\n",
    "        \"\"\"파일 경로에서 로그 파일을 읽어서 분석하고 결과 출력\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            # 로그 파싱\n",
    "            self._parse_log_content(content)\n",
    "            \n",
    "            # txt 파일 출력\n",
    "            if output_txt:\n",
    "                self._export_to_txt(output_txt)\n",
    "            \n",
    "            # Excel 파일 출력\n",
    "            if output_xlsx:\n",
    "                self._export_to_excel(output_xlsx)\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ 파일을 찾을 수 없습니다: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 읽기 오류: {e}\")\n",
    "\n",
    "    def _parse_log_content(self, content: str):\n",
    "        \"\"\"로그 내용 파싱 - 안전한 JSON 파싱\"\"\"\n",
    "        self.logs = []\n",
    "        self.parsed_logs = []\n",
    "        lines = content.strip().split('\\n')\n",
    "        \n",
    "        json_errors = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    log_data = json.loads(line.strip())\n",
    "                    self.logs.append((i+1, log_data))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    json_errors.append((i+1, str(e), line.strip()))\n",
    "                    \n",
    "                    # JSON 수정 시도\n",
    "                    fixed_line = self._try_fix_json(line.strip())\n",
    "                    if fixed_line:\n",
    "                        try:\n",
    "                            log_data = json.loads(fixed_line)\n",
    "                            self.logs.append((i+1, log_data))\n",
    "                            print(f\"✅ JSON 수정 성공 (라인 {i+1})\")\n",
    "                        except:\n",
    "                            # 추가 수정 시도\n",
    "                            fixed_line2 = self._force_fix_json(line.strip())\n",
    "                            if fixed_line2:\n",
    "                                try:\n",
    "                                    log_data = json.loads(fixed_line2)\n",
    "                                    self.logs.append((i+1, log_data))\n",
    "                                    print(f\"✅ 강제 JSON 수정 성공 (라인 {i+1})\")\n",
    "                                except:\n",
    "                                    pass\n",
    "        \n",
    "        # 오류 요약 출력\n",
    "        if json_errors:\n",
    "            print(f\"⚠️ JSON 파싱 오류 발생: {len(json_errors)}개\")\n",
    "            successful_fixes = len(self.logs) - (len(lines) - len(json_errors))\n",
    "            if successful_fixes > 0:\n",
    "                print(f\"✅ 수정 성공: {successful_fixes}개\")\n",
    "            print(f\"✅ 최종 파싱된 로그: {len(self.logs)}개\")\n",
    "        else:\n",
    "            print(f\"✅ 모든 JSON 파싱 성공\")\n",
    "        \n",
    "        print(f\"=== 총 {len(self.logs)}개의 로그 처리 ===\\n\")\n",
    "        \n",
    "        # 각 로그의 REQUEST 파라미터 분석\n",
    "        for log_num, log_data in self.logs:\n",
    "            if 'REQUEST' in log_data:\n",
    "                params = self._parse_request_url(log_data['REQUEST'])\n",
    "                if params:\n",
    "                    parsed_log = {\n",
    "                        'log_number': log_num,\n",
    "                        'timestamp': log_data.get('LOGTIME', 'N/A'),\n",
    "                        'params': params\n",
    "                    }\n",
    "                    self.parsed_logs.append(parsed_log)\n",
    "\n",
    "    def _try_fix_json(self, line: str) -> str:\n",
    "        \"\"\"JSON 수정 시도 - 1차 수정\"\"\"\n",
    "        try:\n",
    "            # 1. 마지막 쉼표 제거\n",
    "            if line.endswith(',}'):\n",
    "                line = line[:-2] + '}'\n",
    "            elif line.endswith(',]'):\n",
    "                line = line[:-2] + ']'\n",
    "            \n",
    "            # 2. 값 내부의 따옴표를 작은따옴표로 변경\n",
    "            import re\n",
    "            \n",
    "            # 모든 \"key\":\"value with quotes\" 패턴을 찾아서 수정\n",
    "            def fix_quotes_in_value(match):\n",
    "                key = match.group(1)\n",
    "                value = match.group(2)\n",
    "                # 값 내부의 모든 따옴표를 작은따옴표로 변경\n",
    "                fixed_value = value.replace('\"', \"'\")\n",
    "                return f'\"{key}\":\"{fixed_value}\"'\n",
    "            \n",
    "            # 값에 따옴표가 포함된 패턴 찾기\n",
    "            problem_pattern = r'\"([^\"]+)\":\"([^\"]*\"[^\"]*)\"'\n",
    "            \n",
    "            # 여러 번 적용하여 모든 문제 해결\n",
    "            prev_line = \"\"\n",
    "            iterations = 0\n",
    "            while line != prev_line and iterations < 5:\n",
    "                prev_line = line\n",
    "                line = re.sub(problem_pattern, fix_quotes_in_value, line)\n",
    "                iterations += 1\n",
    "            \n",
    "            return line\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _force_fix_json(self, line: str) -> str:\n",
    "        \"\"\"JSON 강제 수정 - 2차 수정\"\"\"\n",
    "        try:\n",
    "            # 기본 정리\n",
    "            if line.endswith(',}'):\n",
    "                line = line[:-2] + '}'\n",
    "            elif line.endswith(',]'):\n",
    "                line = line[:-2] + ']'\n",
    "            \n",
    "            # JSON 구조 분석하여 키-값 쌍별로 처리\n",
    "            if not (line.startswith('{') and line.endswith('}')):\n",
    "                return None\n",
    "            \n",
    "            # 중괄호 제거\n",
    "            content = line[1:-1]\n",
    "            \n",
    "            # 키-값 쌍들을 분리\n",
    "            pairs = []\n",
    "            current_pair = \"\"\n",
    "            bracket_count = 0\n",
    "            in_quotes = False\n",
    "            escape_next = False\n",
    "            \n",
    "            for char in content:\n",
    "                if escape_next:\n",
    "                    current_pair += char\n",
    "                    escape_next = False\n",
    "                    continue\n",
    "                \n",
    "                if char == '\\\\':\n",
    "                    current_pair += char\n",
    "                    escape_next = True\n",
    "                    continue\n",
    "                \n",
    "                if char == '\"' and not escape_next:\n",
    "                    in_quotes = not in_quotes\n",
    "                \n",
    "                if not in_quotes:\n",
    "                    if char in '{}[]':\n",
    "                        bracket_count += 1 if char in '{[' else -1\n",
    "                    elif char == ',' and bracket_count == 0:\n",
    "                        pairs.append(current_pair.strip())\n",
    "                        current_pair = \"\"\n",
    "                        continue\n",
    "                \n",
    "                current_pair += char\n",
    "            \n",
    "            if current_pair.strip():\n",
    "                pairs.append(current_pair.strip())\n",
    "            \n",
    "            # 각 키-값 쌍을 수정\n",
    "            fixed_pairs = []\n",
    "            for pair in pairs:\n",
    "                if ':' in pair:\n",
    "                    try:\n",
    "                        # 키와 값 분리 (첫 번째 콜론 기준)\n",
    "                        colon_pos = pair.find(':')\n",
    "                        key_part = pair[:colon_pos].strip()\n",
    "                        value_part = pair[colon_pos+1:].strip()\n",
    "                        \n",
    "                        # 값 부분에서 따옴표로 둘러싸인 문자열인지 확인\n",
    "                        if value_part.startswith('\"') and value_part.endswith('\"'):\n",
    "                            # 값 내부의 따옴표를 작은따옴표로 변경\n",
    "                            inner_value = value_part[1:-1]  # 양쪽 따옴표 제거\n",
    "                            fixed_inner = inner_value.replace('\"', \"'\")\n",
    "                            fixed_value = f'\"{fixed_inner}\"'\n",
    "                            fixed_pairs.append(f'{key_part}:{fixed_value}')\n",
    "                        else:\n",
    "                            fixed_pairs.append(pair)\n",
    "                    except:\n",
    "                        fixed_pairs.append(pair)\n",
    "                else:\n",
    "                    fixed_pairs.append(pair)\n",
    "            \n",
    "            # 다시 조합\n",
    "            result = '{' + ','.join(fixed_pairs) + '}'\n",
    "            return result\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _parse_request_url(self, request_url: str) -> Dict[str, str]:\n",
    "        \"\"\"REQUEST URL에서 파라미터를 파싱하고 한글을 올바르게 처리\"\"\"\n",
    "        try:\n",
    "            # 유니코드 이스케이프 디코딩\n",
    "            url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "            parsed_url = urlparse(url)\n",
    "            \n",
    "            # 쿼리 파라미터를 수동으로 파싱 (복잡한 값 처리)\n",
    "            query = parsed_url.query\n",
    "            processed_params = {}\n",
    "            \n",
    "            if not query:\n",
    "                return processed_params\n",
    "            \n",
    "            # &로 분할하되, URL 인코딩된 &(%26)은 분할하지 않음\n",
    "            params = []\n",
    "            current_param = \"\"\n",
    "            i = 0\n",
    "            \n",
    "            while i < len(query):\n",
    "                if query[i:i+1] == '&' and not query[max(0, i-3):i] == '%26':\n",
    "                    if current_param:\n",
    "                        params.append(current_param)\n",
    "                    current_param = \"\"\n",
    "                else:\n",
    "                    current_param += query[i]\n",
    "                i += 1\n",
    "            \n",
    "            if current_param:\n",
    "                params.append(current_param)\n",
    "            \n",
    "            # 각 파라미터를 키=값으로 분할\n",
    "            for param in params:\n",
    "                if '=' not in param:\n",
    "                    continue\n",
    "                    \n",
    "                # 첫 번째 = 기준으로만 분할 (값에 =이 포함될 수 있음)\n",
    "                key, value = param.split('=', 1)\n",
    "                \n",
    "                # 키와 값 디코딩\n",
    "                decoded_key = self._fix_korean_text(key)\n",
    "                decoded_value = self._fix_korean_text(value)\n",
    "                \n",
    "                processed_params[decoded_key] = decoded_value\n",
    "            \n",
    "            return processed_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"파싱 오류: {e}\")\n",
    "            # 실패 시 기존 방식으로 fallback\n",
    "            try:\n",
    "                url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "                parsed_url = urlparse(url)\n",
    "                query_params = parse_qs(parsed_url.query, keep_blank_values=True)\n",
    "                \n",
    "                processed_params = {}\n",
    "                for key, value_list in query_params.items():\n",
    "                    raw_value = value_list[0] if value_list else ''\n",
    "                    decoded_value = self._fix_korean_text(raw_value)\n",
    "                    processed_params[key] = decoded_value\n",
    "                \n",
    "                return processed_params\n",
    "            except:\n",
    "                return {}\n",
    "\n",
    "    def _fix_korean_text(self, text: str) -> str:\n",
    "        \"\"\"깨진 한글을 올바르게 디코딩\"\"\"\n",
    "        try:\n",
    "            # URL 디코딩 먼저 시도\n",
    "            decoded = unquote(text, encoding='utf-8')\n",
    "            \n",
    "            # 깨진 한글 패턴이 있는지 확인\n",
    "            if self._has_broken_korean(decoded):\n",
    "                # Latin-1로 인코딩 후 UTF-8로 디코딩 (일반적인 mojibake 해결법)\n",
    "                try:\n",
    "                    fixed = decoded.encode('latin-1').decode('utf-8')\n",
    "                    return fixed\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                # 그래도 안되면 바이트 단위로 처리\n",
    "                try:\n",
    "                    # 문자열의 각 문자를 바이트로 변환 후 UTF-8 디코딩\n",
    "                    byte_data = bytes([ord(c) for c in decoded if ord(c) < 256])\n",
    "                    fixed = byte_data.decode('utf-8', errors='ignore')\n",
    "                    return fixed\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                # UTF-8 바이트 패턴으로 복구 시도\n",
    "                try:\n",
    "                    # 특정 깨진 패턴들을 직접 매핑\n",
    "                    broken_patterns = {\n",
    "                        'ê·¹ììëª¨': '극장용모',\n",
    "                        'í¥ê¸°ë¡­ê²': '향기롭게',\n",
    "                        'ì¤ê¸°ì¼ì´': '수기일이',\n",
    "                        'ììëª¨': '용모',\n",
    "                        'ê·¹': '극'\n",
    "                    }\n",
    "                    \n",
    "                    result = decoded\n",
    "                    for broken, fixed in broken_patterns.items():\n",
    "                        result = result.replace(broken, fixed)\n",
    "                    \n",
    "                    if result != decoded:\n",
    "                        return result\n",
    "                        \n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return decoded\n",
    "            \n",
    "        except Exception:\n",
    "            return text\n",
    "\n",
    "    def _has_broken_korean(self, text: str) -> bool:\n",
    "        \"\"\"깨진 한글 패턴이 있는지 확인\"\"\"\n",
    "        broken_patterns = [\n",
    "            'ë¶', 'ìë', 'ê¼¬', 'ë', 'í¼', 'ì¸', 'ê¸°', 'ê¸', 'ìì¹', 'ì', 'ê',\n",
    "            'ê·¹ì', 'í¥ê¸°', 'ì¤ê¸°', 'ììë', 'ëª¨'\n",
    "        ]\n",
    "        return any(pattern in text for pattern in broken_patterns)\n",
    "\n",
    "    def _export_to_txt(self, output_file: str):\n",
    "        \"\"\"파싱 결과를 txt 파일로 출력\"\"\"\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"=== 총 {len(self.logs)}개의 로그 발견 ===\\n\\n\")\n",
    "                \n",
    "                for parsed_log in self.parsed_logs:\n",
    "                    f.write(\"=\" * 60 + \"\\n\")\n",
    "                    f.write(f\"로그 #{parsed_log['log_number']} - {parsed_log['timestamp']}\\n\")\n",
    "                    f.write(\"=\" * 60 + \"\\n\")\n",
    "                    \n",
    "                    params = parsed_log['params']\n",
    "                    if params:\n",
    "                        # URL 정보 (첫 번째 로그에서 추출)\n",
    "                        for log_num, log_data in self.logs:\n",
    "                            if log_num == parsed_log['log_number']:\n",
    "                                if 'REQUEST' in log_data:\n",
    "                                    url = log_data['REQUEST'].encode('utf-8').decode('unicode_escape')\n",
    "                                    parsed_url = urlparse(url)\n",
    "                                    f.write(f\"URL: {parsed_url.netloc}{parsed_url.path}\\n\")\n",
    "                                    f.write(f\"총 파라미터 수: {len(params)}\\n\")\n",
    "                                    f.write(\"-\" * 50 + \"\\n\")\n",
    "                                break\n",
    "                        \n",
    "                        # 파라미터 출력\n",
    "                        for key, value in params.items():\n",
    "                            f.write(f\"{key:20} : {value}\\n\")\n",
    "                    else:\n",
    "                        f.write(\"REQUEST 필드가 없습니다.\\n\")\n",
    "                    \n",
    "                    f.write(\"\\n\")\n",
    "            \n",
    "            print(f\"✅ txt 파일 저장: {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ txt 파일 저장 오류: {e}\")\n",
    "\n",
    "    def _export_to_excel(self, output_file: str):\n",
    "        \"\"\"파싱 결과를 Excel 파일로 출력\"\"\"\n",
    "        try:\n",
    "            excel_data = []\n",
    "            \n",
    "            for parsed_log in self.parsed_logs:\n",
    "                params = parsed_log['params']\n",
    "                \n",
    "                # 각 로그에서 필요한 정보 추출\n",
    "                page_id = params.get('page_id', '')\n",
    "                click_type = params.get('click_type', '')\n",
    "                act_type = params.get('act_type', '')\n",
    "                click_text = params.get('click_text', '')\n",
    "                \n",
    "                # key_list와 value_list 생성\n",
    "                key_list = list(params.keys())\n",
    "                value_list = [self._fix_korean_text(str(v)) for v in params.values()]\n",
    "                \n",
    "                key_list_str = ', '.join(key_list)\n",
    "                value_list_str = ', '.join(value_list)\n",
    "                key_count = len(key_list)\n",
    "                \n",
    "                excel_data.append({\n",
    "                    'page_id': page_id,\n",
    "                    'click_type': click_type,\n",
    "                    'act_type': act_type,\n",
    "                    'click_text': click_text,\n",
    "                    'key_list': key_list_str,\n",
    "                    'value_list': value_list_str,\n",
    "                    'len(key_list)': key_count\n",
    "                })\n",
    "            \n",
    "            # DataFrame 생성 및 Excel 저장\n",
    "            df = pd.DataFrame(excel_data)\n",
    "            df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"✅ Excel 파일 저장: {output_file}\")\n",
    "            print(f\"   총 {len(excel_data)}개 로그 처리됨\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Excel 파일 저장 오류: {e}\")\n",
    "\n",
    "    def analyze_logs(self, file_path: str):\n",
    "        \"\"\"로그 분석 (콘솔 출력)\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            self._parse_log_content(content)\n",
    "            \n",
    "            # 콘솔 출력\n",
    "            for parsed_log in self.parsed_logs:\n",
    "                print(\"=\" * 60)\n",
    "                print(f\"로그 #{parsed_log['log_number']} - {parsed_log['timestamp']}\")\n",
    "                print(\"=\" * 60)\n",
    "                \n",
    "                params = parsed_log['params']\n",
    "                if params:\n",
    "                    # URL 정보 출력\n",
    "                    for log_num, log_data in self.logs:\n",
    "                        if log_num == parsed_log['log_number']:\n",
    "                            if 'REQUEST' in log_data:\n",
    "                                url = log_data['REQUEST'].encode('utf-8').decode('unicode_escape')\n",
    "                                parsed_url = urlparse(url)\n",
    "                                print(f\"URL: {parsed_url.netloc}{parsed_url.path}\")\n",
    "                                print(f\"총 파라미터 수: {len(params)}\")\n",
    "                                print(\"-\" * 50)\n",
    "                            break\n",
    "                    \n",
    "                    # 파라미터 출력\n",
    "                    for key, value in params.items():\n",
    "                        print(f\"{key:20} : {value}\")\n",
    "                else:\n",
    "                    print(\"REQUEST 필드가 없습니다.\")\n",
    "                \n",
    "                print()\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ 파일을 찾을 수 없습니다: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 읽기 오류: {e}\")\n",
    "\n",
    "\n",
    "# QA 검증 클래스\n",
    "class LogQAValidator:\n",
    "    def __init__(self):\n",
    "        self.scenarios = []\n",
    "        self.logs = []\n",
    "        self.unique_keys = []\n",
    "        \n",
    "    def load_scenario_csv(self, csv_file_path: str):\n",
    "        \"\"\"CSV 파일에서 시나리오 로드\"\"\"\n",
    "        self.scenarios = []\n",
    "        df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "        \n",
    "        # 컬럼 구성에 따라 고유키 조합 결정\n",
    "        columns = df.columns.tolist()\n",
    "        if 'page_id' in columns:\n",
    "            self.unique_keys = ['page_id', 'click_type', 'act_type']\n",
    "            print(\"🔑 고유키 조합: page_id + click_type + act_type\")\n",
    "        elif 'click_text' in columns:\n",
    "            self.unique_keys = ['click_text', 'click_type', 'act_type']\n",
    "            print(\"🔑 고유키 조합: click_text + click_type + act_type\")\n",
    "        else:\n",
    "            raise ValueError(\"지원하지 않는 시나리오 형식입니다. page_id 또는 click_text 컬럼이 필요합니다.\")\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            key_list = [k.strip() for k in str(row['key_list']).split(',') if k.strip()]\n",
    "            scenario = {\n",
    "                'expected_keys': key_list\n",
    "            }\n",
    "            \n",
    "            # 고유키 값들 추가\n",
    "            for key in self.unique_keys:\n",
    "                scenario[key] = str(row[key]) if pd.notna(row[key]) else ''\n",
    "            \n",
    "            self.scenarios.append(scenario)\n",
    "        \n",
    "        print(f\"✅ 시나리오 로드: {len(self.scenarios)}개\")\n",
    "\n",
    "    def parse_logs_from_file(self, log_file_path: str):\n",
    "        \"\"\"로그 txt 파일에서 직접 파싱\"\"\"\n",
    "        try:\n",
    "            with open(log_file_path, 'r', encoding='utf-8') as file:\n",
    "                log_content = file.read()\n",
    "            self.parse_logs(log_content)\n",
    "            print(f\"✅ 로그 파일 로드: {log_file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ 파일을 찾을 수 없습니다: {log_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 읽기 오류: {e}\")\n",
    "\n",
    "    def parse_logs(self, log_content: str):\n",
    "        \"\"\"로그 파싱\"\"\"\n",
    "        analyzer = LogAnalyzer()\n",
    "        analyzer._parse_log_content(log_content)\n",
    "        \n",
    "        # LogAnalyzer의 결과를 QA용으로 변환\n",
    "        self.logs = []\n",
    "        for log_num, log_data in analyzer.logs:\n",
    "            if 'REQUEST' in log_data:\n",
    "                params = analyzer._parse_request_url(log_data['REQUEST'])\n",
    "                if params:\n",
    "                    self.logs.append({\n",
    "                        'line': log_num,\n",
    "                        'timestamp': log_data.get('LOGTIME', ''),\n",
    "                        'params': params\n",
    "                    })\n",
    "        \n",
    "        print(f\"✅ QA용 로그 파싱: {len(self.logs)}개\")\n",
    "\n",
    "    def validate_and_export(self, output_file: str):\n",
    "        \"\"\"검증 수행 및 결과 XLSX 출력\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for scenario in self.scenarios:\n",
    "            # 시나리오와 매칭되는 로그 찾기\n",
    "            matched_logs = self._find_matching_logs(scenario)\n",
    "            \n",
    "            if not matched_logs:\n",
    "                # 매칭되는 로그가 없는 경우\n",
    "                for key in scenario['expected_keys']:\n",
    "                    result = {\n",
    "                        'key': key,\n",
    "                        'value': 'NOT_FOUND',\n",
    "                        'pass': 'FAIL'\n",
    "                    }\n",
    "                    \n",
    "                    # 고유키 값들 추가\n",
    "                    for uk in self.unique_keys:\n",
    "                        result[uk] = scenario.get(uk, '')\n",
    "                    \n",
    "                    results.append(result)\n",
    "                continue\n",
    "            \n",
    "            # 각 매칭된 로그에 대해 검증\n",
    "            for log in matched_logs:\n",
    "                actual_keys = set(log['params'].keys())\n",
    "                expected_keys = set(scenario['expected_keys'])\n",
    "                \n",
    "                # 예상 키들 검증\n",
    "                for key in expected_keys:\n",
    "                    value = log['params'].get(key, 'MISSING')\n",
    "                    pass_status = 'PASS' if key in actual_keys else 'FAIL'\n",
    "                    \n",
    "                    result = {\n",
    "                        'key': key,\n",
    "                        'value': value,\n",
    "                        'pass': pass_status\n",
    "                    }\n",
    "                    \n",
    "                    # 고유키 값들 추가\n",
    "                    for uk in self.unique_keys:\n",
    "                        result[uk] = scenario.get(uk, '')\n",
    "                    \n",
    "                    results.append(result)\n",
    "                \n",
    "                # 예상치 못한 키들\n",
    "                unexpected_keys = actual_keys - expected_keys\n",
    "                for key in unexpected_keys:\n",
    "                    result = {\n",
    "                        'key': key,\n",
    "                        'value': log['params'][key],\n",
    "                        'pass': 'UNEXPECTED'\n",
    "                    }\n",
    "                    \n",
    "                    # 고유키 값들 추가\n",
    "                    for uk in self.unique_keys:\n",
    "                        result[uk] = scenario.get(uk, '')\n",
    "                    \n",
    "                    results.append(result)\n",
    "\n",
    "        # XLSX 출력 - 컬럼 순서 정렬\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # 고유키 조합에 따른 컬럼 순서 정의\n",
    "        if 'page_id' in self.unique_keys:\n",
    "            column_order = ['page_id', 'click_type', 'act_type', 'key', 'value', 'pass']\n",
    "        else:\n",
    "            column_order = ['click_text', 'click_type', 'act_type', 'key', 'value', 'pass']\n",
    "        \n",
    "        # 컬럼 순서 적용\n",
    "        df_results = df_results[column_order]\n",
    "        df_results.to_excel(output_file, index=False, engine='openpyxl')\n",
    "        \n",
    "        # 요약 출력\n",
    "        self._print_summary(results)\n",
    "        print(f\"✅ 결과 저장: {output_file}\")\n",
    "\n",
    "    def _find_matching_logs(self, scenario: Dict) -> List[Dict]:\n",
    "        \"\"\"시나리오와 매칭되는 로그 찾기\"\"\"\n",
    "        matched = []\n",
    "        \n",
    "        for log in self.logs:\n",
    "            params = log['params']\n",
    "            \n",
    "            # 동적 고유키로 매칭\n",
    "            is_match = True\n",
    "            for key in self.unique_keys:\n",
    "                if params.get(key, '') != scenario.get(key, ''):\n",
    "                    is_match = False\n",
    "                    break\n",
    "            \n",
    "            if is_match:\n",
    "                matched.append(log)\n",
    "        \n",
    "        return matched\n",
    "\n",
    "    def _print_summary(self, results: List[Dict]):\n",
    "        \"\"\"결과 요약 출력\"\"\"\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        total = len(df)\n",
    "        passed = len(df[df['pass'] == 'PASS'])\n",
    "        failed = len(df[df['pass'] == 'FAIL'])\n",
    "        unexpected = len(df[df['pass'] == 'UNEXPECTED'])\n",
    "        \n",
    "        print(f\"\\n📊 검증 결과:\")\n",
    "        print(f\"   전체: {total}개\")\n",
    "        print(f\"   통과: {passed}개\")\n",
    "        print(f\"   실패: {failed}개\")\n",
    "        print(f\"   예상외: {unexpected}개\")\n",
    "        print(f\"   성공률: {passed/total*100:.1f}%\" if total > 0 else \"   성공률: 0%\")\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "def run_example():\n",
    "    import datetime as dt\n",
    "    timestamp = dt.datetime.today()\n",
    "    formatted = timestamp.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "    \n",
    "    \"\"\"사용 예시\"\"\"\n",
    "    analyzer = LogAnalyzer()\n",
    "\n",
    "    # 로그 분석 및 파일 생성\n",
    "    analyzer.parse_log_file_from_path(\n",
    "        file_path=\"log_file.txt\",\n",
    "        output_txt=f\"./result/log_analysis_result_{formatted}.txt\",\n",
    "        output_xlsx=f\"./result/log_analysis_result_{formatted}.xlsx\"\n",
    "    )\n",
    "\n",
    "    analyzer.parse_log_file_from_path(\n",
    "        file_path=\"log_file.txt\",\n",
    "        output_txt=f\"./result/log_analysis_result.txt\",\n",
    "        output_xlsx=f\"./result/log_analysis_result.xlsx\"\n",
    "    )\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== 로그 분석 및 파일 출력 ===\")\n",
    "    run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JSON 로그를 Excel 형태로 변환 (JSON 오류 자동 수정 + 중복 제거 기능 포함) ===\n",
      "\n",
      "✅ 강제 JSON 수정 성공 (라인 94)\n",
      "✅ 강제 JSON 수정 성공 (라인 422)\n",
      "✅ 강제 JSON 수정 성공 (라인 464)\n",
      "✅ 강제 JSON 수정 성공 (라인 710)\n",
      "✅ 강제 JSON 수정 성공 (라인 731)\n",
      "✅ 강제 JSON 수정 성공 (라인 738)\n",
      "✅ 강제 JSON 수정 성공 (라인 777)\n",
      "⚠️ JSON 파싱 오류 발생: 7개\n",
      "✅ 수정 성공: 7개\n",
      "✅ 최종 파싱된 로그: 777개\n",
      "✅ 로그 파일 로드 완료: 777개 로그\n",
      "✅ 상세 데이터 변환 완료: 4495개 key-value 쌍\n",
      "✅ 결합 데이터 변환 완료: 777개 로그\n",
      "✅ 결합 형태 중복 제거 완료:\n",
      "   원본: 777개\n",
      "   중복 제거 후: 297개\n",
      "   제거된 중복: 480개\n",
      "   중복 제거 기준: page_id, click_type, act_type, click_text\n",
      "✅ 상세 형태 중복 제거 완료:\n",
      "   원본: 4495개\n",
      "   중복 제거 후: 2260개\n",
      "   제거된 중복: 2235개\n",
      "   중복 제거 기준: page_id, click_type, act_type, key, value, click_text\n",
      "✅ 상세 Excel 파일 저장: ./result/log_analysis_detailed.xlsx\n",
      "   총 4495개 key-value 쌍\n",
      "✅ 상세 중복 제거 Excel 파일 저장: ./result/log_analysis_detailed_deduplicated.xlsx\n",
      "   총 2260개 key-value 쌍 (중복 제거됨)\n",
      "✅ 결합 Excel 파일 저장: ./result/log_analysis_combined.xlsx\n",
      "   총 777개 로그\n",
      "✅ 결합 형태 중복 제거 Excel 파일 저장: ./result/log_analysis_combined_deduplicated.xlsx\n",
      "   총 297개 로그 (중복 제거됨)\n",
      "\n",
      "📊 변환 결과 요약:\n",
      "   total_logs: 777\n",
      "   total_key_value_pairs: 4495\n",
      "   average_keys_per_log: 5.79\n",
      "   max_keys_in_single_log: 16\n",
      "   min_keys_in_single_log: 2\n",
      "   unique_keys_count: 80\n",
      "   combined_logs_count: 777\n",
      "   combined_deduplicated_count: 297\n",
      "   combined_duplicates_removed: 480\n",
      "   detailed_key_value_pairs: 4495\n",
      "   detailed_deduplicated_count: 2260\n",
      "   detailed_duplicates_removed: 2235\n",
      "\n",
      "📋 상세 데이터 미리보기 (처음 10개 행):\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "No  | Page ID                   | Click Type   | Act Type     | Click Text      | Key                  | Value                    \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  1 | life-dev/main             |              | click        | 포인트 12,500원 놓치고 | channel              | Rround                   \n",
      "  1 | life-dev/main             |              | click        | 포인트 12,500원 놓치고 | page_url             | https://life-dev.hectoinn\n",
      "  1 | life-dev/main             |              | click        | 포인트 12,500원 놓치고 | reffer_url           | https://life-dev.hectoinn\n",
      "  1 | life-dev/main             |              | click        | 포인트 12,500원 놓치고 | click_text           | 포인트 12,500원 놓치고 있어요!     \n",
      "  1 | life-dev/main             |              | click        | 포인트 12,500원 놓치고 | el_order             | 0                        \n",
      "  2 | tb-hfpayfe/v1/pay/        |              | pageview     |                 | payToken             | eyJjcmVhdGVkRHQiOiIyMDI1M\n",
      "  2 | tb-hfpayfe/v1/pay/        |              | pageview     |                 | uid                  | 868                      \n",
      "  2 | tb-hfpayfe/v1/pay/        |              | pageview     |                 | channel              | Rround                   \n",
      "  2 | tb-hfpayfe/v1/pay/        |              | pageview     |                 | page_url             | https://tb-hfpayfe.hectoi\n",
      "  3 | tb-hfpayfe/v1/pay/        |              | pageview     |                 | payToken             | eyJjcmVhdGVkRHQiOiIyMDI1M\n",
      "... 외 4485개 행\n",
      "\n",
      "📋 상세 중복 제거된 데이터 미리보기 (처음 10개 행):\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "No  | Page ID                   | Click Type   | Act Type     | Click Text      | Key                  | Value                    \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  1 | life-dev/main             |              | click        | 포인트 12,500원 놓치고 | channel              | Rround                   \n",
      "  1 | life-dev/main             |              | click        | 포인트 12,500원 놓치고 | page_url             | https://life-dev.hectoinn\n",
      "  1 | life-dev/main             |              | click        | 포인트 12,500원 놓치고 | reffer_url           | https://life-dev.hectoinn\n",
      "  1 | life-dev/main             |              | click        | 포인트 12,500원 놓치고 | click_text           | 포인트 12,500원 놓치고 있어요!     \n",
      "  1 | life-dev/main             |              | click        | 포인트 12,500원 놓치고 | el_order             | 0                        \n",
      "  2 | tb-hfpayfe/v1/pay/        |              | pageview     |                 | payToken             | eyJjcmVhdGVkRHQiOiIyMDI1M\n",
      "  2 | tb-hfpayfe/v1/pay/        |              | pageview     |                 | uid                  | 868                      \n",
      "  2 | tb-hfpayfe/v1/pay/        |              | pageview     |                 | channel              | Rround                   \n",
      "  2 | tb-hfpayfe/v1/pay/        |              | pageview     |                 | page_url             | https://tb-hfpayfe.hectoi\n",
      "  3 | tb-hfpayfe/v1/pay/        |              | pageview     |                 | payToken             | eyJjcmVhdGVkRHQiOiIyMDI1M\n",
      "... 외 2250개 행\n",
      "\n",
      "📋 결합 데이터 미리보기 (처음 5개 행):\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "No  | Page ID                   | Click Type   | Act Type     | Keys                                     | Values                                   | Count\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  1 | life-dev/main             |              | click        | channel, page_url, reffer_url, click_tex | Rround, https://life-dev.hectoinnovation |     5\n",
      "  2 | tb-hfpayfe/v1/pay/        |              | pageview     | payToken, uid, channel, page_url         | eyJjcmVhdGVkRHQiOiIyMDI1MDYwMjE4MjE0NyIs |     4\n",
      "  3 | tb-hfpayfe/v1/pay/        |              | pageview     | payToken, uid, channel, page_url, reffer | eyJjcmVhdGVkRHQiOiIyMDI1MDYwMjE4MjIwNSIs |     5\n",
      "  4 | ecommerce-dev/main/home   |              | pageview     | channel, page_url                        | Rround, https://ecommerce-dev.hectoinnov |     2\n",
      "  5 | ecommerce-dev/main/home   |              | impression   | channel, page_url, reffer_url, banner_te | Rround, https://ecommerce-dev.hectoinnov |     6\n",
      "... 외 772개 행\n",
      "\n",
      "📋 중복 제거된 데이터 미리보기 (처음 5개 행):\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "No  | Page ID              | Click Type   | Act Type     | Click Text   | Keys                      | Values                    | Count\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  1 | life-dev/main        |              | click        | 포인트 12,500원  | channel, page_url, reffer | Rround, https://life-dev. |     5\n",
      "  2 | tb-hfpayfe/v1/pay/   |              | pageview     | nan          | payToken, uid, channel, p | eyJjcmVhdGVkRHQiOiIyMDI1M |     4\n",
      "  4 | ecommerce-dev/main/h |              | pageview     | nan          | channel, page_url         | Rround, https://ecommerce |     2\n",
      "  5 | ecommerce-dev/main/h |              | impression   | nan          | channel, page_url, reffer | Rround, https://ecommerce |     6\n",
      "  8 | life-dev/main        |              | impression   | nan          | channel, page_url, reffer | Rround, https://life-dev. |     3\n",
      "... 외 292개 행\n",
      "\n",
      "📊 키 분석 (총 80개 고유 키):\n",
      "--------------------------------------------------\n",
      "Key                            | 빈도수       \n",
      "--------------------------------------------------\n",
      "page_url                       |        777\n",
      "channel                        |        666\n",
      "reffer_url                     |        597\n",
      "click_text                     |        230\n",
      "prd_code                       |        164\n",
      "prd_name                       |        154\n",
      "prd_price_final                |        128\n",
      "prd_review_cnt                 |        112\n",
      "prd_review_score               |        112\n",
      "channnel                       |        111\n",
      "prd_tag                        |        109\n",
      "scroll_rate                    |        103\n",
      "banner_url                     |         99\n",
      "banner_text                    |         92\n",
      "categoryId                     |         81\n",
      "prd_price_origin               |         78\n",
      "prd_disc_rate                  |         76\n",
      "banner_position                |         72\n",
      "tab_name                       |         46\n",
      "area_name                      |         40\n",
      "... 외 60개 키\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import re\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "\n",
    "class LogToDetailedConverter:\n",
    "    \"\"\"JSON 로그를 다양한 Excel 형태로 변환하는 클래스\"\"\"\n",
    "    def __init__(self):\n",
    "        self.input_data = []\n",
    "        self.detailed_data = []\n",
    "        self.detailed_deduplicated_data = []\n",
    "        self.combined_data = []\n",
    "        self.deduplicated_data = []\n",
    "\n",
    "    def convert_log_to_all_formats(self, input_log_path: str, \n",
    "                                 detailed_output_path: str, \n",
    "                                 detailed_deduplicated_output_path: str,\n",
    "                                 combined_output_path: str,\n",
    "                                 combined_deduplicated_output_path: str):\n",
    "        \"\"\"\n",
    "        JSON 로그 파일을 상세 형태와 결합 형태 두 가지로 변환\n",
    "        \n",
    "        Args:\n",
    "            input_log_path: log_file.txt 파일 경로\n",
    "            detailed_output_path: log_analysis_detailed.xlsx 출력 파일 경로 (각 key-value가 개별 행)\n",
    "            combined_output_path: log_analysis_combined.xlsx 출력 파일 경로 (key, value가 쉼표로 구분된 열)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. 입력 로그 파일 읽기\n",
    "            self._load_log_file(input_log_path)\n",
    "            \n",
    "            # 2. 상세 데이터로 변환\n",
    "            self._convert_to_detailed()\n",
    "            \n",
    "            # 3. 결합 데이터로 변환\n",
    "            self._convert_to_combined()\n",
    "            \n",
    "            # 4. 결합 형태 중복 제거\n",
    "            self._convert_to_combined_deduplicated()\n",
    "            \n",
    "            # 5. 상세 형태 중복 제거\n",
    "            self._convert_to_detailed_deduplicated()\n",
    "            \n",
    "            # 6. 네 형태 모두 저장\n",
    "            self._save_detailed_excel(detailed_output_path)\n",
    "            self._save_detailed_deduplicated_excel(detailed_deduplicated_output_path)\n",
    "            self._save_combined_excel(combined_output_path)\n",
    "            self._save_combined_deduplicated_excel(combined_deduplicated_output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 변환 중 오류 발생: {e}\")\n",
    "\n",
    "    def convert_log_to_both_formats(self, input_log_path: str, \n",
    "                                  detailed_output_path: str, \n",
    "                                  combined_output_path: str):\n",
    "        \"\"\"\n",
    "        기존 호환성을 위한 메서드 - 중복 제거 없이 두 가지 형태만 변환\n",
    "        \n",
    "        Args:\n",
    "            input_log_path: log_file.txt 파일 경로\n",
    "            detailed_output_path: log_analysis_detailed.xlsx 출력 파일 경로 (각 key-value가 개별 행)\n",
    "            combined_output_path: log_analysis_combined.xlsx 출력 파일 경로 (key, value가 쉼표로 구분된 열)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. 입력 로그 파일 읽기\n",
    "            self._load_log_file(input_log_path)\n",
    "            \n",
    "            # 2. 상세 데이터로 변환\n",
    "            self._convert_to_detailed()\n",
    "            \n",
    "            # 3. 결합 데이터로 변환\n",
    "            self._convert_to_combined()\n",
    "            \n",
    "            # 4. 두 형태 저장\n",
    "            self._save_detailed_excel(detailed_output_path)\n",
    "            self._save_combined_excel(combined_output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 변환 중 오류 발생: {e}\")\n",
    "\n",
    "    def _load_log_file(self, log_path: str):\n",
    "        \"\"\"JSON 로그 파일 로드 및 파싱 (오류 자동 수정 기능 포함)\"\"\"\n",
    "        try:\n",
    "            self.input_data = []\n",
    "            json_errors = []\n",
    "            fixed_count = 0\n",
    "            \n",
    "            with open(log_path, 'r', encoding='utf-8') as file:\n",
    "                for line_no, line in enumerate(file, 1):\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # JSON 파싱\n",
    "                        log_entry = json.loads(line)\n",
    "                        \n",
    "                        # REQUEST URL에서 파라미터 추출\n",
    "                        request_url = log_entry.get('REQUEST', '')\n",
    "                        parsed_data = self._parse_request_url(request_url)\n",
    "                        \n",
    "                        if parsed_data:\n",
    "                            # 기본 로그 정보 추가\n",
    "                            parsed_data['log_line'] = line_no\n",
    "                            parsed_data['logtime'] = log_entry.get('LOGTIME', '')\n",
    "                            parsed_data['clientip'] = log_entry.get('CLIENTIP', '')\n",
    "                            parsed_data['useragent'] = log_entry.get('USERAGENT', '')\n",
    "                            parsed_data['status'] = log_entry.get('STATUS', '')\n",
    "                            \n",
    "                            self.input_data.append(parsed_data)\n",
    "                    \n",
    "                    except json.JSONDecodeError as e:\n",
    "                        json_errors.append((line_no, str(e), line))\n",
    "                        \n",
    "                        # JSON 수정 시도\n",
    "                        fixed_line = self._try_fix_json(line)\n",
    "                        if fixed_line:\n",
    "                            try:\n",
    "                                log_entry = json.loads(fixed_line)\n",
    "                                \n",
    "                                # REQUEST URL에서 파라미터 추출\n",
    "                                request_url = log_entry.get('REQUEST', '')\n",
    "                                parsed_data = self._parse_request_url(request_url)\n",
    "                                \n",
    "                                if parsed_data:\n",
    "                                    # 기본 로그 정보 추가\n",
    "                                    parsed_data['log_line'] = line_no\n",
    "                                    parsed_data['logtime'] = log_entry.get('LOGTIME', '')\n",
    "                                    parsed_data['clientip'] = log_entry.get('CLIENTIP', '')\n",
    "                                    parsed_data['useragent'] = log_entry.get('USERAGENT', '')\n",
    "                                    parsed_data['status'] = log_entry.get('STATUS', '')\n",
    "                                    \n",
    "                                    self.input_data.append(parsed_data)\n",
    "                                    fixed_count += 1\n",
    "                                    print(f\"✅ JSON 수정 성공 (라인 {line_no})\")\n",
    "                            except:\n",
    "                                # 추가 수정 시도\n",
    "                                fixed_line2 = self._force_fix_json(line)\n",
    "                                if fixed_line2:\n",
    "                                    try:\n",
    "                                        log_entry = json.loads(fixed_line2)\n",
    "                                        \n",
    "                                        # REQUEST URL에서 파라미터 추출\n",
    "                                        request_url = log_entry.get('REQUEST', '')\n",
    "                                        parsed_data = self._parse_request_url(request_url)\n",
    "                                        \n",
    "                                        if parsed_data:\n",
    "                                            # 기본 로그 정보 추가\n",
    "                                            parsed_data['log_line'] = line_no\n",
    "                                            parsed_data['logtime'] = log_entry.get('LOGTIME', '')\n",
    "                                            parsed_data['clientip'] = log_entry.get('CLIENTIP', '')\n",
    "                                            parsed_data['useragent'] = log_entry.get('USERAGENT', '')\n",
    "                                            parsed_data['status'] = log_entry.get('STATUS', '')\n",
    "                                            \n",
    "                                            self.input_data.append(parsed_data)\n",
    "                                            fixed_count += 1\n",
    "                                            print(f\"✅ 강제 JSON 수정 성공 (라인 {line_no})\")\n",
    "                                    except:\n",
    "                                        print(f\"❌ JSON 수정 실패 (라인 {line_no}): {e}\")\n",
    "                                else:\n",
    "                                    print(f\"❌ JSON 수정 실패 (라인 {line_no}): {e}\")\n",
    "                        else:\n",
    "                            print(f\"❌ JSON 수정 실패 (라인 {line_no}): {e}\")\n",
    "            \n",
    "            # 오류 요약 출력\n",
    "            if json_errors:\n",
    "                print(f\"⚠️ JSON 파싱 오류 발생: {len(json_errors)}개\")\n",
    "                if fixed_count > 0:\n",
    "                    print(f\"✅ 수정 성공: {fixed_count}개\")\n",
    "                print(f\"✅ 최종 파싱된 로그: {len(self.input_data)}개\")\n",
    "            else:\n",
    "                print(f\"✅ 모든 JSON 파싱 성공\")\n",
    "            \n",
    "            print(f\"✅ 로그 파일 로드 완료: {len(self.input_data)}개 로그\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ 파일을 찾을 수 없습니다: {log_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 로드 오류: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _try_fix_json(self, line: str) -> str:\n",
    "        \"\"\"JSON 수정 시도 - 1차 수정\"\"\"\n",
    "        try:\n",
    "            # 1. 마지막 쉼표 제거\n",
    "            if line.endswith(',}'):\n",
    "                line = line[:-2] + '}'\n",
    "            elif line.endswith(',]'):\n",
    "                line = line[:-2] + ']'\n",
    "            \n",
    "            # 2. 값 내부의 따옴표를 작은따옴표로 변경\n",
    "            import re\n",
    "            \n",
    "            # 모든 \"key\":\"value with quotes\" 패턴을 찾아서 수정\n",
    "            def fix_quotes_in_value(match):\n",
    "                key = match.group(1)\n",
    "                value = match.group(2)\n",
    "                # 값 내부의 모든 따옴표를 작은따옴표로 변경\n",
    "                fixed_value = value.replace('\"', \"'\")\n",
    "                return f'\"{key}\":\"{fixed_value}\"'\n",
    "            \n",
    "            # 값에 따옴표가 포함된 패턴 찾기\n",
    "            problem_pattern = r'\"([^\"]+)\":\"([^\"]*\"[^\"]*)\"'\n",
    "            \n",
    "            # 여러 번 적용하여 모든 문제 해결\n",
    "            prev_line = \"\"\n",
    "            iterations = 0\n",
    "            while line != prev_line and iterations < 5:\n",
    "                prev_line = line\n",
    "                line = re.sub(problem_pattern, fix_quotes_in_value, line)\n",
    "                iterations += 1\n",
    "            \n",
    "            return line\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _force_fix_json(self, line: str) -> str:\n",
    "        \"\"\"JSON 강제 수정 - 2차 수정\"\"\"\n",
    "        try:\n",
    "            # 기본 정리\n",
    "            if line.endswith(',}'):\n",
    "                line = line[:-2] + '}'\n",
    "            elif line.endswith(',]'):\n",
    "                line = line[:-2] + ']'\n",
    "            \n",
    "            # JSON 구조 분석하여 키-값 쌍별로 처리\n",
    "            if not (line.startswith('{') and line.endswith('}')):\n",
    "                return None\n",
    "            \n",
    "            # 중괄호 제거\n",
    "            content = line[1:-1]\n",
    "            \n",
    "            # 키-값 쌍들을 분리\n",
    "            pairs = []\n",
    "            current_pair = \"\"\n",
    "            bracket_count = 0\n",
    "            in_quotes = False\n",
    "            escape_next = False\n",
    "            \n",
    "            for char in content:\n",
    "                if escape_next:\n",
    "                    current_pair += char\n",
    "                    escape_next = False\n",
    "                    continue\n",
    "                \n",
    "                if char == '\\\\':\n",
    "                    current_pair += char\n",
    "                    escape_next = True\n",
    "                    continue\n",
    "                \n",
    "                if char == '\"' and not escape_next:\n",
    "                    in_quotes = not in_quotes\n",
    "                \n",
    "                if not in_quotes:\n",
    "                    if char in '{}[]':\n",
    "                        bracket_count += 1 if char in '{[' else -1\n",
    "                    elif char == ',' and bracket_count == 0:\n",
    "                        pairs.append(current_pair.strip())\n",
    "                        current_pair = \"\"\n",
    "                        continue\n",
    "                \n",
    "                current_pair += char\n",
    "            \n",
    "            if current_pair.strip():\n",
    "                pairs.append(current_pair.strip())\n",
    "            \n",
    "            # 각 키-값 쌍을 수정\n",
    "            fixed_pairs = []\n",
    "            for pair in pairs:\n",
    "                if ':' in pair:\n",
    "                    try:\n",
    "                        # 키와 값 분리 (첫 번째 콜론 기준)\n",
    "                        colon_pos = pair.find(':')\n",
    "                        key_part = pair[:colon_pos].strip()\n",
    "                        value_part = pair[colon_pos+1:].strip()\n",
    "                        \n",
    "                        # 값 부분에서 따옴표로 둘러싸인 문자열인지 확인\n",
    "                        if value_part.startswith('\"') and value_part.endswith('\"'):\n",
    "                            # 값 내부의 따옴표를 작은따옴표로 변경\n",
    "                            inner_value = value_part[1:-1]  # 양쪽 따옴표 제거\n",
    "                            fixed_inner = inner_value.replace('\"', \"'\")\n",
    "                            fixed_value = f'\"{fixed_inner}\"'\n",
    "                            fixed_pairs.append(f'{key_part}:{fixed_value}')\n",
    "                        else:\n",
    "                            fixed_pairs.append(pair)\n",
    "                    except:\n",
    "                        fixed_pairs.append(pair)\n",
    "                else:\n",
    "                    fixed_pairs.append(pair)\n",
    "            \n",
    "            # 다시 조합\n",
    "            result = '{' + ','.join(fixed_pairs) + '}'\n",
    "            return result\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _parse_request_url(self, request_url: str) -> Dict:\n",
    "        \"\"\"REQUEST URL에서 파라미터들을 추출 (개선된 한글 처리)\"\"\"\n",
    "        if not request_url:\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # 유니코드 이스케이프 디코딩\n",
    "            url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "            parsed_url = urlparse(url)\n",
    "            \n",
    "            # 쿼리 파라미터를 수동으로 파싱 (복잡한 값 처리)\n",
    "            query = parsed_url.query\n",
    "            processed_params = {}\n",
    "            \n",
    "            if not query:\n",
    "                return processed_params\n",
    "            \n",
    "            # &로 분할하되, URL 인코딩된 &(%26)은 분할하지 않음\n",
    "            params = []\n",
    "            current_param = \"\"\n",
    "            i = 0\n",
    "            \n",
    "            while i < len(query):\n",
    "                if query[i:i+1] == '&' and not query[max(0, i-3):i] == '%26':\n",
    "                    if current_param:\n",
    "                        params.append(current_param)\n",
    "                    current_param = \"\"\n",
    "                else:\n",
    "                    current_param += query[i]\n",
    "                i += 1\n",
    "            \n",
    "            if current_param:\n",
    "                params.append(current_param)\n",
    "            \n",
    "            # 각 파라미터를 키=값으로 분할\n",
    "            for param in params:\n",
    "                if '=' not in param:\n",
    "                    continue\n",
    "                    \n",
    "                # 첫 번째 = 기준으로만 분할 (값에 =이 포함될 수 있음)\n",
    "                key, value = param.split('=', 1)\n",
    "                \n",
    "                # 키와 값 디코딩\n",
    "                decoded_key = self._fix_korean_text(key)\n",
    "                decoded_value = self._fix_korean_text(value)\n",
    "                \n",
    "                processed_params[decoded_key] = decoded_value\n",
    "            \n",
    "            return processed_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"URL 파싱 오류: {e}\")\n",
    "            # 실패 시 기존 방식으로 fallback\n",
    "            try:\n",
    "                url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "                parsed_url = urlparse(url)\n",
    "                query_params = parse_qs(parsed_url.query, keep_blank_values=True)\n",
    "                \n",
    "                processed_params = {}\n",
    "                for key, value_list in query_params.items():\n",
    "                    raw_value = value_list[0] if value_list else ''\n",
    "                    decoded_value = self._fix_korean_text(raw_value)\n",
    "                    processed_params[key] = decoded_value\n",
    "                \n",
    "                return processed_params\n",
    "            except:\n",
    "                return {}\n",
    "\n",
    "    def _fix_korean_text(self, text: str) -> str:\n",
    "        \"\"\"깨진 한글을 올바르게 디코딩\"\"\"\n",
    "        try:\n",
    "            # URL 디코딩 먼저 시도\n",
    "            from urllib.parse import unquote\n",
    "            decoded = unquote(text, encoding='utf-8')\n",
    "            \n",
    "            # 깨진 한글 패턴이 있는지 확인\n",
    "            if self._has_broken_korean(decoded):\n",
    "                # Latin-1로 인코딩 후 UTF-8로 디코딩 (일반적인 mojibake 해결법)\n",
    "                try:\n",
    "                    fixed = decoded.encode('latin-1').decode('utf-8')\n",
    "                    return fixed\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                # 그래도 안되면 바이트 단위로 처리\n",
    "                try:\n",
    "                    # 문자열의 각 문자를 바이트로 변환 후 UTF-8 디코딩\n",
    "                    byte_data = bytes([ord(c) for c in decoded if ord(c) < 256])\n",
    "                    fixed = byte_data.decode('utf-8', errors='ignore')\n",
    "                    return fixed\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                # UTF-8 바이트 패턴으로 복구 시도\n",
    "                try:\n",
    "                    # 특정 깨진 패턴들을 직접 매핑\n",
    "                    broken_patterns = {\n",
    "                        'ê·¹ììëª¨': '극장용모',\n",
    "                        'í¥ê¸°ë¡­ê²': '향기롭게',\n",
    "                        'ì¤ê¸°ì¼ì´': '수기일이',\n",
    "                        'ììëª¨': '용모',\n",
    "                        'ê·¹': '극'\n",
    "                    }\n",
    "                    \n",
    "                    result = decoded\n",
    "                    for broken, fixed in broken_patterns.items():\n",
    "                        result = result.replace(broken, fixed)\n",
    "                    \n",
    "                    if result != decoded:\n",
    "                        return result\n",
    "                        \n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return decoded\n",
    "            \n",
    "        except Exception:\n",
    "            return text\n",
    "\n",
    "    def _has_broken_korean(self, text: str) -> bool:\n",
    "        \"\"\"깨진 한글 패턴이 있는지 확인\"\"\"\n",
    "        broken_patterns = [\n",
    "            'ë¶', 'ìë', 'ê¼¬', 'ë', 'í¼', 'ì¸', 'ê¸°', 'ê¸', 'ìì¹', 'ì', 'ê',\n",
    "            'ê·¹ì', 'í¥ê¸°', 'ì¤ê¸°', 'ììë', 'ëª¨'\n",
    "        ]\n",
    "        return any(pattern in text for pattern in broken_patterns)\n",
    "\n",
    "    def _convert_to_detailed(self):\n",
    "        \"\"\"로그 데이터를 상세 key-value 행으로 변환\"\"\"\n",
    "        self.detailed_data = []\n",
    "        \n",
    "        # 제외할 기본 필드들 (메타데이터)\n",
    "        exclude_fields = {'log_line', 'logtime', 'clientip', 'useragent', 'status'}\n",
    "        \n",
    "        for log_entry in self.input_data:\n",
    "            log_no = log_entry.get('log_line', 0)\n",
    "            \n",
    "            # 기본 정보 추출\n",
    "            page_id = log_entry.get('page_id', '')\n",
    "            click_type = log_entry.get('click_type', '')\n",
    "            act_type = log_entry.get('act_type', '')\n",
    "            click_text = log_entry.get('click_text', '')  # click_text 추가\n",
    "            \n",
    "            # 나머지 파라미터들을 key-value 쌍으로 처리\n",
    "            for key, value in log_entry.items():\n",
    "                if key in exclude_fields:\n",
    "                    continue\n",
    "                \n",
    "                # 기본 필드들은 건너뛰기 (click_text는 이제 포함됨)\n",
    "                if key in ['page_id', 'click_type', 'act_type']:\n",
    "                    continue\n",
    "                \n",
    "                self.detailed_data.append({\n",
    "                    'no': log_no,\n",
    "                    'page_id': page_id,\n",
    "                    'click_type': click_type,\n",
    "                    'act_type': act_type,\n",
    "                    'click_text': click_text,\n",
    "                    'key': key,\n",
    "                    'value': str(value) if value is not None else ''\n",
    "                })\n",
    "        \n",
    "        print(f\"✅ 상세 데이터 변환 완료: {len(self.detailed_data)}개 key-value 쌍\")\n",
    "\n",
    "    def _convert_to_combined(self):\n",
    "        \"\"\"로그 데이터를 결합 형태로 변환 (key, value가 쉼표로 구분된 열)\"\"\"\n",
    "        self.combined_data = []\n",
    "        \n",
    "        # 제외할 기본 필드들 (메타데이터)\n",
    "        exclude_fields = {'log_line', 'logtime', 'clientip', 'useragent', 'status', \n",
    "                         'page_id', 'click_type', 'act_type'}\n",
    "        \n",
    "        for log_entry in self.input_data:\n",
    "            log_no = log_entry.get('log_line', 0)\n",
    "            \n",
    "            # 기본 정보 추출\n",
    "            page_id = log_entry.get('page_id', '')\n",
    "            click_type = log_entry.get('click_type', '')\n",
    "            act_type = log_entry.get('act_type', '')\n",
    "            click_text = log_entry.get('click_text', '')  # click_text 추가\n",
    "            \n",
    "            # 나머지 파라미터들 수집 (이제 click_text도 포함됨)\n",
    "            keys = []\n",
    "            values = []\n",
    "            \n",
    "            for key, value in log_entry.items():\n",
    "                if key not in exclude_fields:\n",
    "                    keys.append(key)\n",
    "                    values.append(str(value) if value is not None else '')\n",
    "            \n",
    "            # 키와 값을 쉼표로 구분된 문자열로 결합\n",
    "            keys_combined = ', '.join(keys) if keys else ''\n",
    "            values_combined = ', '.join(values) if values else ''\n",
    "            \n",
    "            combined_entry = {\n",
    "                'no': log_no,\n",
    "                'page_id': page_id,\n",
    "                'click_type': click_type,\n",
    "                'act_type': act_type,\n",
    "                'keys_combined': keys_combined,\n",
    "                'values_combined': values_combined,\n",
    "                'key_count': len(keys)\n",
    "            }\n",
    "            \n",
    "            # click_text가 있으면 추가\n",
    "            if click_text:\n",
    "                combined_entry['click_text'] = click_text\n",
    "            \n",
    "            self.combined_data.append(combined_entry)\n",
    "        \n",
    "        print(f\"✅ 결합 데이터 변환 완료: {len(self.combined_data)}개 로그\")\n",
    "\n",
    "    def _convert_to_combined_deduplicated(self):\n",
    "        \"\"\"결합 데이터에서 중복을 제거한 형태로 변환\"\"\"\n",
    "        if not self.combined_data:\n",
    "            print(\"⚠️ 결합 데이터가 없어 중복 제거를 수행할 수 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        # pandas DataFrame으로 변환\n",
    "        df = pd.DataFrame(self.combined_data)\n",
    "        \n",
    "        # 중복 제거 전 개수\n",
    "        original_count = len(df)\n",
    "        \n",
    "        # page_id, click_type, act_type을 기준으로 중복 제거\n",
    "        # click_text가 있는 경우 추가로 포함\n",
    "        dedup_columns = ['page_id', 'click_type', 'act_type']\n",
    "        \n",
    "        # click_text 컬럼이 존재하고 비어있지 않은 행이 있으면 추가\n",
    "        if 'click_text' in df.columns and df['click_text'].notna().any() and (df['click_text'] != '').any():\n",
    "            dedup_columns.append('click_text')\n",
    "        \n",
    "        # 중복 제거\n",
    "        df_deduplicated = df.drop_duplicates(subset=dedup_columns, keep='first')\n",
    "        \n",
    "        # 중복 제거 후 개수\n",
    "        deduplicated_count = len(df_deduplicated)\n",
    "        removed_count = original_count - deduplicated_count\n",
    "        \n",
    "        # DataFrame을 다시 딕셔너리 리스트로 변환\n",
    "        self.deduplicated_data = df_deduplicated.to_dict('records')\n",
    "        \n",
    "        print(f\"✅ 결합 형태 중복 제거 완료:\")\n",
    "        print(f\"   원본: {original_count}개\")\n",
    "        print(f\"   중복 제거 후: {deduplicated_count}개\")\n",
    "        print(f\"   제거된 중복: {removed_count}개\")\n",
    "        print(f\"   중복 제거 기준: {', '.join(dedup_columns)}\")\n",
    "\n",
    "    def _convert_to_detailed_deduplicated(self):\n",
    "        \"\"\"상세 데이터에서 중복을 제거한 형태로 변환\"\"\"\n",
    "        if not self.detailed_data:\n",
    "            print(\"⚠️ 상세 데이터가 없어 중복 제거를 수행할 수 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        # pandas DataFrame으로 변환\n",
    "        df = pd.DataFrame(self.detailed_data)\n",
    "        \n",
    "        # 중복 제거 전 개수\n",
    "        original_count = len(df)\n",
    "        \n",
    "        # page_id, click_type, act_type, key, value를 기준으로 중복 제거\n",
    "        # click_text가 있는 경우 추가로 포함\n",
    "        dedup_columns = ['page_id', 'click_type', 'act_type', 'key', 'value']\n",
    "        \n",
    "        # click_text 컬럼이 존재하고 비어있지 않은 행이 있으면 추가\n",
    "        if 'click_text' in df.columns and df['click_text'].notna().any() and (df['click_text'] != '').any():\n",
    "            dedup_columns.append('click_text')\n",
    "        \n",
    "        # 중복 제거\n",
    "        df_deduplicated = df.drop_duplicates(subset=dedup_columns, keep='first')\n",
    "        \n",
    "        # 중복 제거 후 개수\n",
    "        deduplicated_count = len(df_deduplicated)\n",
    "        removed_count = original_count - deduplicated_count\n",
    "        \n",
    "        # DataFrame을 다시 딕셔너리 리스트로 변환\n",
    "        self.detailed_deduplicated_data = df_deduplicated.to_dict('records')\n",
    "        \n",
    "        print(f\"✅ 상세 형태 중복 제거 완료:\")\n",
    "        print(f\"   원본: {original_count}개\")\n",
    "        print(f\"   중복 제거 후: {deduplicated_count}개\")\n",
    "        print(f\"   제거된 중복: {removed_count}개\")\n",
    "        print(f\"   중복 제거 기준: {', '.join(dedup_columns)}\")\n",
    "\n",
    "    def _extract_click_text_from_keys(self, row):\n",
    "        \"\"\"keys_combined에서 click_text 값을 추출하려고 시도\"\"\"\n",
    "        try:\n",
    "            keys = row.get('keys_combined', '').split(', ')\n",
    "            values = row.get('values_combined', '').split(', ')\n",
    "            \n",
    "            if len(keys) == len(values):\n",
    "                for i, key in enumerate(keys):\n",
    "                    if key.strip() == 'click_text':\n",
    "                        return values[i].strip() if i < len(values) else ''\n",
    "            return ''\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    def _save_detailed_excel(self, output_path: str):\n",
    "        \"\"\"상세 데이터를 Excel 파일로 저장\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(self.detailed_data)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"⚠️ 저장할 상세 데이터가 없습니다.\")\n",
    "                return\n",
    "            \n",
    "            # 컬럼 순서 정의\n",
    "            column_order = ['no', 'page_id', 'click_type', 'act_type', 'click_text', 'key', 'value']\n",
    "            \n",
    "            # 존재하는 컬럼만 선택\n",
    "            available_columns = [col for col in column_order if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "            \n",
    "            # Excel 파일 저장\n",
    "            df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"✅ 상세 Excel 파일 저장: {output_path}\")\n",
    "            print(f\"   총 {len(self.detailed_data)}개 key-value 쌍\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 저장 오류: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _save_detailed_deduplicated_excel(self, output_path: str):\n",
    "        \"\"\"상세 중복 제거된 데이터를 Excel 파일로 저장\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(self.detailed_deduplicated_data)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"⚠️ 저장할 상세 중복 제거 데이터가 없습니다.\")\n",
    "                return\n",
    "            \n",
    "            # 컬럼 순서 정의\n",
    "            column_order = ['no', 'page_id', 'click_type', 'act_type', 'click_text', 'key', 'value']\n",
    "            \n",
    "            # 존재하는 컬럼만 선택\n",
    "            available_columns = [col for col in column_order if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "            \n",
    "            # Excel 파일 저장\n",
    "            df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"✅ 상세 중복 제거 Excel 파일 저장: {output_path}\")\n",
    "            print(f\"   총 {len(self.detailed_deduplicated_data)}개 key-value 쌍 (중복 제거됨)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 저장 오류: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _save_combined_deduplicated_excel(self, output_path: str):\n",
    "        \"\"\"결합 형태 중복 제거된 데이터를 Excel 파일로 저장\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(self.deduplicated_data)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"⚠️ 저장할 중복 제거 데이터가 없습니다.\")\n",
    "                return\n",
    "            \n",
    "            # 컬럼 순서 정의\n",
    "            column_order = ['no', 'page_id', 'click_type', 'act_type', 'keys_combined', 'values_combined', 'key_count']\n",
    "            \n",
    "            # click_text 컬럼이 있으면 추가\n",
    "            if 'click_text' in df.columns:\n",
    "                column_order.insert(4, 'click_text')\n",
    "            \n",
    "            # 존재하는 컬럼만 선택\n",
    "            available_columns = [col for col in column_order if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "            \n",
    "            # Excel 파일 저장\n",
    "            df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"✅ 결합 형태 중복 제거 Excel 파일 저장: {output_path}\")\n",
    "            print(f\"   총 {len(self.deduplicated_data)}개 로그 (중복 제거됨)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 저장 오류: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _save_combined_excel(self, output_path: str):\n",
    "        \"\"\"결합 데이터를 Excel 파일로 저장\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame(self.combined_data)\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"⚠️ 저장할 결합 데이터가 없습니다.\")\n",
    "                return\n",
    "            \n",
    "            # 컬럼 순서 정의\n",
    "            column_order = ['no', 'page_id', 'click_type', 'act_type', 'keys_combined', 'values_combined', 'key_count']\n",
    "            \n",
    "            # click_text 컬럼이 있으면 추가\n",
    "            if 'click_text' in df.columns:\n",
    "                column_order.insert(4, 'click_text')\n",
    "            \n",
    "            # 존재하는 컬럼만 선택\n",
    "            available_columns = [col for col in column_order if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "            \n",
    "            # Excel 파일 저장\n",
    "            df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"✅ 결합 Excel 파일 저장: {output_path}\")\n",
    "            print(f\"   총 {len(self.combined_data)}개 로그\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 저장 오류: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_conversion_summary(self) -> Dict:\n",
    "        \"\"\"변환 결과 요약 정보 반환\"\"\"\n",
    "        if not self.detailed_data:\n",
    "            return {\"error\": \"변환된 데이터가 없습니다.\"}\n",
    "        \n",
    "        # 통계 계산\n",
    "        total_logs = len(self.input_data)\n",
    "        total_key_values = len(self.detailed_data)\n",
    "        avg_keys_per_log = total_key_values / total_logs if total_logs > 0 else 0\n",
    "        \n",
    "        # 가장 많은 키를 가진 로그 찾기\n",
    "        key_counts = {}\n",
    "        for item in self.detailed_data:\n",
    "            log_no = item['no']\n",
    "            key_counts[log_no] = key_counts.get(log_no, 0) + 1\n",
    "        \n",
    "        max_keys = max(key_counts.values()) if key_counts else 0\n",
    "        min_keys = min(key_counts.values()) if key_counts else 0\n",
    "        \n",
    "        # 고유한 키 수 계산\n",
    "        unique_keys = set(item['key'] for item in self.detailed_data)\n",
    "        \n",
    "        # 중복 제거 통계\n",
    "        combined_count = len(self.combined_data) if self.combined_data else 0\n",
    "        combined_deduplicated_count = len(self.deduplicated_data) if self.deduplicated_data else 0\n",
    "        combined_duplicates_removed = combined_count - combined_deduplicated_count\n",
    "        \n",
    "        detailed_count = len(self.detailed_data) if self.detailed_data else 0\n",
    "        detailed_deduplicated_count = len(self.detailed_deduplicated_data) if self.detailed_deduplicated_data else 0\n",
    "        detailed_duplicates_removed = detailed_count - detailed_deduplicated_count\n",
    "        \n",
    "        return {\n",
    "            \"total_logs\": total_logs,\n",
    "            \"total_key_value_pairs\": total_key_values,\n",
    "            \"average_keys_per_log\": round(avg_keys_per_log, 2),\n",
    "            \"max_keys_in_single_log\": max_keys,\n",
    "            \"min_keys_in_single_log\": min_keys,\n",
    "            \"unique_keys_count\": len(unique_keys),\n",
    "            \"unique_keys\": sorted(list(unique_keys)),\n",
    "            \"combined_logs_count\": combined_count,\n",
    "            \"combined_deduplicated_count\": combined_deduplicated_count,\n",
    "            \"combined_duplicates_removed\": combined_duplicates_removed,\n",
    "            \"detailed_key_value_pairs\": detailed_count,\n",
    "            \"detailed_deduplicated_count\": detailed_deduplicated_count,\n",
    "            \"detailed_duplicates_removed\": detailed_duplicates_removed\n",
    "        }\n",
    "\n",
    "    def preview_data(self, num_rows: int = 10):\n",
    "        \"\"\"변환된 상세 데이터 미리보기\"\"\"\n",
    "        print(f\"\\n📋 상세 데이터 미리보기 (처음 {num_rows}개 행):\")\n",
    "        print(\"-\" * 140)\n",
    "        print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Click Text':15} | {'Key':20} | {'Value':25}\")\n",
    "        print(\"-\" * 140)\n",
    "        \n",
    "        if self.detailed_data:\n",
    "            for item in self.detailed_data[:num_rows]:\n",
    "                page_id = item['page_id'][:25] if len(item['page_id']) > 25 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                click_text = str(item.get('click_text', ''))[:15] if len(str(item.get('click_text', ''))) > 15 else str(item.get('click_text', ''))\n",
    "                key = item['key'][:20] if len(item['key']) > 20 else item['key']\n",
    "                value = item['value'][:25] if len(item['value']) > 25 else item['value']\n",
    "                \n",
    "                print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {click_text:15} | {key:20} | {value:25}\")\n",
    "            \n",
    "            if len(self.detailed_data) > num_rows:\n",
    "                print(f\"... 외 {len(self.detailed_data) - num_rows}개 행\")\n",
    "        else:\n",
    "            print(\"변환된 데이터가 없습니다.\")\n",
    "\n",
    "    def preview_combined_deduplicated_data(self, num_rows: int = 5):\n",
    "        \"\"\"결합 형태 중복 제거된 데이터 미리보기\"\"\"\n",
    "        print(f\"\\n📋 결합 형태 중복 제거된 데이터 미리보기 (처음 {num_rows}개 행):\")\n",
    "        print(\"-\" * 160)\n",
    "        \n",
    "        # 헤더 동적 생성\n",
    "        has_click_text = self.deduplicated_data and any('click_text' in item for item in self.deduplicated_data[:3])\n",
    "        if has_click_text:\n",
    "            print(f\"{'No':3} | {'Page ID':20} | {'Click Type':12} | {'Act Type':12} | {'Click Text':12} | {'Keys':25} | {'Values':25} | {'Count':5}\")\n",
    "        else:\n",
    "            print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Keys':30} | {'Values':30} | {'Count':5}\")\n",
    "        \n",
    "        print(\"-\" * 160)\n",
    "        \n",
    "        if self.deduplicated_data:\n",
    "            for item in self.deduplicated_data[:num_rows]:\n",
    "                page_id = item['page_id'][:20] if len(item['page_id']) > 20 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                \n",
    "                if 'click_text' in item and item['click_text']:\n",
    "                    click_text = item['click_text'][:12] if len(str(item['click_text'])) > 12 else str(item['click_text'])\n",
    "                    keys = item['keys_combined'][:25] if len(item['keys_combined']) > 25 else item['keys_combined']\n",
    "                    values = item['values_combined'][:25] if len(item['values_combined']) > 25 else item['values_combined']\n",
    "                    print(f\"{item['no']:3} | {page_id:20} | {click_type:12} | {act_type:12} | {click_text:12} | {keys:25} | {values:25} | {item['key_count']:5}\")\n",
    "                else:\n",
    "                    keys = item['keys_combined'][:25] if len(item['keys_combined']) > 25 else item['keys_combined']\n",
    "                    values = item['values_combined'][:25] if len(item['values_combined']) > 25 else item['values_combined']\n",
    "                    print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {keys:25} | {values:25} | {item['key_count']:5}\")\n",
    "            \n",
    "            if len(self.deduplicated_data) > num_rows:\n",
    "                print(f\"... 외 {len(self.deduplicated_data) - num_rows}개 행\")\n",
    "        else:\n",
    "            print(\"변환된 데이터가 없습니다.\")\n",
    "\n",
    "    def preview_detailed_deduplicated_data(self, num_rows: int = 10):\n",
    "        \"\"\"상세 중복 제거된 데이터 미리보기\"\"\"\n",
    "        print(f\"\\n📋 상세 중복 제거된 데이터 미리보기 (처음 {num_rows}개 행):\")\n",
    "        print(\"-\" * 140)\n",
    "        print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Click Text':15} | {'Key':20} | {'Value':25}\")\n",
    "        print(\"-\" * 140)\n",
    "        \n",
    "        if self.detailed_deduplicated_data:\n",
    "            for item in self.detailed_deduplicated_data[:num_rows]:\n",
    "                page_id = item['page_id'][:25] if len(item['page_id']) > 25 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                click_text = str(item.get('click_text', ''))[:15] if len(str(item.get('click_text', ''))) > 15 else str(item.get('click_text', ''))\n",
    "                key = item['key'][:20] if len(item['key']) > 20 else item['key']\n",
    "                value = item['value'][:25] if len(item['value']) > 25 else item['value']\n",
    "                \n",
    "                print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {click_text:15} | {key:20} | {value:25}\")\n",
    "            \n",
    "            if len(self.detailed_deduplicated_data) > num_rows:\n",
    "                print(f\"... 외 {len(self.detailed_deduplicated_data) - num_rows}개 행\")\n",
    "        else:\n",
    "            print(\"변환된 데이터가 없습니다.\")\n",
    "\n",
    "    def preview_combined_deduplicated_data(self, num_rows: int = 5):\n",
    "        \"\"\"결합 형태 중복 제거된 데이터 미리보기\"\"\"\n",
    "        print(f\"\\n📋 중복 제거된 데이터 미리보기 (처음 {num_rows}개 행):\")\n",
    "        print(\"-\" * 160)\n",
    "        \n",
    "        # 헤더 동적 생성\n",
    "        has_click_text = self.deduplicated_data and any('click_text' in item for item in self.deduplicated_data[:3])\n",
    "        if has_click_text:\n",
    "            print(f\"{'No':3} | {'Page ID':20} | {'Click Type':12} | {'Act Type':12} | {'Click Text':12} | {'Keys':25} | {'Values':25} | {'Count':5}\")\n",
    "        else:\n",
    "            print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Keys':30} | {'Values':30} | {'Count':5}\")\n",
    "        \n",
    "        print(\"-\" * 160)\n",
    "        \n",
    "        if self.deduplicated_data:\n",
    "            for item in self.deduplicated_data[:num_rows]:\n",
    "                page_id = item['page_id'][:20] if len(item['page_id']) > 20 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                \n",
    "                if 'click_text' in item and item['click_text']:\n",
    "                    click_text = item['click_text'][:12] if len(str(item['click_text'])) > 12 else str(item['click_text'])\n",
    "                    keys = item['keys_combined'][:25] if len(item['keys_combined']) > 25 else item['keys_combined']\n",
    "                    values = item['values_combined'][:25] if len(item['values_combined']) > 25 else item['values_combined']\n",
    "                    print(f\"{item['no']:3} | {page_id:20} | {click_type:12} | {act_type:12} | {click_text:12} | {keys:25} | {values:25} | {item['key_count']:5}\")\n",
    "                else:\n",
    "                    keys = item['keys_combined'][:25] if len(item['keys_combined']) > 25 else item['keys_combined']\n",
    "                    values = item['values_combined'][:25] if len(item['values_combined']) > 25 else item['values_combined']\n",
    "                    print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {keys:25} | {values:25} | {item['key_count']:5}\")\n",
    "            \n",
    "            if len(self.deduplicated_data) > num_rows:\n",
    "                print(f\"... 외 {len(self.deduplicated_data) - num_rows}개 행\")\n",
    "        else:\n",
    "            print(\"변환된 데이터가 없습니다.\")\n",
    "\n",
    "    def preview_combined_data(self, num_rows: int = 5):\n",
    "        \"\"\"결합 데이터 미리보기\"\"\"\n",
    "        print(f\"\\n📋 결합 데이터 미리보기 (처음 {num_rows}개 행):\")\n",
    "        print(\"-\" * 150)\n",
    "        print(f\"{'No':3} | {'Page ID':25} | {'Click Type':12} | {'Act Type':12} | {'Keys':40} | {'Values':40} | {'Count':5}\")\n",
    "        print(\"-\" * 150)\n",
    "        \n",
    "        if self.combined_data:\n",
    "            for item in self.combined_data[:num_rows]:\n",
    "                page_id = item['page_id'][:25] if len(item['page_id']) > 25 else item['page_id']\n",
    "                click_type = item['click_type'][:12] if len(item['click_type']) > 12 else item['click_type']\n",
    "                act_type = item['act_type'][:12] if len(item['act_type']) > 12 else item['act_type']\n",
    "                keys = item['keys_combined'][:40] if len(item['keys_combined']) > 40 else item['keys_combined']\n",
    "                values = item['values_combined'][:40] if len(item['values_combined']) > 40 else item['values_combined']\n",
    "                \n",
    "                print(f\"{item['no']:3} | {page_id:25} | {click_type:12} | {act_type:12} | {keys:40} | {values:40} | {item['key_count']:5}\")\n",
    "            \n",
    "            if len(self.combined_data) > num_rows:\n",
    "                print(f\"... 외 {len(self.combined_data) - num_rows}개 행\")\n",
    "        else:\n",
    "            print(\"변환된 데이터가 없습니다.\")\n",
    "\n",
    "    def get_unique_keys_analysis(self):\n",
    "        \"\"\"고유 키 분석\"\"\"\n",
    "        if not self.detailed_data:\n",
    "            print(\"분석할 데이터가 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        # 키별 빈도수 계산\n",
    "        key_frequency = {}\n",
    "        for item in self.detailed_data:\n",
    "            key = item['key']\n",
    "            key_frequency[key] = key_frequency.get(key, 0) + 1\n",
    "        \n",
    "        # 빈도수 순으로 정렬\n",
    "        sorted_keys = sorted(key_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\n📊 키 분석 (총 {len(sorted_keys)}개 고유 키):\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Key':30} | {'빈도수':10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for key, freq in sorted_keys[:20]:  # 상위 20개만 표시\n",
    "            key_display = key[:30] if len(key) > 30 else key\n",
    "            print(f\"{key_display:30} | {freq:10}\")\n",
    "        \n",
    "        if len(sorted_keys) > 20:\n",
    "            print(f\"... 외 {len(sorted_keys) - 20}개 키\")\n",
    "\n",
    "\n",
    "def run_log_converter_example():\n",
    "    \"\"\"로그 변환기 사용 예시\"\"\"\n",
    "    converter = LogToDetailedConverter()\n",
    "\n",
    "    print(\"=== JSON 로그를 Excel 형태로 변환 (JSON 오류 자동 수정 + 중복 제거 기능 포함) ===\\n\")\n",
    "    \n",
    "    # 로그 파일을 네 가지 형태로 변환\n",
    "    converter.convert_log_to_all_formats(\n",
    "        \"log_file_.txt\",                              # 입력 JSON 로그 파일\n",
    "        \"./result/log_analysis_detailed.xlsx\",                # 상세 형태 출력\n",
    "        \"./result/log_analysis_detailed_deduplicated.xlsx\",   # 상세 형태 중복 제거 출력\n",
    "        \"./result/log_analysis_combined.xlsx\",                # 결합 형태 출력\n",
    "        \"./result/log_analysis_combined_deduplicated.xlsx\"    # 결합 형태 중복 제거 출력\n",
    "    )\n",
    "    \n",
    "    # 결과 요약 출력\n",
    "    summary = converter.get_conversion_summary()\n",
    "    print(f\"\\n📊 변환 결과 요약:\")\n",
    "    for key, value in summary.items():\n",
    "        if key != 'unique_keys':  # unique_keys는 너무 길어서 별도 출력\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # 데이터 미리보기\n",
    "    converter.preview_data(10)\n",
    "    converter.preview_detailed_deduplicated_data(10)\n",
    "    converter.preview_combined_data(5)\n",
    "    converter.preview_combined_deduplicated_data(5)\n",
    "    \n",
    "    # 키 분석\n",
    "    converter.get_unique_keys_analysis()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_log_converter_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
