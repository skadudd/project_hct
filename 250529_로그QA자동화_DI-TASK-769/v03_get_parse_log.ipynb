{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 로그 분석 및 파일 출력 ===\n",
      "JSON 파싱 오류 (라인 94): Expecting ',' delimiter: line 1 column 618 (char 617)\n",
      "=== 총 385개의 로그 발견 ===\n",
      "\n",
      "✅ txt 파일 저장: ./result/log_analysis_result.txt\n",
      "✅ Excel 파일 저장: ./result/log_analysis_result.xlsx\n",
      "   총 385개 로그 처리됨\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from urllib.parse import parse_qs, urlparse, unquote\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "class LogAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "        self.parsed_logs = []\n",
    "\n",
    "    def parse_log_file_from_path(self, file_path: str, output_txt: str = None, output_xlsx: str = None):\n",
    "        \"\"\"\n",
    "        파일 경로에서 로그 파일을 읽어서 분석하고 결과 출력\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            # 로그 파싱\n",
    "            self._parse_log_content(content)\n",
    "            \n",
    "            # txt 파일 출력\n",
    "            if output_txt:\n",
    "                self._export_to_txt(output_txt)\n",
    "            \n",
    "            # Excel 파일 출력\n",
    "            if output_xlsx:\n",
    "                self._export_to_excel(output_xlsx)\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ 파일을 찾을 수 없습니다: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 읽기 오류: {e}\")\n",
    "\n",
    "    def _parse_log_content(self, content: str):\n",
    "        \"\"\"로그 내용 파싱\"\"\"\n",
    "        self.logs = []\n",
    "        self.parsed_logs = []\n",
    "        lines = content.strip().split('\\n')\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    log_data = json.loads(line.strip())\n",
    "                    self.logs.append((i+1, log_data))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"JSON 파싱 오류 (라인 {i+1}): {e}\")\n",
    "        \n",
    "        print(f\"=== 총 {len(self.logs)}개의 로그 발견 ===\\n\")\n",
    "        \n",
    "        # 각 로그의 REQUEST 파라미터 분석\n",
    "        for log_num, log_data in self.logs:\n",
    "            if 'REQUEST' in log_data:\n",
    "                params = self._parse_request_url(log_data['REQUEST'])\n",
    "                if params:\n",
    "                    parsed_log = {\n",
    "                        'log_number': log_num,\n",
    "                        'timestamp': log_data.get('LOGTIME', 'N/A'),\n",
    "                        'params': params\n",
    "                    }\n",
    "                    self.parsed_logs.append(parsed_log)\n",
    "\n",
    "    def _parse_request_url(self, request_url: str) -> Dict[str, str]:\n",
    "        \"\"\"REQUEST URL에서 파라미터를 파싱하고 한글을 올바르게 처리\"\"\"\n",
    "        try:\n",
    "            # 유니코드 이스케이프 디코딩\n",
    "            url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "            parsed_url = urlparse(url)\n",
    "            query_params = parse_qs(parsed_url.query, keep_blank_values=True)\n",
    "            \n",
    "            processed_params = {}\n",
    "            for key, value_list in query_params.items():\n",
    "                raw_value = value_list[0] if value_list else ''\n",
    "                decoded_value = self._fix_korean_text(raw_value)\n",
    "                processed_params[key] = decoded_value\n",
    "            \n",
    "            return processed_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"파싱 오류: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _fix_korean_text(self, text: str) -> str:\n",
    "        \"\"\"깨진 한글을 올바르게 디코딩\"\"\"\n",
    "        try:\n",
    "            decoded = unquote(text, encoding='utf-8')\n",
    "            \n",
    "            if self._has_broken_korean(decoded):\n",
    "                try:\n",
    "                    fixed = decoded.encode('latin-1').decode('utf-8')\n",
    "                    return fixed\n",
    "                except:\n",
    "                    try:\n",
    "                        byte_data = bytes([ord(c) for c in decoded if ord(c) < 256])\n",
    "                        fixed = byte_data.decode('utf-8', errors='ignore')\n",
    "                        return fixed\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            return decoded\n",
    "            \n",
    "        except Exception:\n",
    "            return text\n",
    "\n",
    "    def _has_broken_korean(self, text: str) -> bool:\n",
    "        \"\"\"깨진 한글 패턴이 있는지 확인\"\"\"\n",
    "        broken_patterns = [\n",
    "            'ë¶', 'ìë', 'ê¼¬', 'ë', 'í¼', 'ì¸', 'ê¸°', 'ê¸', 'ìì¹', 'ì', 'ê'\n",
    "        ]\n",
    "        return any(pattern in text for pattern in broken_patterns)\n",
    "\n",
    "    def _export_to_txt(self, output_file: str):\n",
    "        \"\"\"파싱 결과를 txt 파일로 출력\"\"\"\n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"=== 총 {len(self.logs)}개의 로그 발견 ===\\n\\n\")\n",
    "                \n",
    "                for parsed_log in self.parsed_logs:\n",
    "                    f.write(\"=\" * 60 + \"\\n\")\n",
    "                    f.write(f\"로그 #{parsed_log['log_number']} - {parsed_log['timestamp']}\\n\")\n",
    "                    f.write(\"=\" * 60 + \"\\n\")\n",
    "                    \n",
    "                    params = parsed_log['params']\n",
    "                    if params:\n",
    "                        # URL 정보 (첫 번째 로그에서 추출)\n",
    "                        for log_num, log_data in self.logs:\n",
    "                            if log_num == parsed_log['log_number']:\n",
    "                                if 'REQUEST' in log_data:\n",
    "                                    url = log_data['REQUEST'].encode('utf-8').decode('unicode_escape')\n",
    "                                    parsed_url = urlparse(url)\n",
    "                                    f.write(f\"URL: {parsed_url.netloc}{parsed_url.path}\\n\")\n",
    "                                    f.write(f\"총 파라미터 수: {len(params)}\\n\")\n",
    "                                    f.write(\"-\" * 50 + \"\\n\")\n",
    "                                break\n",
    "                        \n",
    "                        # 파라미터 출력\n",
    "                        for key, value in params.items():\n",
    "                            f.write(f\"{key:20} : {value}\\n\")\n",
    "                    else:\n",
    "                        f.write(\"REQUEST 필드가 없습니다.\\n\")\n",
    "                    \n",
    "                    f.write(\"\\n\")\n",
    "            \n",
    "            print(f\"✅ txt 파일 저장: {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ txt 파일 저장 오류: {e}\")\n",
    "\n",
    "    def _export_to_excel(self, output_file: str):\n",
    "        \"\"\"파싱 결과를 Excel 파일로 출력 (page_id, click_type, act_type, click_text, key_list, value_list, len(key_list))\"\"\"\n",
    "        try:\n",
    "            excel_data = []\n",
    "            \n",
    "            for parsed_log in self.parsed_logs:\n",
    "                params = parsed_log['params']\n",
    "                \n",
    "                # 각 로그에서 필요한 정보 추출\n",
    "                page_id = params.get('page_id', '')\n",
    "                click_type = params.get('click_type', '')\n",
    "                act_type = params.get('act_type', '')\n",
    "                click_text = params.get('click_text', '')\n",
    "                \n",
    "                # key_list와 value_list 생성\n",
    "                key_list = list(params.keys())\n",
    "                value_list = list(params.values())\n",
    "                \n",
    "                key_list_str = ', '.join(key_list)\n",
    "                value_list_str = ', '.join(str(v) for v in value_list)\n",
    "                key_count = len(key_list)\n",
    "                \n",
    "                excel_data.append({\n",
    "                    'page_id': page_id,\n",
    "                    'click_type': click_type,\n",
    "                    'act_type': act_type,\n",
    "                    'click_text': click_text,\n",
    "                    'key_list': key_list_str,\n",
    "                    'value_list': value_list_str,\n",
    "                    'len(key_list)': key_count\n",
    "                })\n",
    "            \n",
    "            # DataFrame 생성 및 Excel 저장\n",
    "            df = pd.DataFrame(excel_data)\n",
    "            df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"✅ Excel 파일 저장: {output_file}\")\n",
    "            print(f\"   총 {len(excel_data)}개 로그 처리됨\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Excel 파일 저장 오류: {e}\")\n",
    "\n",
    "    def analyze_logs(self, file_path: str):\n",
    "        \"\"\"로그 분석 (기존 기능 유지)\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            self._parse_log_content(content)\n",
    "            \n",
    "            # 콘솔 출력\n",
    "            for parsed_log in self.parsed_logs:\n",
    "                print(\"=\" * 60)\n",
    "                print(f\"로그 #{parsed_log['log_number']} - {parsed_log['timestamp']}\")\n",
    "                print(\"=\" * 60)\n",
    "                \n",
    "                params = parsed_log['params']\n",
    "                if params:\n",
    "                    # URL 정보 출력\n",
    "                    for log_num, log_data in self.logs:\n",
    "                        if log_num == parsed_log['log_number']:\n",
    "                            if 'REQUEST' in log_data:\n",
    "                                url = log_data['REQUEST'].encode('utf-8').decode('unicode_escape')\n",
    "                                parsed_url = urlparse(url)\n",
    "                                print(f\"URL: {parsed_url.netloc}{parsed_url.path}\")\n",
    "                                print(f\"총 파라미터 수: {len(params)}\")\n",
    "                                print(\"-\" * 50)\n",
    "                            break\n",
    "                    \n",
    "                    # 파라미터 출력\n",
    "                    for key, value in params.items():\n",
    "                        print(f\"{key:20} : {value}\")\n",
    "                else:\n",
    "                    print(\"REQUEST 필드가 없습니다.\")\n",
    "                \n",
    "                print()\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ 파일을 찾을 수 없습니다: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 읽기 오류: {e}\")\n",
    "\n",
    "# 사용 예시\n",
    "def run_example():\n",
    "    \"\"\"사용 예시\"\"\"\n",
    "    analyzer = LogAnalyzer()\n",
    "    \n",
    "    # 1. 로그 분석 및 txt, Excel 파일 생성\n",
    "    analyzer.parse_log_file_from_path(\n",
    "        file_path=\"log_file.txt\",\n",
    "        output_txt=\"./result/log_analysis_result.txt\",\n",
    "        output_xlsx=\"./result/log_analysis_result.xlsx\"\n",
    "    )\n",
    "    \n",
    "    # 2. 콘솔에 로그 분석 결과 출력만 하고 싶은 경우\n",
    "    # analyzer.analyze_logs(\"log_file.txt\")\n",
    "\n",
    "# 기존 QA 검증 클래스 (기존 기능 유지)\n",
    "class LogQAValidator:\n",
    "    def __init__(self):\n",
    "        self.scenarios = []\n",
    "        self.logs = []\n",
    "        self.unique_keys = []\n",
    "        \n",
    "    def load_scenario_csv(self, csv_file_path: str):\n",
    "        \"\"\"CSV 파일에서 시나리오 로드\"\"\"\n",
    "        self.scenarios = []\n",
    "        df = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "        \n",
    "        # 컬럼 구성에 따라 고유키 조합 결정\n",
    "        columns = df.columns.tolist()\n",
    "        if 'page_id' in columns:\n",
    "            self.unique_keys = ['page_id', 'click_type', 'act_type']\n",
    "            print(\"🔑 고유키 조합: page_id + click_type + act_type\")\n",
    "        elif 'click_text' in columns:\n",
    "            self.unique_keys = ['click_text', 'click_type', 'act_type']\n",
    "            print(\"🔑 고유키 조합: click_text + click_type + act_type\")\n",
    "        else:\n",
    "            raise ValueError(\"지원하지 않는 시나리오 형식입니다. page_id 또는 click_text 컬럼이 필요합니다.\")\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            key_list = [k.strip() for k in str(row['key_list']).split(',') if k.strip()]\n",
    "            scenario = {\n",
    "                'expected_keys': key_list\n",
    "            }\n",
    "            \n",
    "            # 고유키 값들 추가\n",
    "            for key in self.unique_keys:\n",
    "                scenario[key] = str(row[key]) if pd.notna(row[key]) else ''\n",
    "            \n",
    "            self.scenarios.append(scenario)\n",
    "        \n",
    "        print(f\"✅ 시나리오 로드: {len(self.scenarios)}개\")\n",
    "\n",
    "    def parse_logs_from_file(self, log_file_path: str):\n",
    "        \"\"\"로그 txt 파일에서 직접 파싱\"\"\"\n",
    "        try:\n",
    "            with open(log_file_path, 'r', encoding='utf-8') as file:\n",
    "                log_content = file.read()\n",
    "            self.parse_logs(log_content)\n",
    "            print(f\"✅ 로그 파일 로드: {log_file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ 파일을 찾을 수 없습니다: {log_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파일 읽기 오류: {e}\")\n",
    "\n",
    "    def parse_logs(self, log_content: str):\n",
    "        \"\"\"로그 파싱\"\"\"\n",
    "        self.logs = []\n",
    "        for i, line in enumerate(log_content.strip().split('\\n')):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                log_data = json.loads(line.strip())\n",
    "                if 'REQUEST' not in log_data:\n",
    "                    continue\n",
    "                    \n",
    "                params = self._parse_url(log_data['REQUEST'])\n",
    "                if params:\n",
    "                    self.logs.append({\n",
    "                        'line': i + 1,\n",
    "                        'timestamp': log_data.get('LOGTIME', ''),\n",
    "                        'params': params\n",
    "                    })\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "                \n",
    "        print(f\"✅ 로그 파싱: {len(self.logs)}개\")\n",
    "\n",
    "    def _parse_url(self, request_url: str) -> Dict[str, str]:\n",
    "        \"\"\"URL 파라미터 파싱\"\"\"\n",
    "        try:\n",
    "            url = request_url.encode('utf-8').decode('unicode_escape')\n",
    "            parsed = urlparse(url)\n",
    "            query_params = parse_qs(parsed.query, keep_blank_values=True)\n",
    "            \n",
    "            result = {}\n",
    "            for key, values in query_params.items():\n",
    "                if values:\n",
    "                    decoded = unquote(values[0], encoding='utf-8')\n",
    "                    # 한글 깨짐 복구 시도\n",
    "                    try:\n",
    "                        if any(ord(c) > 127 for c in decoded):\n",
    "                            decoded = decoded.encode('latin-1').decode('utf-8')\n",
    "                    except:\n",
    "                        pass\n",
    "                    result[key] = decoded\n",
    "            return result\n",
    "        except:\n",
    "            return {}\n",
    "\n",
    "    def validate_and_export(self, output_file: str):\n",
    "        \"\"\"검증 수행 및 결과 XLSX 출력\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for scenario in self.scenarios:\n",
    "            # 시나리오와 매칭되는 로그 찾기\n",
    "            matched_logs = self._find_matching_logs(scenario)\n",
    "            \n",
    "            if not matched_logs:\n",
    "                # 매칭되는 로그가 없는 경우\n",
    "                for key in scenario['expected_keys']:\n",
    "                    result = {\n",
    "                        'key': key,\n",
    "                        'value': 'NOT_FOUND',\n",
    "                        'pass': 'FAIL'\n",
    "                    }\n",
    "                    \n",
    "                    # 고유키 값들 추가\n",
    "                    for uk in self.unique_keys:\n",
    "                        result[uk] = scenario.get(uk, '')\n",
    "                    \n",
    "                    results.append(result)\n",
    "                continue\n",
    "            \n",
    "            # 각 매칭된 로그에 대해 검증\n",
    "            for log in matched_logs:\n",
    "                actual_keys = set(log['params'].keys())\n",
    "                expected_keys = set(scenario['expected_keys'])\n",
    "                \n",
    "                # 예상 키들 검증\n",
    "                for key in expected_keys:\n",
    "                    value = log['params'].get(key, 'MISSING')\n",
    "                    pass_status = 'PASS' if key in actual_keys else 'FAIL'\n",
    "                    \n",
    "                    result = {\n",
    "                        'key': key,\n",
    "                        'value': value,\n",
    "                        'pass': pass_status\n",
    "                    }\n",
    "                    \n",
    "                    # 고유키 값들 추가\n",
    "                    for uk in self.unique_keys:\n",
    "                        result[uk] = scenario.get(uk, '')\n",
    "                    \n",
    "                    results.append(result)\n",
    "                \n",
    "                # 예상치 못한 키들 (선택사항)\n",
    "                unexpected_keys = actual_keys - expected_keys\n",
    "                for key in unexpected_keys:\n",
    "                    result = {\n",
    "                        'key': key,\n",
    "                        'value': log['params'][key],\n",
    "                        'pass': 'UNEXPECTED'\n",
    "                    }\n",
    "                    \n",
    "                    # 고유키 값들 추가\n",
    "                    for uk in self.unique_keys:\n",
    "                        result[uk] = scenario.get(uk, '')\n",
    "                    \n",
    "                    results.append(result)\n",
    "\n",
    "        # XLSX 출력 - 컬럼 순서 정렬\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # 고유키 조합에 따른 컬럼 순서 정의\n",
    "        if 'page_id' in self.unique_keys:\n",
    "            column_order = ['page_id', 'click_type', 'act_type', 'key', 'value', 'pass']\n",
    "        else:  # click_text 기반\n",
    "            column_order = ['click_text', 'click_type', 'act_type', 'key', 'value', 'pass']\n",
    "        \n",
    "        # 컬럼 순서 적용\n",
    "        df_results = df_results[column_order]\n",
    "        df_results.to_excel(output_file, index=False, engine='openpyxl')\n",
    "        \n",
    "        # 요약 출력\n",
    "        self._print_summary(results)\n",
    "        print(f\"✅ 결과 저장: {output_file}\")\n",
    "\n",
    "    def _find_matching_logs(self, scenario: Dict) -> List[Dict]:\n",
    "        \"\"\"시나리오와 매칭되는 로그 찾기\"\"\"\n",
    "        matched = []\n",
    "        \n",
    "        for log in self.logs:\n",
    "            params = log['params']\n",
    "            \n",
    "            # 동적 고유키로 매칭\n",
    "            is_match = True\n",
    "            for key in self.unique_keys:\n",
    "                if params.get(key, '') != scenario.get(key, ''):\n",
    "                    is_match = False\n",
    "                    break\n",
    "            \n",
    "            if is_match:\n",
    "                matched.append(log)\n",
    "        \n",
    "        return matched\n",
    "\n",
    "    def _print_summary(self, results: List[Dict]):\n",
    "        \"\"\"결과 요약 출력\"\"\"\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        total = len(df)\n",
    "        passed = len(df[df['pass'] == 'PASS'])\n",
    "        failed = len(df[df['pass'] == 'FAIL'])\n",
    "        unexpected = len(df[df['pass'] == 'UNEXPECTED'])\n",
    "        \n",
    "        print(f\"\\n📊 검증 결과:\")\n",
    "        print(f\"   전체: {total}개\")\n",
    "        print(f\"   통과: {passed}개\")\n",
    "        print(f\"   실패: {failed}개\")\n",
    "        print(f\"   예상외: {unexpected}개\")\n",
    "        print(f\"   성공률: {passed/total*100:.1f}%\" if total > 0 else \"   성공률: 0%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 로그 분석 및 파일 출력 예시\n",
    "    print(\"=== 로그 분석 및 파일 출력 ===\")\n",
    "    run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
