{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\miniconda3\\envs\\pymc\\lib\\site-packages\\google\\auth\\_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery import job\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "PROJCECT = 'ballosodeuk'\n",
    "bq = bigquery.Client(project=PROJCECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH prod as (\n",
    "  SELECT Event_Date,User_ID, JSON_EXTRACT_ARRAY(Semantic_Event_Properties, '$.products') AS products\n",
    "  FROM `ballosodeuk.airbridge_lake.app`\n",
    "  WHERE  \n",
    "    TIMESTAMP_TRUNC(_PARTITIONTIME, DAY) >= TIMESTAMP(\"2024-04-29\") AND\n",
    "    Event_Category = 'Ad Impression (App)' AND Event_Label = '홈_상단_캐시버튼' AND Event_Action = '직광고_쿠팡' And\n",
    "    Semantic_Event_Properties is not Null AND\n",
    "    Event_Date between \"2024-04-11\" AND \"2024-04-24\"\n",
    "    ) \n",
    "    ,\n",
    "\n",
    "click as (\n",
    "  select distinct User_ID\n",
    "  from `ballosodeuk.airbridge_lake.app`\n",
    "  where \n",
    "    TIMESTAMP_TRUNC(_PARTITIONTIME, DAY) >= TIMESTAMP(\"2024-04-29\") AND \n",
    "    Event_Category = 'Ad Click (App)' AND\n",
    "    Event_Date between \"2024-04-11\" AND \"2024-04-24\"\n",
    ")\n",
    "  \n",
    "select p.Event_Date, p.User_ID,\n",
    "  ARRAY_TO_STRING(ARRAY(\n",
    "  select JSON_EXTRACT_SCALAR(products, '$.name')\n",
    "  from UNNEST(p.products) As products\n",
    "  ),',') as product_names,\n",
    "  IF(p.Event_Date between \"2024-04-11\" AND \"2024-04-17\",'before','after') as period\n",
    "from prod p\n",
    "join \n",
    "  click c on p.User_ID = c.User_ID\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT User_ID\n",
    "FROM `ballosodeuk.airbridge_lake.app_2024` \n",
    "WHERE Event_Date Between \"2024-04-26\" AND \"2024-05-02\"\n",
    "  ANd Event_Category = \"Sign-up (App)\"\n",
    "  AND Channel = 'invite_code'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "job_config = bigquery.QueryJobConfig()\n",
    "job_config.use_query_cache = False\n",
    "job_config.allow_large_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "query_job = bq.query(query,job_config=job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "996it [33:08,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: malloc of size 1610612736 failed\n"
     ]
    }
   ],
   "source": [
    "# 쿼리 실행 및 결과 저장\n",
    "# query_job = bq.query(query)\n",
    "\n",
    "# # 결과를 날짜별로 Parquet 파일로 저장\n",
    "# try:\n",
    "#     for partition in query_job.result().to_dataframe_iterable():\n",
    "#         if not partition.empty:\n",
    "#             # 이벤트 날짜를 YYYY-mm-dd 형식으로 변환하여 파일명 생성\n",
    "#             partition_date = partition['Event_Date'].iloc[0]\n",
    "#             # 저장할 경로 확인 및 생성\n",
    "#             save_path = f'../result/{partition_date}.parquet'\n",
    "#             # os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "#             # Parquet 파일로 저장\n",
    "#             partition.to_parquet(save_path)\n",
    "# except Exception as e:\n",
    "#     print(f'error: {e}')\n",
    "\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "\n",
    "data_by_date = {}\n",
    "\n",
    "# Process each partition in the iterable\n",
    "try:\n",
    "    for partition in tqdm(query_job.result().to_dataframe_iterable()):\n",
    "        if not partition.empty:\n",
    "            for index, row in partition.iterrows():\n",
    "                date_key = row['Event_Date']\n",
    "                if date_key not in data_by_date:\n",
    "                    data_by_date[date_key] = []\n",
    "                data_by_date[date_key].append(row)\n",
    "\n",
    "    # Now write each group to a Parquet file\n",
    "    for date_key, rows in data_by_date.items():\n",
    "        df = pd.DataFrame(rows)\n",
    "        save_path = f'../result/{date_key}.parquet'\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        df.to_parquet(save_path, engine='pyarrow', compression='snappy', index=False)\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(61678d00-0f6c-4aaf-9750-e1467234cea7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(55ce079c-b2b5-4b23-9b93-c9b3647e5061)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(d5804fd3-5355-49ce-aeda-7b9bbdacb13d)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(bd4fd3dd-d491-455b-8e53-db39b6effa74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(51bde99e-99d0-46eb-9a88-31bac53e4265)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>(ba141450-a8d4-427c-8032-ea7fcbeef1e8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>(8dc9ca7b-993e-413d-b39d-710b55508334)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>(a7c7df98-2704-4a6a-8725-fa3359a8e875)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>(5795eb60-f5cb-40ac-9190-d88b8c881744)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>(68b3bbd8-73f6-4885-a4e5-70f21cd453fb)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>761 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0\n",
       "0    (61678d00-0f6c-4aaf-9750-e1467234cea7)\n",
       "1    (55ce079c-b2b5-4b23-9b93-c9b3647e5061)\n",
       "2    (d5804fd3-5355-49ce-aeda-7b9bbdacb13d)\n",
       "3    (bd4fd3dd-d491-455b-8e53-db39b6effa74)\n",
       "4    (51bde99e-99d0-46eb-9a88-31bac53e4265)\n",
       "..                                      ...\n",
       "756  (ba141450-a8d4-427c-8032-ea7fcbeef1e8)\n",
       "757  (8dc9ca7b-993e-413d-b39d-710b55508334)\n",
       "758  (a7c7df98-2704-4a6a-8725-fa3359a8e875)\n",
       "759  (5795eb60-f5cb-40ac-9190-d88b8c881744)\n",
       "760  (68b3bbd8-73f6-4885-a4e5-70f21cd453fb)\n",
       "\n",
       "[761 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(query_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [13:48:48, 920.91s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m     combined_table \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mconcat_tables([table_to_append, new_table])\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# 파일에 저장\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     pq\u001b[38;5;241m.\u001b[39mwrite_table(combined_table, save_path)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# 새 파일 작성\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_parquet(save_path, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m, compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnappy\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\envs\\stats\\Lib\\site-packages\\pyarrow\\parquet\\core.py:3127\u001b[0m, in \u001b[0;36mwrite_table\u001b[1;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, **kwargs)\u001b[0m\n\u001b[0;32m   3103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3104\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ParquetWriter(\n\u001b[0;32m   3105\u001b[0m             where, table\u001b[38;5;241m.\u001b[39mschema,\n\u001b[0;32m   3106\u001b[0m             filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3125\u001b[0m             write_page_index\u001b[38;5;241m=\u001b[39mwrite_page_index,\n\u001b[0;32m   3126\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m-> 3127\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(table, row_group_size\u001b[38;5;241m=\u001b[39mrow_group_size)\n\u001b[0;32m   3128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   3129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(where):\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\envs\\stats\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1108\u001b[0m, in \u001b[0;36mParquetWriter.write_table\u001b[1;34m(self, table, row_group_size)\u001b[0m\n\u001b[0;32m   1103\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTable schema does not match schema used to create file: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1104\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtable:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m vs. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1105\u001b[0m            \u001b[38;5;241m.\u001b[39mformat(table\u001b[38;5;241m.\u001b[39mschema, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema))\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m-> 1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mwrite_table(table, row_group_size\u001b[38;5;241m=\u001b[39mrow_group_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 결과를 날짜별로 Parquet 파일로 저장\n",
    "try:\n",
    "    for partition in tqdm(query_job.result().to_dataframe_iterable()):\n",
    "        if not partition.empty:\n",
    "            for index, row in partition.iterrows():\n",
    "                # 이벤트 날짜를 YYYY-mm-dd 형식으로 변환하여 파일명 생성\n",
    "                partition_date = row['Event_Date']\n",
    "                save_path = f'../result/{partition_date}.parquet'\n",
    "                # 파일 경로 확인 및 생성\n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                \n",
    "                # 데이터 프레임 생성, 인덱스 제거\n",
    "                df = pd.DataFrame([row]).reset_index(drop=True)\n",
    "                \n",
    "                # 파일이 이미 존재하면 데이터를 읽어 추가 후 저장, 존재하지 않으면 새로 작성\n",
    "                if os.path.exists(save_path):\n",
    "                    # 기존 데이터 읽기\n",
    "                    table_to_append = pq.read_table(save_path)\n",
    "                    # 새 데이터를 PyArrow 테이블로 변환, 인덱스 제거\n",
    "                    new_table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "                    # 데이터 합치기\n",
    "                    combined_table = pa.concat_tables([table_to_append, new_table])\n",
    "                    # 파일에 저장\n",
    "                    pq.write_table(combined_table, save_path)\n",
    "                else:\n",
    "                    # 새 파일 작성\n",
    "                    df.to_parquet(save_path, engine='pyarrow', compression='snappy', index=False)\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: (\"Could not convert Row(('2024-04-24', '634b0a1e-bf91-49f1-940d-8266e3ab0a96', '워킹맨 아울렛 1+1 봄 여름 시즌 스판소재 런닝 자켓 아웃도어 외출복 일상복 빅사이즈 F101,지프 스피릿 남성 헨리넥 긴팔 티셔츠 305001,워킹맨 아울렛 1+1 봄 여름 시즌 후드탈부착형 경랑 라이트 메쉬 나일론 스판 바람막이 아웃도어 아우터 빅사이즈 F100,에버유어스 남성 바람막이 점퍼 아웃도어 등산복 후드 탈 부착자켓,곽씨네슈퍼 남성 골프 긴팔 티셔츠 골프복 골프웨어,에버유어스 아웃도어 봄 가을 남성 후드 탈부착 방풍방수 바람막이 자켓 등산복 재킷 일상복 상의 2color,레이지옴므 남성용 기본 무지 제인 집업가디건 간절기 니트 남자 카디건,[벤브로] 남자 봄 여름 가을 바람막이 경량 방수 후드자켓,언탭트 남성용 부드러운 워셔블 비스코스 니트 가디건,마운티스트 드라이 소프트 긴팔 집업 티셔츠 (남) 봄가을초겨울 따뜻한 부드러운,워킹맨 아울렛 4세트 봄 초여름 시즌 쾌적한 쿨론 집업 티셔츠 AS227,워킹맨 아울렛 1+1 봄 초여름 데일리 카라 긴팔티셔츠 작업복 일상복 근무복 아웃도어 티셔츠 AF216,워킹맨 아울렛 1+1 봄 가을 시즌 스판소재 남성 아웃도어 긴팔 스판 티셔츠 작업복 일상복 현장복 근무복 홈웨어 빅사이즈 AF205,봄 가을 시즌 남성 긴팔 집업 데일리 티셔츠 등산복 작업복 빅사이즈 스판티셔츠 A220,벤브로 남성용 데일리 라이더 자켓 캐주얼 가죽자켓+워런티 카드,워킹맨 아울렛 남성 1+1 봄 가을 시즌 스판 건빵 익스트림 펜츠 아웃도어 작업복 캠핑바지 낚시바지 일상복 BF128,샤필 에어포스 가을 겨울 항공점퍼 용 호랑이 잉어 남자 여 자수 점퍼 일본 스카쟌 남성 항공점퍼 누빔 가을 블루종 바람막이,영앤올드 겨울 남성 경량 웰론 패딩 점퍼 잠바 엠보 근무복 단체복 사무실 일상복,블랙야크 남여 공용 반팔티 B트리플라운드S 기능성 아이스 라운드 3팩 3종 세트', 'after'), {'Event_Date': 0, 'User_ID': 1, 'product_names': 2, 'period': 3}) with type Row: did not recognize Python value type when inferring an Arrow data type\", 'Conversion failed for column 0 with type object')\n"
     ]
    }
   ],
   "source": [
    "## 판다스 거치지 않은 더 빠른 쿼리\n",
    "from google.cloud import bigquery\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import os\n",
    "\n",
    "# 결과를 날짜별로 Parquet 파일로 저장\n",
    "try:\n",
    "    # Prepare a writer to handle output stream\n",
    "    writers = {}\n",
    "    for row in query_job.result():\n",
    "        # 이벤트 날짜를 YYYY-mm-dd 형식으로 변환하여 파일명 생성\n",
    "        partition_date = row['Event_Date']\n",
    "        save_path = f'../result/{partition_date}.parquet'\n",
    "\n",
    "        # 파일 경로 확인 및 생성\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        if save_path not in writers:\n",
    "            schema = pa.Table.from_pandas(pd.DataFrame([row])).schema\n",
    "            writers[save_path] = pq.ParquetWriter(save_path, schema, compression='snappy')\n",
    "        \n",
    "        # Write row to the corresponding Parquet file\n",
    "        table = pa.Table.from_pandas(pd.DataFrame([row]), schema=schema)\n",
    "        writers[save_path].write_table(table)\n",
    "\n",
    "    # Close all writers\n",
    "    for writer in writers.values():\n",
    "        writer.close()\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: \n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare a writer to handle output stream\n",
    "writers = {}\n",
    "\n",
    "# Define Arrow Schema explicitly based on your expected data types\n",
    "schema = pa.schema([\n",
    "    ('Event_Date', pa.string()),\n",
    "    ('User_ID', pa.string()),\n",
    "    ('product_names', pa.string()),\n",
    "    ('period', pa.string())\n",
    "])\n",
    "\n",
    "try:\n",
    "    for row in query_job.result():\n",
    "        # Convert the Row to a dictionary first, then create a DataFrame\n",
    "        row_dict = {\n",
    "            'Event_Date': str(row['Event_Date']),\n",
    "            'User_ID': str(row['User_ID']),\n",
    "            'product_names': str(row['product_names']),\n",
    "            'period': str(row['period'])\n",
    "        }\n",
    "        df = pd.DataFrame([row_dict])\n",
    "        \n",
    "        # 이벤트 날짜를 YYYY-mm-dd 형식으로 변환하여 파일명 생성\n",
    "        partition_date = row['Event_Date']\n",
    "        save_path = f'../result/{partition_date}.parquet'\n",
    "\n",
    "        # 파일 경로 확인 및 생성\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        if save_path not in writers:\n",
    "            writers[save_path] = pq.ParquetWriter(save_path, schema, compression='snappy')\n",
    "        \n",
    "        # Write row to the corresponding Parquet file\n",
    "        table = pa.Table.from_pandas(df, schema=schema, preserve_index=False)\n",
    "        writers[save_path].write_table(table)\n",
    "\n",
    "    # Close all writers\n",
    "    for writer in writers.values():\n",
    "        writer.close()\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../result\\\\2024-04-12.parquet'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dir = os.listdir('../result')\n",
    "path = os.path.join('../result',dir[3]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(path)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\envs\\stats\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    668\u001b[0m     path,\n\u001b[0;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    676\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\envs\\stats\\Lib\\site-packages\\pandas\\io\\parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    277\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    278\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n\u001b[0;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\envs\\stats\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2956\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   2949\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2950\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword is no longer supported with the new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2951\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets-based implementation. Specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2952\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to temporarily recover the old \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2953\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbehaviour.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2954\u001b[0m     )\n\u001b[0;32m   2955\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2956\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m _ParquetDatasetV2(\n\u001b[0;32m   2957\u001b[0m         source,\n\u001b[0;32m   2958\u001b[0m         schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[0;32m   2959\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m   2960\u001b[0m         partitioning\u001b[38;5;241m=\u001b[39mpartitioning,\n\u001b[0;32m   2961\u001b[0m         memory_map\u001b[38;5;241m=\u001b[39mmemory_map,\n\u001b[0;32m   2962\u001b[0m         read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[0;32m   2963\u001b[0m         buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[0;32m   2964\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m   2965\u001b[0m         ignore_prefixes\u001b[38;5;241m=\u001b[39mignore_prefixes,\n\u001b[0;32m   2966\u001b[0m         pre_buffer\u001b[38;5;241m=\u001b[39mpre_buffer,\n\u001b[0;32m   2967\u001b[0m         coerce_int96_timestamp_unit\u001b[38;5;241m=\u001b[39mcoerce_int96_timestamp_unit,\n\u001b[0;32m   2968\u001b[0m         thrift_string_size_limit\u001b[38;5;241m=\u001b[39mthrift_string_size_limit,\n\u001b[0;32m   2969\u001b[0m         thrift_container_size_limit\u001b[38;5;241m=\u001b[39mthrift_container_size_limit,\n\u001b[0;32m   2970\u001b[0m     )\n\u001b[0;32m   2971\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   2972\u001b[0m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[0;32m   2973\u001b[0m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[0;32m   2974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\envs\\stats\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2496\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[1;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001b[0m\n\u001b[0;32m   2492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2493\u001b[0m     fragment \u001b[38;5;241m=\u001b[39m parquet_format\u001b[38;5;241m.\u001b[39mmake_fragment(single_file, filesystem)\n\u001b[0;32m   2495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mFileSystemDataset(\n\u001b[1;32m-> 2496\u001b[0m         [fragment], schema\u001b[38;5;241m=\u001b[39mschema \u001b[38;5;129;01mor\u001b[39;00m fragment\u001b[38;5;241m.\u001b[39mphysical_schema,\n\u001b[0;32m   2497\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparquet_format,\n\u001b[0;32m   2498\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfragment\u001b[38;5;241m.\u001b[39mfilesystem\n\u001b[0;32m   2499\u001b[0m     )\n\u001b[0;32m   2500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2502\u001b[0m \u001b[38;5;66;03m# check partitioning to enable dictionary encoding\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\envs\\stats\\Lib\\site-packages\\pyarrow\\_dataset.pyx:1358\u001b[0m, in \u001b[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\envs\\stats\\Lib\\site-packages\\pyarrow\\error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\envs\\stats\\Lib\\site-packages\\pyarrow\\error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf, SparkContext\n\u001b[1;32m----> 2\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(conf\u001b[38;5;241m=\u001b[39mSparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMyApp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\envs\\stats\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\envs\\stats\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\Owner\\miniconda3\\envs\\stats\\Lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext(conf=SparkConf().setAppName(\"MyApp\").setMaster(\"local\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
